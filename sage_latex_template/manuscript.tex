% sage_latex_guidelines.tex V1.20, 14 January 2017

\documentclass[Afour,sageh,times]{sagej}

\usepackage{moreverb,url}

\usepackage[colorlinks,bookmarksopen,bookmarksnumbered,citecolor=red,urlcolor=red]{hyperref}

\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\def\volumeyear{2016}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}
\usepackage{color}
\usepackage{xcolor}
\usepackage{subfigure}
%\usepackage[hidelinks]{hyperref}
%\usepackage[printonlyused]{acronym}
\usepackage{acronym}
\usepackage{listings}
\usepackage{algorithm,algpseudocode}
%\usepackage{math}
\usepackage{tikz}
\usepackage{pgfplots}

%\journal{Computer Physics Communications}

% Custom colors
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\lstset{
    showstringspaces=false,
    extendedchars=true,
    morekeywords={self},
    emph={ParticleFor,ParticleInteraction,Assign},
    basicstyle=\footnotesize\ttfamily,
    commentstyle=\slshape\color{deepgreen},
    stringstyle=\ttfamily\color{deepred},
    keywordstyle=\ttfamily\color{deepblue},
    emphstyle=\ttfamily\color{violet},
    breaklines=true,
    breakatwhitespace=true,
    columns=flexible,
    numbers=left,
    numberstyle=\tiny,
    basewidth=.5em,
    xleftmargin=.5cm,
    captionpos=b,
    frame=lines
}

% Acronyms
\newacro{AoS}{array of structures}
\newacro{AoSoA}{array of structures of arrays}
\newacro{CPI}{cycles per instruction}
\newacro{DEM}{discrete element method}
\newacro{EAM}{embedded atom method}
\newacro{FCC}{face-centered cubic}
\newacro{FN}{full neighbor-lists}
\newacro{HN}{half neighbor-lists}
\newacro{ILP}{instruction-level parallelism}
\newacro{IR}{intermediate representation}
\newacro{DSL}{domain-specific language}
\newacro{AST}{abstract syntax tree}
\newacro{ISA}{instruction set architecture}
\newacro{LJ}{Lennard-Jones}
\newacro{MD}{Molecular dynamics}
\newacro{MPI}{message passing interface}
\newacro{SIMD}{single instruction, multiple data}
\newacro{SoA}{structure of arrays}
\newacro{PBC}{periodic boundary conditions}
\newacro{HPM}{hardware performance monitoring}

% Changes during review
%\usepackage{soul}
%\usepackage{xcolor}
%\DeclareRobustCommand{\RMchange}[1]{{\sethlcolor{cyan}\hl{#1}}}
%\newcommand{\RMchange}[1]{{\colorbox{purple!70}{#1}}}
%\newcommand{\RMchange}[1]{{\setlength{\fboxsep}{0pt}\colorbox{purple!70}{#1}}}
\newcommand{\RMchange}[1]{{\color{blue} #1}} 
%\newcommand{\RMchange}[1]{#1}

%Predefined commands
\newcommand{\bq}{\begin{equation}}
\newcommand{\eq}{\end{equation}}
\newcommand{\bytes}{\mbox{bytes}}
\newcommand{\byte}{\mbox{byte}}
\newcommand{\second}{\mbox{s}}
\newcommand{\seconds}{\mbox{s}}
\newcommand{\flop}{\mbox{flop}}
\newcommand{\flops}{\mbox{flops}}
\newcommand{\NJFLOP}{\mbox{nJ/\flop}}
\newcommand{\instr}{\mbox{instr}}
\newcommand{\cycle}{\mbox{cy}}
\newcommand{\iter}{\mbox{it}}
\newcommand{\cycles}{\mbox{cy}}
\newcommand{\FCY}{\mbox{\flop/\cycle}}
\newcommand{\FIT}{\mbox{\flop/\iter}}
\newcommand{\BIT}{\mbox{\byte/\iter}}
\newcommand{\FR}{\mbox{\flops/\mbox{row}}}
\newcommand{\BR}{\mbox{\byte/\mbox{row}}}
\newcommand{\CYF}{\mbox{\cycles/\flop}}
\newcommand{\CS}{\mbox{\cycle/\second}}
\newcommand{\GCS}{\mbox{G\cycle/\second}}
\newcommand{\word}{\mbox{Word}}
\newcommand{\words}{\mbox{Words}}
\newcommand{\order}[1]{\mbox{${\cal O}\left(\mbox{#1}\right)$}}
\newcommand{\bit}{\mbox{bit}}
\newcommand{\bits}{\mbox{bits}}
\newcommand{\GBPS}{\mbox{G\bit/\second}}
\newcommand{\MBPS}{\mbox{M\bit/\second}}
\newcommand{\FS}{\mbox{\flop/\second}}
\newcommand{\BS}{\mbox{\byte/\second}}
\newcommand{\BC}{\mbox{\byte/\cycle}}
\newcommand{\GBS}{\mbox{G\byte/\second}}
\newcommand{\MBS}{\mbox{M\byte/\second}}
\newcommand{\GWS}{\mbox{G\word/\second}}
\newcommand{\GFS}{\mbox{G\flop/\second}}
\newcommand{\MFS}{\mbox{M\flop/\second}}
\newcommand{\lup}{\mbox{LUP}}
\newcommand{\lups}{\mbox{LUPs}}
\newcommand{\LUPCY}{\mbox{\lup/\cycle}}
\newcommand{\LUPS}{\mbox{\lup/\second}}
\newcommand{\MLUPS}{\mbox{M\lup/\second}}
\newcommand{\GLUPS}{\mbox{G\lup/\second}}
\newcommand{\GHZ}{\mbox{GHz}}
\newcommand{\ns}{\mbox{ns}}
\newcommand{\WF}{\mbox{\word/\flop}}
\newcommand{\BF}{\mbox{\byte/\flop}}
\newcommand{\FB}{\mbox{\flop/\byte}}
\newcommand{\BL}{\mbox{\byte/\lup}}
\newcommand{\GB}{\mbox{GB}}
\newcommand{\KB}{\mbox{kB}}
\newcommand{\MB}{\mbox{MB}}
\newcommand{\GiB}{\mbox{GiB}}
\newcommand{\MiB}{\mbox{MiB}}
\newcommand{\KiB}{\mbox{KiB}}
\newcommand{\W}{\mbox{W}}
\newcommand{\muarch}{\mbox{$\mu$-arch}}
\newcommand{\muop}{\mbox{$\mu$-op}}
\newcommand{\muops}{\mbox{$\mu$-ops}}
\newcommand{\eos}{\;.}
\newcommand{\cma}{\;,}
\newcommand{\rlm}{roof{}line model}
\newcommand{\rl}{roof{}line}
\newcommand{\Rlm}{Roof{}line model}
\newcommand{\Rl}{Roof{}line}
\newcommand{\olsep}{\|}
\newcommand{\nolsep}{|}
\newcommand{\ecmspace}{\,}
\newcommand{\TOL}{$T_{\mathrm{c}\_\mathrm{OL}}$}
\newcommand{\NNZR}{$N_\mathrm{nzr}$}
\newcommand{\NR}{$N_\mathrm{r}$}
\newcommand{\NNZ}{$N_\mathrm{nz}$}
\newcommand{\ecm}[6]{\mbox{$\left\{{#1}\ecmspace\olsep\ecmspace {#2}\ecmspace\nolsep\ecmspace {#3}\ecmspace\nolsep\ecmspace {#4}\ecmspace\nolsep\ecmspace {#5}\right\}\ecmspace{#6}$}}
\newcommand{\epsep}{\rceil}
\newcommand{\ecmp}[4]{\mbox{$\left\{{#1}\ecmspace\epsep\ecmspace {#2}\ecmspace\epsep\ecmspace {#3}\right\}\ecmspace{#4}$}}
\newcommand{\ecme}[4]{\mbox{$\left({#1}\ecmspace\epsep\ecmspace {#2}\ecmspace\epsep\ecmspace {#3}\right)\ecmspace{#4}$}}
\newcommand{\sellcs}{SELL-\texorpdfstring{$C$-$\sigma$}{C-sigma}}
\newcommand{\likwid}{\texttt{LIKWID}}
\newcommand{\likwidperfctr}{\texttt{likwid-perfctr}}
\newcommand{\likwidpin}{\texttt{likwid-pin}}
\newcommand{\likwidbench}{\texttt{likwid-bench}}
\newcommand{\lmbench}{\texttt{lmbench}}

\begin{document}

\runninghead{Rafael Ravedutti Lucio Machado, Jan Eitzinger and Harald Köstler}

%\title{P4IRS: A Code Generation Framework for Parallel and Performance-Portable Particle Simulations}
\title{P4IRS: An Intermediate Representation and Compiler for Parallel and Performance-Portable Particle Simulations}

\author{Rafael Ravedutti Lucio Machado\affilnum{1}, Jan Eitzinger\affilnum{1}, Harald Köstler\affilnum{1}}

\affiliation{\affilnum{1}Erlangen National High Performance Computing Center}
\corrauth{Rafael Ravedutti Lucio Machado, Cauerstraße 11, Erlangen, 91058, Bayern, Germany}
\email{rafael.r.ravedutti@fau.de}

\begin{abstract}
Various physics simulations today rely on simulating particle interactions, where particles can represent point masses (Molecular Dynamics), rigid bodies (Discrete Element Method) or even massive bodies such as planets.
Evaluating and calculating the required particle interactions in a simulation is computationally expensive, hence suitable algorithms and proper optimizations to exploit available parallelism in the target hardware are important to reach good performance.
However, it is difficult to maintain flexible implementations while keeping state-of-the-art performance, as most packages are developed individually and have their own hard-coded, fine-tuned implementations.
To combine flexibility and optimal performance, we introduce P4IRS, an intermediate representation and compiler for particle simulations which aims at generating high-performing code.
We describe P4IRS and its features, provide some usage examples for MD and DEM fields and discuss the benefits we can obtain with code generation.
Finally, we evaluate the performance and scalability from the code generated by P4IRS on modern processors, accelerators and supercomputers.
\end{abstract}

\keywords{code generation, particle simulations, molecular dynamics, discrete element method, HPC}

\maketitle

\section{Introduction}

The computation of particle-pair interactions is widely used in physics simulations, especially in the fields of Molecular Dynamics and Discrete Element Method.
To efficiently compute the inter-particle contributions, different strategies can be used to increase efficiency and make use of the parallelism offered by the target hardware.
Most simulation packages, however, have their own hard-coded implementations, which are strictly tightened to their data structures and programming artifacts.
Also, several kernel implementations for the same potential or force field are present in a single package to account for different hardwares and optimizations strategies, leading to higher maintainability costs and higher portability effort\RMchange{s}.

To tackle these limitations, we developed P4IRS --- Parallel and Performance-Portable Particles Intermediate Representation and Simulator, an intermediate representation and compiler tool where particle interactions can be simply expressed as Python methods, and efficient and parallel code for the interaction description can be generated for multi-CPU and multi-GPU targets.
P4IRS provide its own intermediate representation, which allows it to perform compiler analysis and transformations, to then generate code for the proper backend such as C++ and CUDA with OpenMP/MPI.

P4IRS can generate several kernel flavours of the same potential such as Linked Cells (\cite{linkedcells}) and Verlet Lists (\cite{verletlists}), where data layouts for properties and specific arrays (such as the neighbor-lists) can be switched to the case with \RMchange{the} best performance.
A runtime interface in C++ is also available and can be used to integrate P4IRS with other simulation tools, which is particularly important for integrating P4IRS simulations with our multi-physics framework waLBerLa (\cite{walberla1,walberla2}).
%as an example we show how P4IRS can use the load-balancing communication algorithm from waLBerLa to compute simulations with a more heterogeneous distribution of particles.

Most of \RMchange{the} P4IRS internal routines such as building Linked Cells and Verlet Lists are implemented in Python by constructing the intermediate representation for them with the assistance of operator overloading (see \nameref{sec:backend} section).
In this way, we leverage P4IRS analysis and transformations to achieve performance portability for these routines as well, without requiring multiple tailored implementations.

The goals of this paper are summarized as follows:
\begin{itemize}
    \item We present our code generator tool P4IRS, describing how it works and how it can be used to generate efficient particle simulation codes.
    \item We discuss the advantages of using P4IRS to achieve performance portability, also comparing it with other previous approaches such as tinyMD (\cite{tinymd}) and MESA-PD (\cite{mesapd1,mesapd2}).
    \item We describe P4IRS interfaces to allow its generated code to be integrated with other simulation tools, specifically in the context of domain-partitioning and communication.
    \item We provide performance results for Intel Ice Lake and AMD Milan CPUs, as well as for NVIDIA A40 and A100 accelerators to demonstrate the state-of-the-art performance of the applications generated by P4IRS in comparison to MD-Bench and MESA-PD.
    \item We display weak-scaling results in the Fritz and JUWELS Booster clusters to demonstrate the weak-scaling capabilities of P4IRS in MD and DEM simulations on modern supercomputers.
\end{itemize}

This paper is structured as follows: In the \nameref{sec:related_work} section we list related work for MD and DEM simulation packages, code generation tools for particle simulations and performance evaluation of such simulations.
In the \nameref{sec:background} section we explain the basic theory for particle simulations focusing on MD and DEM methods, also covering optimization strategies and choices.
In the \nameref{sec:pairs} section we present P4IRS with its current features and application design.
In the \nameref{sec:evaluation} section we evaluate the performance and scalability \RMchange{of} P4IRS applications using \ac{MD} and \ac{DEM} experiments.
Finally, the \nameref{sec:conclusion} section presents our conclusion\RMchange{s} and outlook for P4IRS, also covering our future work.

\section{Related Work}
\label{sec:related_work}

\subsection{Simulation Packages}
\label{sec:packages}

Diverse simulation fields can be described by updating particle trajectories based on their interactions with other particles.
In this paper, we focus mainly on \ac{MD} and \ac{DEM}, where particles represent either point masses (i.e. atoms) or rigid bodies with specific shapes (i.e. spheres, half-spaces, ellipses), respectively.
Most available packages use similar techniques to compute trajectories, but each \RMchange{has its} own specific algorithms, parallelism strategies and hand-tuned implementations for specific hardware.
With P4IRS, we focus on using a single description of the problem, and then leverage its implemented routines and code transformations to produce the backend implementation for the specific target, which currently can be C++ with OpenMP and CUDA, also supporting MPI for distributed-memory parallel clusters.

In MD, there exist many packages with distinct strategies and simulation fields.
LAMMPS (\cite{lammps1,lammps2}) is a molecular package for the material-modeling field which uses the standard Verlet Lists algorithm to optimize the non-bonded short-range force calculations.
It offers many inter-atomic potentials, each one with different kernel implementations for a specific target such as CPU, GPU or Kokkos (\cite{kokkos}).
GROMACS (\cite{gromacs1,gromacs2}) is another package for MD with \RMchange{a} focus on \RMchange{life sciences simulations,} which uses its own Cluster Pair algorithm to optimize the \RMchange{computation of} short-range forces.
It does not support as many potentials as LAMMPS, but its algorithm tends to be significantly faster than the Verlet Lists since it employs SIMD parallelism more efficiently.
Furthermore, the optimal GROMACS kernel implementations rely on SIMD intrinsics to attain higher performance, which makes them harder to be maintained and ported to other architectures.
Also, the GPU algorithm in GROMACS differs from the CPU, since it relies on a strategy named ``super-clustering'' to reduce the amount of redundant computations caused by adjusting the cluster sizes to fit the SIMD width in the GPU.
Other MD packages such as HOOMD-blue (\cite{hoomdblue}), NAMD (\cite{namd}), AMBER (\cite{amber1}) and CHARMM (\cite{charmm}) also contain dedicated and hand-tuned implementations in general purpose languages for each supported backend.
Each version is tested, maintained, debugged and optimized independently.

Packages that support DEM simulations are in general more flexible since simulation with more particle properties such as rotations and shapes are needed.
However, these do not have usually the same optimization techniques as the MD simulation packages, and the reason is most likely that it is not so feasible to support flexibility with sophisticated optimization algorithms as the complexity of the code grows considerably.
Examples of such packages are the LIGGGHTS DEM (\cite{liggghts}) (based on LAMMPS), MercuryDPM (\cite{mercurydpm}) and YADE (\cite{yade}).
To mitigate such limitations, a few code generation approaches were developed for both MD and DEM.

\subsection{Code Generation Frameworks}
\label{sec:codegen}

% TODO: OpenMM, MDL, ppmd, OpenFPM, PPME
% TODO: MESA-PD, tinyMD

Solutions based on code generation are also available for specific fields of particle simulations, and these aim at providing a higher level of flexibility and extensibility while striv\RMchange{ing} for efficient performance in modern accelerators.

OpenMM (\cite{openmm}) is a \ac{MD} package that makes use of Python as a front-end for simulation protocols and file I/O, it contains a C++ API for forces and integrators as well as optimized kernel implementations for CUDA, C++ and OpenCL.
Integrators and forces can be implemented by using mathematical descriptions in OpenMM, but its use cases are restricted to \ac{MD} simulations and several features required for a DEM simulation or other types of particle simulations (such as contact history and different shapes) are not available.

MESA-PD (\cite{mesapd3}) is a general particle dynamics framework that is built-in the waLBerla framework for the simulation of particles and fluids.
Its design principle of separation of code and data allows to introduce arbitrary interaction models with ease, which makes it also useful for \ac{MD}.
By using its code generation approach through Jinja templates\footnote{\url{https://palletsprojects.com/projects/jinja/}}, it can be adapted very effectively to any simulation scenario.
As successor of the ``physics engine'' (pe) rigid particle dynamics framework it inherits its scalability and advanced load balancing functionalities.
One of the limitations of MESA-PD is that it does not support GPU code generation, and since its code generation approach is based on Jinja templates, the effort for implementing new functionalities may not be lower compared to other simulation packages without code generation techniques (for instance, most of its force kernels are not generated but rather implemented in C++).

In previous work we developed tinyMD, a proxy-app (also based on miniMD) created to evaluate the portability of MD and DEM applications with the AnyDSL (\cite{anydsl1,anydsl2}) framework.
TinyMD uses higher-order functions to abstract device iteration loops, data layouts and communication strategies, providing a domain-specific library to implement pair-wise interaction kernels that execute efficiently on multi-CPU and multi-GPU targets.
One of \RMchange{the limitations of tinyMD} is that its flexibility and optimization settings are restricted to what can be achieved through higher-order functions and the AnyDSL compilation framework, which restricts the backend to the LLVM toolchain (since it generates LLVM IR at the end).
This means that the compiler optimizations from GCC and ICC cannot be applied to AnyDSL implementations (in contrast to C/C++ implementations), and one of the reasons we develop P4IRS is to overcome this restriction.


\subsection{Performance Evaluation}
\label{sec:perfeval}

Most performance-oriented research for \ac{MD} systems uses implemented packages as a black-box and adjust different simulation parameters to analyse their implications in the overall performance.
In some cases, however, in-depth performance studies were made to gain a broader understanding of efficient ways to achieve SIMD vectorization with SSE/AVX instructions (\cite{mdsimd}).
Most in-depth studies are performed using mini-apps or proxy-apps, one example is miniMD, a proxy-app that provides a simple copper face-cubic-centered lattice simulation where atom forces are computed with kernels extracted from LAMMPS for either the \ac{LJ} or \ac{EAM} potentials.

The Exascale Computing Project (ECP) Co-design Center for Particle Applications (CoPA) (\cite{ecpcopa}) focuses on co-designing particle applications to prepare them for exascale computing.
They provide several proxy-apps for particle simulations such as CabanaMD and ExaMiniMD, which are designed in a modular fashion to account for different optimization strategies and particle data layouts, and are built on top of Kokkos to be able to execute on many-core CPU and GPU targets.

The performance-focused prototyping harness MD-Bench (\cite{mdbench1,mdbench2}) is a toolbox developed for performance-engineering \ac{MD} non-bonded short-range force calculation algorithms, with focus on in-core execution on CPUs and GPUs.
It currently supports the Verlet Lists and Cluster Pair algorithms from LAMMPS and GROMACS, respectively.
One of our goals is to use the knowledge obtained with MD-Bench and integrate it into P4IRS to achieve state-of-the-art performance and flexibility altogether in particle simulations.

Based on our searches, there are not many in-depth performance studies focusing on SIMD and in-core execution for kernels used in DEM simulations, and most existing studies focus on evaluating their distributed-memory parallelism in weak-scaling settings.
With P4IRS, we intend to bridge the gap between performance strategies across different simulation domains, and use the most efficient algorithm whenever it is possible without requiring significant efforts such as reimplementing kernels and redesigning applications with tailored data structures \RMchange{to improve} the performance.

To evaluate performance on a single CPU core or GPU in this paper, we use MD-Bench and miniMD as the baseline performance, as they are well-known implementations that reach state-of-the-art performance for MD simulations.
For DEM cases, we use MESA-PD as the comparison target, since our focus is to show that we can achieve performance-portability with the flexibility that MESA-PD provides. 

\section{Background and Theory}
\label{sec:background}

Particle simulations consist of solving N-body problems and have a wide range of applications as many fields can be represented as a system of interacting particles.
In this sense, particles can be either atoms, rigid bodies or even planets, stars and galaxies.
Interaction contributions can come either from inter-atomic van-der-Walls forces based on the \ac{LJ} potential, or even from gravitational forces generated by super-massive particles.
In order to simulate such systems, however, we need well-defined and closed system of particles with known boundary conditions.
In the scope of this paper, particle contributions are computed for particles that lie within a range or ``cutoff radius''. 
Strategies to compute approximations \RMchange{of} particles beyond this range (also known as long-range contributions) also exist, but are not disclosed.
To provide distinct examples of simulations and \RMchange{to} demonstrate a certain level of flexibility from P4IRS, we focus on MD and DEM simulations in this work.

\subsection{Molecular Dynamics}
\label{sec:md}

\ac{MD} simulations are widely used today to study the behavior of microscopic structures.
These simulations reproduce the interactions \RMchange{between} atoms in these structures on a macroscopic scale while allowing us to observe the evolution \RMchange{of} the system of particles \RMchange{on} a time-scale that is simpler to analyze.

Different areas such as material\RMchange{s} science to study the evolution of the system for specific materials, chemistry to analyze the evolution of chemical processes, or biology to reproduce the behavior of certain proteins and bio-molecules resort to simulations of \ac{MD} systems.

A \ac{MD} system to be simulated \RMchange{consists} of a number of atoms, the initial state (such as the atoms' position or velocities), and the boundary conditions of the system.
Here, we use \ac{PBC} in all directions, hence when one atom crosses the domain, it reappears on the opposite side with the same velocity.
%These simulations are defined by establishing the number of atoms in a system, the initial state of the system (such as the atoms' positions or velocities), and the boundary conditions for the system.
Fundamentally, atom trajectories in classical \ac{MD} systems are computed by integrating \RMchange{the} Newton-Euler equations for every atom $i$:
\begin{equation}
    \hat F_i = m_i \dot{\hat v}_i \label{eq:newton_force}
\end{equation}
\begin{equation}
    \hat v_i = \dot{\hat x}_i \label{eq:newton_velocity}
\end{equation}
Where $m_i$, $\hat v_i$ and $\hat x_i$ are the mass, the velocity vector and the position vector of the atom $i$, respectively.
The force $F_i$ then must be calculated to integrate the positions of the atoms, and these are described by the negative gradient of the potential of interest.
The mutual force between atom $i$ and $j$ as governed by the \ac{LJ} potential
is given as:
\begin{equation}
    \hat{F}_{LJ}(\hat{x_i}, \hat{x_j}) = -\frac{24\varepsilon}{x_{ij}} \left( \frac{\sigma}{x_{ij}} \right)^{6} \left[ 2\left(\frac{\sigma}{x_{ij}}\right)^{6} - 1\right] \hat{x}_{ij}~,
    %V_{LJ} = 4\varepsilon \left[ \left(\frac{\sigma}{r}\right)^{12} - \left(\frac{\sigma}{r}\right)^6 \right]
    \label{eq:lennard_jones}
\end{equation}
Where $\hat{x}_{ij}$ is the distance vector between atoms $i$ and $j$, $x_{ij}$ is the distance between atoms $i$ and $j$, $\varepsilon$ is the width of the potential well, and $\sigma$ specifies at which distance the potential is~$0$.

\subsection{Discrete Element Method}
\label{sec:dem}

Besides computing trajectories for zero-dimensional point masses as done in MD, we can model particles as objects with specific shapes and spatial extension using the Discrete Element Method (DEM).
This requires support of more degrees of freedom for particles, since spatial properties (such as rotation) need to be taken into consideration.
In this work, we consider particles with homogeneous density, and then use the center of mass for each particle as its position, which coincides with its geometrical center due to its density properties.
Equations of motions are extended with the following terms for the torque and angular velocity:

\begin{equation}
    \hat{\tau}{_i} = I \dot{\hat \omega}_i + {\hat \omega}_i \times I {\hat \omega}_i \label{eq:newton_torque}
\end{equation}
\begin{equation}
    \hat{\omega}{_i} = \dot{\varphi}_i \label{eq:newton_angular_velocity}
\end{equation}

With the torque vector $\hat{\tau}{_i}$, angular velocity vector $\hat{\omega}{_i}$, rotation $\varphi$ (typically stored as a quaternion, rotation matrix or rotation vector) and moment of inertia \RMchange{around} the center of mass $I = I(\varphi_i)$.
A collision detection step is required to check when two particles overlap, and parameters about the collision such as the contact point and penetration depth are computed and used in the force calculation.
Note that the reason a penetration occurs during a collision is that the collision is not detected right when it starts because the simulation is discretized by the time-step size.
For simplification purposes, we only calculate collision detections analytically in this work since there are no particles with complex shapes involved.

In this work, we use the penalty-based Linear Spring-Dashpot model to calculate forces \RMchange{between} particles for the DEM cases.
The collision force is split into two components, one representing the force in the normal direction and one representing the force in the tangential direction.

The relative contact velocity $\hat{u}_{ij}$ is of importance for most contact models.
Considering $\hat{c}_{ij}$ the contact point and $\hat{n}_{ij}$ the normal direction vector between the particles $i$ and $j$, the relative contact velocity and its normal and tangential components $\hat{u}_{ij}^{n}$ and $\hat{u}_{ij}^{t}$ can be expressed as:


\begin{equation}
    \hat{u}_{ij} = \hat{v}_{i} - \hat{v}_{j} + \hat{\omega}_{i} \times (\hat{c}_{ij} - \hat{x}_{i}) - \hat{\omega}_{j} \times (\hat{c}_{ij} - \hat{x}_{j})
    \label{eq:relative_contact_velocity}
\end{equation}

\begin{equation}
    \hat{u}_{ij}^{n} = \hat{u}_{ij} \cdot \hat{n}_{ij} * \hat{n}_{ij}
    \label{eq:relative_contact_velocity_n}
\end{equation}

\begin{equation}
    \hat{u}_{ij}^{t} = \hat{u}_{ij} - \hat{u}_{ij}^{n}
    \label{eq:relative_contact_velocity_t}
\end{equation}

Then, the force components in the normal direction $\hat{F}_{SD}^{n}$ and in the tangential direction $\hat{F}_{SD}^{t}$ can be defined as:

\begin{equation}
    \hat{F}_{SD}^{n}(\hat{x_i}, \hat{x_j}) = k^{n}\delta_{ij}\hat{n}_{ij} - \xi^{n}\hat{u}_{ij}^{n}
    \label{eq:linear_spring_dashpot_normal}
\end{equation}

\begin{equation}
    \hat{F}_{SD}^{t}(\hat{x_i}, \hat{x_j}) = -\min \left\{ k^{t}\left| \delta^{t}_{ij} \right|,\mu\left|\hat{F}_{SD}^{n}(\hat{x_i}, \hat{x_j}) \right| \right\} \hat{\delta}^{t}_{ij}
    \label{eq:linear_spring_dashpot_tangential}
\end{equation}

\begin{equation}
    \hat{\delta}^{t}_{ij} \gets \hat{\delta}^{t}_{ij} + \hat{u}_{ij}^{t}dt
    \label{eq:update_rule}
\end{equation}

Where $k^{n}$ is the normal spring stiffness, $k^{t}$ the tangential spring stiffness, $\xi^{n}$ the normal velocity damping, $\mu$ the friction, $\delta_{ij}$ the penetration depth and $dt$ the time-step size.
The update rule in \autoref{eq:update_rule} and modifications in \autoref{eq:linear_spring_dashpot_tangential} were properly included to fix frictional contact accordingly (\cite{dem1,dem2}).
The tangential spring displacement $\hat{\delta}^{t}_{ij}$ is initialized as a zero vector in the first contact between the particles $i$ and $j$, and the update rule is carried out every time-step the contact persists.
From the algorithmic point of view, this means that a contact history data structure is required to keep track of the last computed value for every pair of particles that are in contact.

\subsection{Optimization Strategies}
\label{sec:opts}

\begin{figure*}[htb]
    \centering
    \subfigure[Linked Cells]{\label{fig:linked_cells}\includegraphics[width=0.25\textwidth]{linked_cells.pdf}}
    \subfigure[Verlet Lists]{\label{fig:verlet_lists}\includegraphics[width=0.25\textwidth]{verlet_lists.pdf}}
    \caption{Comparison between (a) Linked Cells and (b) Verlet Lists algorithms, in each case, the blue particles are used as the candidates to compute the forces with the red particle. In the Linked Cells, all particles in the neighbor cells are considered (considering the size of the cell is higher or equal than the cutoff radius). In the Verlet Lists, a list of particles is created by evaluating which particle distances are smaller than the cutoff radius plus a ``skin'' value, which represents a sphere in the 3D case.}
\end{figure*}

To calculate the resulting force for a particle in a specific time-step, it is necessary to compute all partial force contributions from each of the other particles present in the system.
In a naive strategy, this results in a quadratic complexity and severely affects the application performance, which limits the system size and the time-scale of the simulation.

When force contributions become negligible at long-distance interactions or when a contact between particles is needed, different algorithm\RMchange{ic} strategies can be employed to perform computations only for particles within a specific cut-off region.
There are two main data structures which are commonly used for efficiently computing particle interactions in state-of-the-art simulation packages: the Linked Cells and the Verlet Lists.

Linked Cells strategy (see Figure~\ref{fig:linked_cells}) sorts particles spatially according to their position in cells (also called bins), where cells have a fixed size, and mapping particles into cells can be done directly by computing in which cell a particle is contained within.
After, forces can be computed for each particle by traversing its current and neighbor cells, and only adding contributions for particles within these cells.

The Linked Cells results in computing only forces for particles within a region that is discretized by their current and neighbor cells/bins, which can still results in a significant amount of redundant computations.
In order to avoid even more redundant computations, the Verlet Lists strategy can be used jointly with Linked Cells.
The Verlet Lists (see Figure~\ref{fig:verlet_lists}) constructs a list of neighbor particles for every particle, and this list can be created by using the Linked Cells structure, evaluating the distances among particles within close bins and then using the distance to determine whether such particles should be added into the list.
In the Verlet Lists case, the region for searching the neighbor particles is defined by a sphere, where the center of the sphere is the position of the particle and the radius is the cutoff radius plus a ``skin''.
The ``skin'' is used to avoid building the neighbor-lists at every time-step, it extends the neighbor-search and avoid missing computations from particles that end up entering the cut-off region in subsequent time-steps in which the neighbor-lists are not reconstructed.

Despite avoiding computations from long-distance particles, another common optimization is to take advantage of Newton's Third Law and compute forces in only one direction.
This means that only half of particle-pairs are traversed when computing the forces, and partial forces are decreased for the neighbor particles during their computation.
Ideally, this should lead to a factor of two speedup, but this strategy has a few drawbacks: (a) parallelism is harmed since decreasing \RMchange{the} force\RMchange{s} of neighbor particles requires atomic operations to avoid race conditions and (b) when the code is vectorized with SIMD intrinsics (such as AVX2 and AVX512), gather and scatter instructions are needed to build up the SIMD registers with forces of the neighbor particles, which results in scattered memory accesses and extra instructions.
At the end, this strategy is likely to be beneficial for more arithmetic-intensive kernels, and the trade-off between parallelism and less arithmetic operations must be assessed to know when it is advantageous.

\section{P4IRS}
\label{sec:pairs}

% Show examples (LJ, EAM)
% Linked Cells / Verlet Lists
% Can generate CPU/GPU code
% Vectors, quaternions and matrices
% Properties, feature properties and contact properties
% Contact history
% Shape partitioning

\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.6\textwidth]{pairs_overview.png}
  \caption{Overview of P4IRS architecture. The Frontend is defined in high-level Python language, and describes the computation and optimization strategies for the simulation. The compiler uses the data defined in the Frontend, combines with its internal kernel implementations and perform several analyses and transformations. Finally, it generates the backend code for the target hardware, which currently is either C++ or CUDA code with OpenMP and MPI support.}
  \label{fig:features}
\end{figure*}

P4IRS\footnote{\url{https://i10git.cs.fau.de/software/pairs}} is an open-source, stand-alone compiler and domain-specific language for particle simulations which aims at generating optimized code for different target hardwares.
It is released as a Python package and allows users to define kernels, integrators and other particle routines in a high-level and straightforward fashion without the need to implement any backend code.

\autoref{fig:features} displays an overview of P4IRS.
\RMchange{On} the left side, the front-end library from P4IRS is displayed with the available features, all these are defined through a Python interface by the user for modeling the simulation and determining the optimization schedule.
In the middle, the backend with the internal methods (deep-embedded in Python) with the compiler transformations and analyses are shown, these make use of the front-end representation to produce the final backend code.
\RMchange{On} the right side, the final generated code is integrated and compiled together with the runtime library and functionalities to achieve parallelism and access to external \RMchange{functions}.
The next subsections explore the different P4IRS features in more detail.

\subsection{Frontend and Features}
\label{sec:frontend}

\autoref{lst:pairslj} shows a simple description in P4IRS to compute forces with the \ac{LJ} potential.
Functions can be provided through a method (line 1), and the parameters in the method refer to different particles in the system.
In this way, many-body potentials can also be described through methods with more than two parameters.
Some keywords are also provided by P4IRS for writing the kernels, in line 2 the \emph{squared\_distance} method is used for obtaining the squared distance for the atoms \emph{i} and \emph{j}, and in line 4 the \emph{delta} method is used for obtaining the delta vector.
Also, in lines 3 and 4 it is possible to see accesses to feature properties called \emph{sigma6} and \emph{epsilon}, which are defined during the simulation setup as properties which depend on the atom types feature.
In line 4, the \emph{apply} method is used for applying the computed term into the \emph{force} property of the particles, the method allows the identification of particle propert\RMchange{y} reductions and is also used when generating the kernel variant using the half neighbor-lists.

\begin{lstlisting}[language=Python,
		   label={lst:pairslj},
		   caption={Lennard-Jones force description in P4IRS.}]
def lennard_jones(i, j):
    sr2 = 1.0 / squared_distance(i, j)
    sr6 = sr2 * sr2 * sr2 * sigma6[i, j]
    apply(force, delta(i, j) * (48.0 * sr6 * (sr6 - 0.5) * sr2 * epsilon[i, j]))
\end{lstlisting}

Functions that update every particle separately can also be described with a single parameter.
\autoref{lst:euler} displays how to implement a simple Euler integrator for \ac{MD} (see line 1).
In lines 2-3, there are also property accesses for the particles' linear velocities, forces, masses and positions.
Note that vector operations are intrinsically supported in P4IRS, which is also the case for matrices and quaternions data structures.
P4IRS methods also allow the specification of parameters such as the \emph{dt} specified in line 3, these must be given when setting up the simulation.

\begin{lstlisting}[language=Python,
		   label={lst:euler},
		   caption={Euler integrator description in P4IRS.}]
def euler(i):
    velocity[i] += dt * force[i] / mass[i]
    position[i] += dt * velocity[i]
\end{lstlisting}

\autoref{lst:pairs_md_setup} shows the simulation setup for the \ac{MD} case. In the first part (lines 6-10), a \emph{simulation} instance is created, and its identifier, the list of particle shapes used, the number of time-steps and the floating-point precision are specified.
Further, the properties needed for each particle in the simulation are specified (lines 13-16), together with their data types and (if needed) initial values.
Some properties are also defined as volatile (in this case the \emph{force} at line 16) property, which means \RMchange{that} they are reset after \RMchange{each} time-step.
To add feature properties, it is first required to add the specified feature as shown in line 19 (in this example, each atom has a feature named \emph{type}), and then specify the feature name for these properties (see lines 20-21).
Next, the simulation domain is specified (line 24) and the initial state of the system is defined by reading data from a file (lines 27-30), a list of properties must be given in the same order as they appear in the file, and the particle shape\RMchange{s} must also be included.
Different entries can be used to read particles with different shapes in the same simulation.

\begin{lstlisting}[language=Python,
		   label={lst:pairs_md_setup},
		   caption={Simple example for MD simulation setup in P4IRS.}]
import pairs

# ...

# Simulation setup
psim = pairs.simulation(
  "md", # Simulation identifier
  [pairs.point_mass()], # List of shapes
  timesteps=200, # Number of time-steps
  double_prec=True) # Use double-precision

# Particle properties
psim.add_position('position')
psim.add_property('mass', pairs.real(), 1.0)
psim.add_property('velocity', pairs.vector())
psim.add_property('force', pairs.vector(), volatile=True)

# Features and their properties
psim.add_feature('type', ntypes)
psim.add_feature_property('type', 'epsilon', pairs.real(), [...])
psim.add_feature_property('type', 'sigma6', pairs.real(), [...])

# Simulation domain
psim.set_domain([xmin, ymin, zmin, xmax, ymax, zmax])

# Initial state
psim.read_particle_data(
  "data/copper_fcc_lattice.input",
  ['type', 'mass', 'position', 'velocity'],
  pairs.point_mass())

# Optimization settings
psim.reneighbor_every(20)
psim.compute_half()
psim.build_neighbor_lists(cutoff_radius + skin)
psim.vtk_output("output/md", every=20)

# Kernels to compute
psim.compute(lennard_jones, cutoff_radius)
psim.compute(euler, symbols={'dt': dt})

# Target hardware
if target == 'gpu':
  psim.target(pairs.target_gpu())
else:
  psim.target(pairs.target_cpu())

psim.generate()
\end{lstlisting}

After the properties and simulation declarations, the optimization settings (or schedule) are specified (see lines 33-35).
These can be, for instance, whether the Verlet Lists strategy is used, the particle reneighboring frequency, and whether half neighbor-lists must be used.
Note that, ideally, optimization strategies should not change the semantics of the generated program nor the simulation results, but since particle simulations are sensitive to the order of computation performed and the floating-point precision, it is possible that simulations with different optimization strategies lead to different results.
This is not an issue for particle simulations as long as it is still possible to validate the simulations through macroscopic properties such as the temperature or pressure (for \ac{MD} simulations) or through analysis of the physical dynamics in the system (for \ac{DEM} simulations).

The \emph{vtk\_output} directive (line 36) is used to write data to VTK files at every 20 time-steps.
Finally, the kernels to be computed must be specified using the \emph{compute} method (lines 39-40).
The cutoff radius must be specified for kernels with more than one parameter/particle and other parameters used in such kernels (such as the \emph{dt} in the Euler kernel) must also be provided as a Python dictionary.
The final steps define the target and call the \emph{generate} method to then trigger the code generator.
P4IRS provide\RMchange{s} targets with default settings via the \emph{target\_cpu} and \emph{target\_gpu} settings, but custom targets can be defined with their own options such as the thread and block configurations.

\subsection{Contact History and Contact Properties}
\label{sec:contact_history}

When implementing DEM kernels such as the Linear Spring-Dashpot, properties computed during two contacted particles must be used across subsequent time-step interactions whenever they happen.
For this reason, P4IRS supports contact properties, which are properties tightened to a contact in the simulation.
These can be simply added through the \emph{add\_contact\_property} method as can be seen in \autoref{lst:contactprops1} for the sticking, tangential spring displacement and the impact velocity magnitude:

\begin{lstlisting}[language=Python,
		   label={lst:contactprops1},
		   caption={Contact properties setup example.}]
psim.add_contact_property(
  'is_sticking', pairs.int32(), 0)
psim.add_contact_property(
  't_spring_disp', pairs.vector(), [0.0, 0.0, 0.0])
psim.add_contact_property(
  'impact_velocity', pairs.real(), 0.0)
\end{lstlisting}

These properties can be used directly in kernels in the same way as feature properties.
\autoref{lst:contactprops2} shows an example of their usage in the Linear Spring-Dashpot kernel.
Note that when such properties are not stored for the particle pair, their default values are used.

\begin{lstlisting}[language=Python,
		   label={lst:contactprops2},
		   caption={Contact properties usage example.}]
def linear_spring_dashpot(i, j):
    # ...
    t_disp = t_spring_disp[i, j]
    impact_vel = impact_velocity[i, j]
    impact_magnitude = select(impact_vel > 0.0, impact_vel, length(rel_vel))
    # ...
    t_spring_disp[i, j] = new_t_disp
    impact_velocity[i, j] = impact_magnitude
    is_sticking[i, j] = n_sticking
\end{lstlisting}

To provide fast and efficient \RMchange{access to} contact properties when using neighbor-lists, P4IRS keep\RMchange{s} the particles in the same indexes in both data structures.
This prevents a lookup operation within the kernel for finding the contact index, and also keeps data access contiguous for the contact history data structure.
When Linked Cells are used without a Verlet Lists, this optimization is not possible, which should significantly increase the cost for accessing contact properties in a kernel.

\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.75\textwidth]{pairs_architecture.pdf}
  \caption{\RMchange{P4IRS architecture. The user specifies the Particle Computations to be performed, which are compiled to the Python AST and further mapped to the P4IRS IR. Optimization Choices and Simulation Setup methods are used to determine which internal kernels are utilized, and the P4IRS IR is constructed in-place for the Internal Particle Optimization Kernels based on the inputs. Many Compilation Passes are performed to transform the P4IRS IR before triggering the Code Generator, which generates the Optimized Kernels, Data Structures and the PairsSimulation interface class. If whole-program generation is not used, a backend Simulation Code is required, which uses the PairsSimulation interface to call P4IRS generated kernels. The final Binary is then generated with any backend compiler of the user's choice (i.e., GCC, CLANG, ...), and it must be linked with the Runtime Library. The Runtime Library uses the P4IRS Metadata structure to handle different particle data structures and to adapt to different simulations.}}
  \label{fig:pairs_arch}
\end{figure*}

\subsection{\RMchange{Intermediate Representation}}

\RMchange{This and the following sections focus on describing the architecture from the P4IRS compiler.
\autoref{fig:pairs_arch} depicts a general overview of the P4IRS architecture from the time the user input is given to the time the final binary is generated.

An important aspect of a \ac{DSL} is to be able to use domain knowledge for its compiler transformations, thus enabling code optimizations that may not be detected by general purpose compilers.
In order to achieve this, P4IRS provides its own \ac{IR} which can represent elements that are specific to particle computations.
Note that in this paper we use the term \ac{IR} to denominate the internal data structure utilized to represent the parsed code.
Our \ac{IR} is not formally defined, as is usually the case for most \ac{IR}s from general-purpose compilers, and it is not our intention here to provide a formal and rigorous definition for it.
Instead, we present an informal and rather simple \ac{AST} representation that provide elements associated with particle computations and data structures.
These particle-specific elements in the AST can then be used in P4IRS compilation passes, and at some point during the compilation are lowered, resulting in the low-level AST that corresponds to the code implementation that matches the strategies given by the user.}

\begin{lstlisting}[language=Python,
                   label={lst:build_neigh},
                   caption={Neighbor-lists intermediate representation construction in P4IRS. Elements that are explicitly built for the AST (both high and low level) are shown in purple, and implicit elements such as expressions are constructed via operator-overloading and bound to a statement at some point in the construction.}]
class BuildNeighborLists(Lowerable):
  # ...
  @pairs_device_block
  def lower(self):
    # ...
    sim.module_name("build_neighbor_lists")
    sim.check_resize(sim.neighbor_capacity, numneighs)

    for i in ParticleFor(sim):
      for shape in range(sim.max_shapes()):
        Assign(sim, numneighs[i][shape], 0)

    for interaction_data in ParticleInteraction(sim, 2, cutoff_radius, use_cell_lists=True):
      i = interaction_data.i()
      j = interaction_data.j()
      shape = interaction_data.shape()

      nb_start = sum([numneighs[i][s] for s in range(shape)], 0)
      n = numneighs[i][shape]
      Assign(sim, neighbors[i][nb_start + n], j)
      Assign(sim, numneighs[i][shape], n + 1)
\end{lstlisting}

\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.75\textwidth]{pairs_ir.pdf}
  \caption{\RMchange{P4IRS IR for the BuildNeighborLists example. Red boxes represent elements that are specific to particle simulations, and are used for domain-specific transformations in the \ac{AST}, at some point these elements are lowered to generic, primitive elements that should be straightforward to map to code in a general-purpose language or even an IR such as the LLVM IR. Green boxes represent some of these lower level, generic elements. Blue boxes represent symbols from the simulation, and the dashed red lines show when these symbols come from a particle-specific element. Yellow boxes represent constants, and the two ``s'' elements represent patterns that are repeated in the constructed \ac{AST} according to the referenced loop in their respective orange boxes.}}
  \label{fig:pairs_ir}
\end{figure*}


\RMchange{The P4IRS IR can be constructed in two ways: (a) by mapping the Python IR generated with the \emph{compile()} method on Python functions, and (b) by explicitly constructing the IR elements in-place.
The former is used for custom particle interactions (such as the \emph{lennard\_jones} method from \autoref{lst:pairslj}) and the latter is normally used internally by P4IRS to construct the IR for internal routines.}
\autoref{lst:build_neigh} displays an example for constructing the intermediate representation that generates the code that builds the neighbor-lists for each particle in the system.
The \emph{BuildNeighborLists} class is a separate, high-level element from the AST in P4IRS.
During the compilation process, P4IRS performs analyses and transformations to lower this element by using its \emph{lower} method, and decorators such as the \emph{@pairs\_device\_block} are used to determine the target hardware where the code must be executed.

\autoref{fig:pairs_ir} depicts a schematic diagram of the constructed P4IRS \ac{IR} for the \emph{BuildNeighborLists} example.
\RMchange{Note the domain-specific elements in the IR, such as \emph{ParticleInteraction} and \emph{ParticleFor}; as discussed earlier, these can be used during compilation passes to gain domain knowledge, and at some point they are lowered according to the user-specified strategies.
For instance, the \emph{ParticleInteraction} abstract element can be lowered to a code representation that traverses pairs of particles using either the Linked Cells or Verlet Lists data structures.}
In this case, the \emph{ParticleInteraction} is lowered by using the linked-cells data structure to build the neighbor-lists as is done in traditional implementations.
The interaction object also contains methods to access the current particle (i), the neighbor particle (j) and the shape for the neighbor particle (shape).
The latter is particularly important for the shape-partitioning and multi-kernel code generation discussed in the \nameref{sec:shape_partitioning} section.

Other primitive elements such as assignments (Assign) and array accesses, as well as subexpressions, can be observed in \autoref{lst:build_neigh}.
These are simple AST nodes which are not lowered during the compiler analyses and transformations, but can be in some cases optimized away.
Note that Python artifacts can help construct the IR such as the iteration loop at line 10, as well as the summation at line 18.
These are not directly built into the P4IRS AST, but rather executed during the lowering process to construct the list of assignments for all shapes and the summation of all array-access elements constructed in-place, respectively.

\subsection{Backend and Code Generation}
\label{sec:backend}

\RMchange{
The final step of the P4IRS compiler is the code generation, which converts the optimized \ac{IR} into low-level hardware assembly instructions or to another high-level language (source-to-source compilation).
In its current state, P4IRS generates C++ and CUDA code with MPI for distributed memory parallelism, but new backend languages, intermediate representations and technologies can be included without difficulties by extending the code generator.
Although most of the code generation is straightforward for the \ac{IR} at its lowest level, there are important aspects that need to be considered to handle different backends and achieve performance portability.

P4IRS creates ``modules'' for one or more routines when building its IR}, and every module determines a point \RMchange{at which data must be synchronized for the target on which it will be executed.}
For example, the code generated from the constructed IR in the \emph{lower} method from \emph{BuildNeighborLists} is preceded by the appropriate copy calls into the GPU for all the arrays and properties \RMchange{referenced} within the IR subtree.
To avoid complex control-flow analysis to determine which properties and arrays actually have to be copied (since some of them can already be synchronized with the device), we provide bitmasks to represent each array and property defined in the simulation, and provide methods to check every array and property used within the intermediate representation.
Only when the bit is not set in the device, the copy method for the respective array or property is called.
Also, when properties or arrays are changed within the subtree, specific code is generated to clean up the respective bits.

\RMchange{In addition,} the \emph{check\_resize} (see line 7 at \autoref{lst:build_neigh}) method is used to specify which array sizes can grow during the module execution, and which variable or array is used as their current capacity.
In this case, the \emph{neighbor\_capacity} can be increased based on the \emph{numneighs} array, so when one particle contains more neighbors than \emph{neighbor\_capacity}, a proper code is generated to reallocate the arrays that contain the referred capacity as one of its dimension sizes.
The compiler handles the reallocation cases when the target is an accelerator such as a GPU, which needs the kernel to be re-executed after arrays are reallocated since it is not possible to reallocate them directly in the accelerator.
P4IRS also takes care of avoiding memory access violation in the current kernel execution by wrapping the accesses around branches which assure the accessed indexes are smaller than the current capacity of the arrays.

\RMchange{When} the IR is constructed for a specific routine, P4IRS then focus\RMchange{es} on determining which loops will be scheduled to the GPU.
As a general case, every outer-most loop in a block that uses the \emph{@pairs\_device\_block} decorator is a candidate to be replaced by a kernel launch to be run in parallel.
In \autoref{lst:build_neigh}, both loops at lines 9 and 13 are candidates and their body instructions are then wrapped around a separate GPU kernel.
The loops are then replaced by a kernel launch statement for the new generated kernel.
Note that some candidates may not be offloaded to the GPU, the reason is that P4IRS can use internal heuristics to determine whether a loop should or should not be executed in the GPU.
Domain-knowledge can also be used in the GPU offloading analyses since elements like \emph{ParticleFor} and \emph{ParticleInteraction} can be used during the compiler passes before they are lowered.

\subsection{Runtime \RMchange{Library}}

Apart from the kernel implementations and functions to prepare the kernel launches and move data between host and accelerator processors, particle simulations also may need to call external libraries and deal with filesystem I/O.
A few case examples where this is required are, for instance, to write particle data to a file in a visualization format like VTK, and reading the initial state of the particle system to setup the simulation.
Since most of these procedures do not require extensive optimizations as they are not compute-intensive and their bottleneck is an external I/O device (such as a disk), building up the intermediate representation for them and extending the code generator to support such external calls is simply not worthwhile.

To handle this, P4IRS provides a runtime library which contains such functionalities directly implemented in C++, and these make use of and synchronize (when necessary) the data structures from the generated simulation.
To access the simulation data, an instance (referred to as \emph{PairsRuntime} object) contains all the meta-data for the simulation, and can be used in these implemented routines.

\begin{lstlisting}[language=C++,
		   label={lst:pairsruntime},
	   	   caption={P4IRS runtime routine example for writing VTK data into a file.}]
void vtk_write_data(
  PairsRuntime *ps, const char *filename,
  int start, int end, int timestep) {

  std::string output_filename(filename);
  auto position = ps->getAsVectorProperty(
    ps->getPropertyByName("position"));
  auto dom_part = ps->getDomainPartitioner();
  // ...
  filename_oss << filename << "_";
  if(dom_part->getWorldSize() > 1) {
    filename_oss << dom_part->getRank() << "_";
  }
  // ...
  filename_oss << timestep << ".vtk";
  std::ofstream out_file(filename_oss.str());
  // ...
  ps->copyPropertyToHost(position);
  for(int i = start; i < end; i++) {
    out_file << position(i, 0) << " ";
    out_file << position(i, 1) << " ";
    out_file << position(i, 2) << "\n";
  }
  // ...
}
\end{lstlisting}

\autoref{lst:pairsruntime} shows an example for the routine to write data into VTK files, note that the first parameter is a \emph{PairsRuntime} object, and can be used to check whether properties exist, check if they are synced with the host device and (in case not) copy the data from the host.
The meta-data object is \RMchange{fed} at the start of the simulation with the code generated by P4IRS, and it is therefore dependent on each simulation.
It is important to enhance that accessing simulation data through the \emph{PairsRuntime} instance is not as efficient as accessing \RMchange{it} directly in the generated code since the way of accessing the data structures (such as the property layouts) must be evaluated during runtime.
Therefore, the use of such \RMchange{an} instance \RMchange{should} be avoided for performance-intensive routines.

\RMchange{The runtime library must be linked to the generated binary during execution, so it must be properly built for the target hardware and platform, and either installed on the target system or its path must be included in the list of paths where dynamic libraries are searched for (for example, the \emph{LD\_LIBRARY\_PATH} environment variable on Linux systems).
Although it is compiled separately from the generated code and should ideally be linked dynamically at runtime, it is possible to compile both together and statically link the runtime library code with the simulation code, thus generating a stand-alone, platform-specific binary that does not require further linking with the runtime code.}

\subsection{Shape Partitioning and Multi-Kernel}
\label{sec:shape_partitioning}

When computing forces for DEM simulations that contain different shapes, different calculation formulas are needed for some of the contact elements such as the penetration depth, contact point and distance.
From a computational point of view, this introduces branching in the inner-most loop where each path leads to different arithmetic instructions be executed and therefore inhibits SIMD vectorization.
Algorithm \ref{alg:branching} displays an example of such issue for a kernel with spheres and half-spaces.
Note that when traversing the neighbors, their shapes are checked to determine which code path is taken during the interaction, hence the code is not SIMD-friendly.

\begin{algorithm}[H]
  \caption{Example kernel with branching for different shapes.}
  \label{alg:branching}
  \begin{algorithmic}[1]
    \For{$i$ \textbf{in} \texttt{sphere\_particles()}}
      \For{$j$ \textbf{in} \texttt{neighbors($i$)}}
        \If{\texttt{shape($j$) == SPHERE}}
          \State \texttt{// Sphere-Sphere}
        \EndIf
        \If{\texttt{shape($j$) == HALFSPACE}}
          \State \texttt{// Sphere-Halfspace}
        \EndIf
        \State \texttt{// ...}
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

To provide a simpler way to achieve vectorized code for such DEM kernels, \RMchange{it is possible to} partition particles by shape in the acceleration data structures (Linked Cells and Verlet Lists), which allows distinguishing them during the force computation.
After, \RMchange{multiple loops can be used} in the code for handling different pair of shapes, \RMchange{and} \RMchange{vectorizing} these resulting loops should be easier since no branches are needed to calculate different contact elements for the particles being computed.
Algorithm \ref{alg:no_branching} displays the same kernel as Algorithm \ref{alg:branching} without the branching issue.
Note that the outer-most loop can also be split into two parts as well, which leads to two different kernels for each pair of shapes.
For GPUs, this can be beneficial since different threads will be used for different pair of shapes (parallelism is done at the level of the outer-most loop) and no threads will be disabled due to traversing a different code path.

\begin{algorithm}[H]
  \caption{Example kernel without branching for different shapes.}
  \label{alg:no_branching}
  \begin{algorithmic}[1]
    \For{$i$ \textbf{in} \texttt{sphere\_particles()}}
      \For{$j$ \textbf{in} \texttt{neighbor\_spheres($i$)}}
        \State \texttt{// Sphere-Sphere}
      \EndFor
      \For{$j$ \textbf{in} \texttt{neighbor\_halfspaces($i$)}}
        \State \texttt{// Sphere-Halfspace}
      \EndFor
      \State \texttt{// ...}
    \EndFor
  \end{algorithmic}
\end{algorithm}

Note that there is no guarantee that the internal loop will be vectorized for CPU, since there may still be branches and instructions in the force kernel that make it not possible.
Besides, the backend compiler must be able to properly analyze and apply the transformations to actually generate the vectorized code to use AVX/SSE instructions, which may not happen even when the code is amenable to vectorization.

\RMchange{P4IRS is able to generate this algorithmic optimization automatically for the shapes involved in the simulation.
This is possible by using the domain knowledge provided in the AST, where the \emph{ParticleInteraction} symbol can be lowered to several loops that handle different pairs of shapes in the simulation.
Note that P4IRS does not require complex and sophisticated analyses to identify these opportunities for splitting the interaction loop because its \ac{IR} already exposes the domain knowledge necessary for applying this strategy.
As discussed, this is an important benefit of using a \ac{DSL} because the \ac{IR}s from general-purpose compilers need to work at a lower level in order to be able to represent any generic code.}

Apart from splitting the inner-most loop, P4IRS makes it possible to split the outer-most loop and generate two distinct kernels, each one computing different types of interactions (in this case, Sphere-Sphere and Sphere-Halfspace variants), we name this strategy here as ``Multi-Kernel partition''.
This requires data from the left-side particles to be loaded again, which is not necessarily a significant problem since they are contiguous in memory.
Besides exposing more SIMD opportunities, this can also lead to better memory accesses from the right-side particle properties when there are not many particles of a specific shape in the simulation (such as the half-space obstacles).
The reason is the properties for these obstacles are accessed separately from other particle properties and should be closer in the memory hierarchy in subsequent accesses since there should be no cache pollution from evaluating and computing interactions with other sphere neighbors in the same kernel.

\subsection{Neighbor-lists per Cell}
\label{sec:neighbor_lists_per_cell}

When computing particle interactions with the Linked Cells algorithm, it is necessary to iterate at every surrounding cell and then every particle that belongs to them.
This introduces a nested-loop, and the SIMD execution becomes restricted to the inner-most loop (i.e. particles within the same neighbor cell).
To mitigate this and provide more SIMD opportunities, it is possible to create a neighbor-list for each cell (instead of for each particle as in the Verlet Lists algorithm), and then merge the two loops into a single one that iterates over the neighbor list.
In this way, the force for each particle is computed by traversing the neighbor list constructed for the cell it belongs to, without affecting the amount of interactions computed.

Note that this introduces a new kernel to build the neighbor-lists, and this is only beneficial when the force calculation performance improvement compensates the time spent in the new kernel.
Besides, in cases where the code is not properly vectorized (especially in CPU where explicit instructions must be generated), the improvement may not be substantial.
In GPUs, however, this should lead to better execution since threads are not halted waiting for the surrounding neighbor cells to be computed when the amount of particles among them is different.

\subsection{Domain Partitioning and Communication}
\label{sec:domain_partitioning}

In order to attain distributed-memory parallelism and software flexibility while attaining good performance, P4IRS provides a customizable and robust communication module that facilitates integrating new domain partitioning schemes into its generated simulations.
This is done by combining its runtime functions and code generation techniques, and allows the usage of the waLBerla domain-partitioning scheme based on a block-forest data structure.

Essentially, P4IRS communication is split into three routines:

\begin{itemize}
    \item \emph{borders:} define particles within the halo region which must be send to neighbor ranks as ``ghost'' particles; only non-volatile properties that are accessed in a neighbor particle within a kernel are sent.
    \item \emph{synchronize:} uses the defined particle in the borders routine to exchange data; only non-volatile properties that are accessed in neighbor particles within a kernel and that change across time-steps are sent.
    \item \emph{exchange:} send particles that overlap the current rank's domain, it becomes a local particle in the neighbor rank; all properties except volatiles are sent; contact history data is also exchanged.
\end{itemize}

These methods make use of a domain-partitioner class that can be implemented in P4IRS runtime to determine which particles must be sent.
The method is implemented in C++ and fill-in domain buffers that are used for evaluating if a particle is outside the domain or within the border using its position.
This means that to include new domain partitioning schemes in P4IRS, it is only necessary to define a C++ runtime class, which can make use of external libraries and therefore can use the waLBerla block-forest domain-partitioning.
Note that the reason for using buffers instead of evaluating the particle positions directly in the runtime code is performance, since particle packing and unpacking kernels should be executed in the target processor to achieve parallelism and to avoid copying data back and forth from the accelerator and host processors.

\section{Evaluation}
\label{sec:evaluation}

% Testbed
%   - Ice Lake, Milan, A40, A100
%   - Fritz: Intel(R) oneAPI DPC++/C++ Compiler 2022.1.0 (2022.1.0.20220316)
%     - -Ofast -xCORE-AVX512 -qopt-zmm-usage=high (Ice Lake)
%   - Alex: icpc (ICC) 2021.6.0 20220226
%     - -Ofast -xHost -qopt-zmm-usage=high (Milan) 

\subsection{Materials and Methods}

In this paper we conducted experiments to evaluate the base performance of P4IRS generated codes in the following processors and accelerators:

\begin{itemize}
  \item \textbf{Ice Lake:} Intel Xeon Platinum 8360Y “Ice Lake” processors (36 cores per chip) running at a base frequency of 2.4 GHz and 54 MB Shared L3 cache per chip, 256 GB of DDR4 RAM.
  \item \textbf{Milan:} AMD EPYC 7713 “Milan” processors (64 cores per chip) running at 2.0 GHz with 256 MB Shared L3 cache per chip
  \item \textbf{A40:} Nvidia A40 GPU with 48 GB DDR6 @ 696 GB/s; 37.42 TFlop/s in FP32.
  \item \textbf{A100:} Nvidia A100 GPU with 40 GB HBM2 @ 1,555 GB/s; HGX board with NVLink; 9.7 TFlop/s in FP64 or 19.5 TFlop/s in FP32.
\end{itemize}

And we also perform experiments to evaluate P4IRS weak-scalability in the following clusters:

\begin{itemize}
  \item \textbf{Fritz:} Each compute node has two Ice Lake processors; 256 GB of DDR4 RAM; Blocking HDR100 Infiniband with up to 100 GBit/s bandwidth per link and direction.
  \item \textbf{JUWELS Booster:} Each compute node has two AMD EPYC ``Rome'' 7402 CPU, with 48 cores (simultaneous multi-threading) at 2.8 GHz; 512 GB of DDR4 memory at 3200 MHz; Four Nvidia A100 GPU with 40 GB HBM2e memory; Four InfiniBand HDR (Connect-X6) interconnect channels.
\end{itemize}

\begin{table*}[htb]
    \centering
    \begin{tabular}{c|c|c}
%        \hline
        Target & Compiler & Flags \\
        \hline
        Ice Lake & ICC 2021.10.0 & \emph{-Ofast -xCORE-AVX512 -qopt-zmm-usage=high} \\
        Milan & ICC 2021.10.0 & \emph{-Ofast -march=core-avx2} \\
        A40 & NVCC 12.1.r12.1 & \emph{-Ofast -march=core-avx2} \\
        A100 (single GPU) & NVCC 12.1.r12.1 & \emph{-Ofast -march=core-avx2} \\
        A100 (JUWELS Booster) & NVCC 12.2.91 & \emph{-Ofast -march=core-avx2} \\
        Rome (JUWELS Booster) & ICX 2023.2.0 & \emph{-Ofast -march=core-avx2} \\
%        \hline
    \end{tabular}
    \caption{\RMchange{Compiler and flag specifications used for the experiments in this paper.}}
    \label{tab:compilers}
\end{table*}

\autoref{tab:compilers} \RMchange{displays the compiler and flags used for the experiments in this paper.}
The icc (ICC) 2021.10.0 20230609 compiler was used for CPU experiments (except in the JUWELS cluster), with the \emph{-Ofast -xCORE-AVX512 -qopt-zmm-usage=high} flags for Fritz / Ice Lake (AVX512) and \emph{-Ofast -march=core-avx2} flags for Milan (AVX2).
For CUDA, we use the NVCC compiler V12.1.r12.1 for the single GPU experiments, and the same flags used for Milan (AVX2) since it is the host processor in the systems with the A40/A100 GPUs.
In the JUWELS Booster, we used the Intel(R) oneAPI DPC++/C++ \RMchange{"ICX"} Compiler 2023.2.0 (2023.2.0.20230721) \RMchange{as a backend compiler for} NVCC V12.2.91, and the same flags used for Milan since both CPUs have AVX2 with FMA as the largest SIMD vector extension available.
We make use of the Likwid toolkit V5.3 (\cite{likwid}) to pin tasks to specific CPU cores and measure HPM counters in the target CPU (Ice Lake) for the single core experiments, where P4IRS is able to properly generate the instrumentation markers and MD-Bench kernels are already instrumented.

Experiments for \ac{MD} use the standard Copper FCC lattice (see \autoref{fig:copper_fcc_lattice}) test-case from miniMD/MD-Bench, and we use our results from MD-Bench as a performance reference since it is a well optimized code for \ac{MD}.
The \ac{DEM} test case is a ``Settling Spheres'' benchmark (see \autoref{fig:settling_spheres}), on which several spheres fall into a ``ground'' (half-space), thus generating a ``bed'' of spherical particles.
Due to its multiple shapes and dynamics that consists of many interactions among spheres and half-spaces, it should provide a good reference for DEM simulations.
We compare our single-core results from P4IRS with MESA-PD using 2 MPI ranks since it is not possible to run this setup in MESA-PD using a single rank.
GPU results are not available for MESA-PD as it is not capable of generating GPU code, but we show GPU performance results for P4IRS comparing different code generation strategies.

We also show weak-scaling results to demonstrate P4IRS scaling capabilities.
For this we use regular domain-partitioning since these experiments do not require sophisticated load-balancing algorithms (Copper lattice is an homogeneous test-case in all dimensions, and Settling Spheres can obtain acceptable weak-scaling by regularly partitioning the domain in X and Y dimensions).
We use double floating-point precision and display results when enabling and disabling Newton's 3rd Law optimization (half neighbor-lists).
For \ac{MD} simulations, we show results with the Verlet Lists optimization strategy since it is widely employed in the field, while for \ac{DEM} we display results with the Linked Cells data-structure because particles represent rigid bodies that need to be in contact for their contributions to be computed.
Since the amount of contacts is typically way less than atomic interactions, there are no significant advantages of using the Verlet Lists data structure.

\begin{figure*}[tb]
    \centering
    \subfigure[t = 0]{\label{fig:md_ts0}\includegraphics[width=0.32\textwidth]{md_0.png}}
    \subfigure[t = 100]{\label{fig:md_ts100}\includegraphics[width=0.32\textwidth]{md_100.png}}
    \subfigure[t = 200]{\label{fig:md_ts200}\includegraphics[width=0.32\textwidth]{md_200.png}}
    \caption{The MD simulation example used in this paper, which is a Copper FCC lattice simulation with homogeneous distribution. Forces are computed with the van der Waals potential and PBC is used in all directions. In average, every atom contains roughly 76 neighbors through the whole simulation (200 time-steps).}
    \label{fig:copper_fcc_lattice}
\end{figure*}

\begin{figure*}[tb]
    \centering
    \subfigure[t = 0]{\label{fig:dem_ts0}\includegraphics[width=0.48\textwidth]{dem_0.png}}
    \subfigure[t = 2000]{\label{fig:dem_ts2000}\includegraphics[width=0.48\textwidth]{dem_2000.png}}
    \vskip\baselineskip
    \subfigure[t = 4000]{\label{fig:dem_ts4000}\includegraphics[width=0.48\textwidth]{dem_4000.png}}
    \subfigure[t = 10000]{\label{fig:dem_ts10000}\includegraphics[width=0.48\textwidth]{dem_10000.png}}
    \caption{The DEM simulation example used in this paper which can be called ``Settling Spheres'' or ``Bed Generation''. Spherical particles fall into the ground (represented by a half-space) to generate a bed at the end of the simulation. Forces are computed for gravity, buoyancy and the Linear Spring-Dashpot method to achieve the desired result and PBC is used in the X and Y dimensions.}
    \label{fig:settling_spheres}
\end{figure*}

\subsection{Molecular Dynamics}

% MD
%   - Single core/GPU (vs MD-Bench) --- one figure for CPU and one for GPU
%     - Full and half-neighbor lists
%     - Runtime, MFLOP/s, Vectorization ratio, # instructions, ... (table)
% Weak-scaling
%   Fritz (figure) - base = 128x128x128 per node
%     - Full and half-neighbor lists
%   Alex (figure)
%     - Full and half-neighbor lists
%     - A40 and A100

For \ac{MD} case, we use the Verlet Lists algorithm as it is the standard one for such simulations.
The simulations runs through 200 time-steps, and the neighbor-lists are rebuilt at every 20 time-steps of the simulation (and at the start).
We use the simple \ac{LJ} potential (as shown in \autoref{lst:pairslj}) to calculate the force contributions among atoms.

\colorlet{force-cpu}{teal!80}
\colorlet{neigh-cpu}{teal!50}
\colorlet{other-cpu}{teal!20}

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.73]
    \pgfplotsset{
        ybar stacked, ymin=0, ymax=32, xmin=0.5, xmax=2.5, xtick=data,
        xtick={1,...,2},
        xticklabels={Ice Lake, Milan},
        xticklabel style={yshift=-10pt},
        ylabel={time to solution (s)}, ylabel style={yshift=-1ex},
        legend cell align={left},
        /pgf/bar width=12pt,% bar width
        scatter/position=absolute,
        node near coords style={
            font=\footnotesize,
            at={(axis cs:\pgfkeysvalueof{/data point/x},\pgfkeysvalueof{/pgfplots/ymin})},
            anchor=north,
            yshift={-\pgfkeysvalueof{/pgfplots/major tick length} + 4pt},
        },
    }
    \begin{axis}[bar shift=-18pt, nodes near coords style={xshift=-18pt},
        legend pos = outer north east, legend style = {name = pairs}]
        \addplot [fill=force-cpu, nodes near coords=A] table [x=arch, y=pairsfn] {data/single-core-force.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=pairsfn] {data/single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=pairsfn] {data/single-core-other.csv};
        \legend{Force, Neigh, Other}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{A:} P4IRS (FN)}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{B:} MD-Bench (FN)}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{C:} P4IRS (HN)}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{D:} MD-Bench (HN)}
    \end{axis}
    \begin{axis}[bar shift= -6pt, nodes near coords style={xshift=-6pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-cpu, nodes near coords=B] table [x=arch, y=mdbenchfn] {data/single-core-force.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=mdbenchfn] {data/single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=mdbenchfn] {data/single-core-other.csv};
    \end{axis}
    \begin{axis}[bar shift= 6pt, nodes near coords style={xshift=6pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-cpu, nodes near coords=C] table [x=arch, y=pairshn] {data/single-core-force.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=pairshn] {data/single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=pairshn] {data/single-core-other.csv};
    \end{axis}
    \begin{axis}[bar shift= 18pt, nodes near coords style={xshift=18pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-cpu, nodes near coords=D] table [x=arch, y=mdbenchhn] {data/single-core-force.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=mdbenchhn] {data/single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=mdbenchhn] {data/single-core-other.csv};
    \end{axis}
\end{tikzpicture}
\vspace{-3ex}
	\caption{Execution time for P4IRS and MD-Bench simulations in seconds (lower is better) of a Copper FCC lattice system with $32^{3}$ unit-cells (4 atoms per unit-cell) during 200 time-steps on Ice Lake and Milan CPUs. Tests were performed in a single core (no parallelism) with fixed frequency. Results are shown for both full and half neighbor-lists (FN/HN).}
\vspace{-2ex}
\centering
\label{fig:md_single_core_runtimes}
\end{figure}

\autoref{fig:md_single_core_runtimes} display the runtime measurements for P4IRS and MD-Bench simulations of the Copper FCC lattice case with $32^{3}$ unit cells (4 atoms per unit-cell).
It is noticeable that the kernels generated by P4IRS achieve competitive results in comparison to MD-Bench, a hand-tuned implementation for MD cases, thus demonstrating P4IRS capabilities to generate fast code.
For Ice Lake, performance is slightly better for MD-Bench using \ac{FN}, with about 10\% speedup for the force calculation but 25\% worse performance for the neighbor-lists creation.
When using \ac{HN}, the force calculation time is roughly the same for both versions, but P4IRS still keeps the advantage when building the neighbor-lists, resulting in a 4\% better performance for P4IRS in the overall runtime.
In general, runtime differences for the whole simulation vary in the range of about 5-6\% in Fritz, which is not so significant and may not be the case when using different compilers and flags.

In Milan, P4IRS is faster than MD-Bench for the \ac{FN} case, and the advantage also comes from the neighbor-lists creation.
For \ac{HN}, results are similar, but it is possible to see that P4IRS still builds the neighbor-lists slightly faster, where MD-Bench has a moderately better performance for the force computation.
Overall, we can assume that P4IRS is able to achieve good base performance when running in a single CPU core, while providing a more flexible and high-level description of the problem which is not the case for the hand-tuned and hard-coded C implementation from MD-Bench.

\begin{table*}[htb]
    \centering
    \begin{tabular}{c|c|c|c|c}
%        \hline
        Variant & pairs-fn & pairs-hn & mdbench-fn & mdbench-hn \\
        \hline
        Runtime [\seconds] & 23.26 & 12.36 & 20.89 & 12.25 \\
        AVX512 Perf. [\GFS] & 2.866 & --- & 3.192 & --- \\
        Scalar Perf. [GUOPS/s] & --- & 1.777 & --- & 1.794 \\
        CPI & 3.597 & 0.8292 & 2.9961 & 0.7211 \\
        Total instructions $(\times 10^9)$ & $15.442$ & $41.668$ & $16.666$ & $40.587$ \\
        FP instructions $(\times 10^9)$ & $8.416$ & $21.983$ & $8.416$ & $21.985$ \\
        Arithmetic ratio [\%] & 54.49 & 52.75 & 50.50 & 54.16 \\
        Vectorization ratio [\%] & 99.0574 & 0.0 & 99.0609 & 0.0 \\
%        \hline
    \end{tabular}
    \caption{Measurements from P4IRS and MD-Bench Lennard-Jones force kernels for both full and half neighbor-lists cases on a single Ice Lake CPU core.}
    \label{tab:md_single_core_stats}
\end{table*}

For a deeper evaluation of both versions for the \ac{MD} case, \autoref{tab:md_single_core_stats} displays HPM measurements for the force calculation kernels in Ice Lake with AVX512.
The important thing to notice is that the compiler was not able to vectorize the cases with \ac{HN}, but the runtimes are still smaller for these cases.
The arithmetic instructions ratio for all cases lie between 50-55\%, which means that about half of the instructions are used for organizing data into the vector registers.
For more computational-intensive kernels, the arithmetic ratio should increase and the measured AVX512/Scalar performance should have a more significant impact in the runtime.

The performance with \ac{HN} is significantly worse but it is compensated by the fact it computes roughly half the interactions.
Another important thing to consider is the introduction of the Intel patch to mitigate the Downfall vulnerability (also known as Gather Data Sampling), which significantly affects the performance of the gather instructions in the Intel CPUs and therefore affect the \ac{FN} cases performance with AVX512 vectorization.
In particular, the separation of concerns introduced by P4IRS together with code generation techniques can be a first step on dealing with such issues by avoiding gather instructions, which can be done by producing explicit SIMD intrinsics to gather the data with separate instructions to load and then organize (through permutation, shuffles and swizzles) it into the vector registers, or using different algorithms such as the Cluster Pair from GROMACS, which keeps particles data in an array-of-struct-of-arrays (AoSoA) fashion, thus requiring only simple load instructions to build-up the vector registers.

\colorlet{force-gpu}{purple!80}
\colorlet{neigh-gpu}{purple!50}
\colorlet{other-gpu}{purple!20}

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.73]
    \pgfplotsset{
        ybar stacked, ymin=0, ymax=1.5, xmin=0.5, xmax=2.5, xtick=data,
        xtick={1,...,2},
        xticklabels={A40, A100},
        xticklabel style={yshift=-10pt},
        ylabel={time to solution (s)}, ylabel style={yshift=-1ex},
        legend cell align={left},
        /pgf/bar width=12pt,% bar width
        scatter/position=absolute,
        node near coords style={
            font=\footnotesize,
            at={(axis cs:\pgfkeysvalueof{/data point/x},\pgfkeysvalueof{/pgfplots/ymin})},
            anchor=north,
            yshift={-\pgfkeysvalueof{/pgfplots/major tick length} + 4pt},
        },
    }
    \begin{axis}[bar shift=-12pt, nodes near coords style={xshift=-12pt},
        legend pos = outer north east, legend style = {name = pairs}]
        \addplot [fill=force-gpu, nodes near coords=A] table [x=arch, y=pairsfn] {data/single-gpu-force.csv};
        \addplot [fill=neigh-gpu] table [x=arch, y=pairsfn] {data/single-gpu-neigh.csv};
        \addplot [fill=other-gpu] table [x=arch, y=pairsfn] {data/single-gpu-other.csv};
        \legend{Force, Neigh, Other}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{A:} P4IRS (FN)}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{B:} MD-Bench (FN)}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{C:} P4IRS (HN)}
    \end{axis}
    \begin{axis}[bar shift= -0pt, nodes near coords style={xshift=-0pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-gpu, nodes near coords=B] table [x=arch, y=mdbenchfn] {data/single-gpu-force.csv};
        \addplot [fill=neigh-gpu] table [x=arch, y=mdbenchfn] {data/single-gpu-neigh.csv};
        \addplot [fill=other-gpu] table [x=arch, y=mdbenchfn] {data/single-gpu-other.csv};
    \end{axis}
    \begin{axis}[bar shift= 12pt, nodes near coords style={xshift=12pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-gpu, nodes near coords=C] table [x=arch, y=pairshn] {data/single-gpu-force.csv};
        \addplot [fill=neigh-gpu] table [x=arch, y=pairshn] {data/single-gpu-neigh.csv};
        \addplot [fill=other-gpu] table [x=arch, y=pairshn] {data/single-gpu-other.csv};
    \end{axis}
\end{tikzpicture}
\vspace{-1ex}
	\caption{Execution time for P4IRS and MD-Bench simulations in seconds (lower is better) of a Copper FCC lattice system with $50^{3}$ unit-cells (4 atoms per unit-cell) during 200 time-steps on A40 and A100 GPUs. Results are shown for both full and half neighbor-lists (FN/HN) cases, and atomic operations affect the performance for the HN case.}
\vspace{-2ex}
\centering
\label{fig:md_single_gpu_runtimes}
\end{figure}

\autoref{fig:md_single_gpu_runtimes} displays the runtime measurements for P4IRS and MD-Bench simulations in the A40 and A100 GPUs.
MD-Bench still do not have a GPU version with \ac{HN}, so we just display it using \ac{FN}.
For A40, MD-Bench shows a slightly better performance (about 3\%) for computing the forces while P4IRS with \ac{FN} also provides slightly better performance (about 7\%) for building the neighbor-lists.
In the overall performance, MD-Bench is faster only by about 2\% in comparison to P4IRS for the \ac{FN} case, which corroborates our claim that P4IRS is able to generate efficient GPU code.
For the \ac{HN} case, the performance is not even close to a factor of two as one would expect, which can be attributed to the usage of atomics, and this can be different for more arithmetic-intensive kernels (the \ac{LJ} kernel used has significantly less arithmetic instructions compared to realistic cases) since the overall amount of operations reduced should be larger.
For A100, however, the performance difference is larger, where MD-Bench performance is about 14\% faster in comparison to P4IRS.
When using the \ac{HN}, however, we achieve very close performance to MD-Bench with \ac{FN}, and it is noticeable that the speedup is very far from the factor of two as well.
Despite the observed performance divergence in A100, it is possible to see that P4IRS code still has good performance in comparison to our hand-tuned CUDA implementation, supporting our claim in generating performance-portable kernels.

\begin{table*}[htb]
    \centering
    \begin{tabular}{c|c|c|c|c}
%        \hline
        Variant & pairs-fn & mdbench-fn & pairs-hn & mdbench-hn \\
        \hline
        Ice Lake & 0.962 & 1.019 & 1.662 & 1.595 \\
        Milan & 0.908 & 0.883 & 1.354 & 1.367 \\
        A40  & 26.452 & 27.025 & 29.688 & --- \\
        A100 & 75.113 & 87.381 & 87.968 & --- \\
%        \hline
    \end{tabular}
    \caption{\RMchange{Summary of performance results (in millions of particle updates per second) for each P4IRS and MD-Bench simulation run of the MD benchmark on a single core of the Ice Lake CPU, a single core of the Milan CPU, a single A40 GPU, and a single A100 GPU.}}
    \label{tab:md_particle_updates_per_second_md}
\end{table*}

\RMchange{\autoref{tab:md_particle_updates_per_second_md} summarizes the results in million particle updates per second, running on a single CPU core and on a single GPU for the \ac{MD} case.
Note that our goal is not to compare performance on a single CPU core to performance on a single GPU, as this is not a fair comparison.
However, we do want to show that we can achieve a significant performance improvement when running P4IRS code on a GPU, and the results are close (and in some cases better) when comparing P4IRS code to MD-Bench on all the systems listed.}

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.95]
    \tikzstyle{every node}=[font=\small]
    \begin{axis}[
            width=0.5\textwidth, height=8cm,
            xmode=log,log basis x={2},
            %xmin=0.75,   xmax=3072,
            xmin=1,   xmax=64,
            ymin=10,   ymax=30,
            log ticks with fixed point,
            xlabel={\# nodes}, xlabel style={yshift= 1ex},
            xticklabel={ % hack for precision issues with 1024 and 2048
                \pgfkeys{/pgf/fpu=true}
                \pgfmathparse{int(2^\tick)}
                \pgfmathprintnumber[fixed]{\pgfmathresult}
            },
            ylabel={time to solution (s)}, ylabel style={yshift=-1ex},
            grid=major,
            legend pos=north east,
            %legend style={at={(0.97,0.5)},anchor=east},
        ]
        \addplot table[x=nodes,y=pairsfn] {data/md-scaling-fritz.csv};
        \addplot table[x=nodes,y=pairshn] {data/md-scaling-fritz.csv};
        \legend{P4IRS-FN, P4IRS-HN}
    \end{axis}
\end{tikzpicture}
\vspace{-2ex}
\caption{Weak-scaling results for P4IRS (FN and HN) on Fritz, each node simulates 200 time-steps of a Copper FCC lattice system with $128^3$ unit cells, with approximately 116.508 atoms per CPU core.}
\vspace{-2ex}
\label{fig:md_weak_scaling_fritz}
\end{figure}

\autoref{fig:md_weak_scaling_fritz} shows the results for P4IRS weak-scaling in the Fritz supercomputer.
For each node, a system of $128^3$ unit cells is included, with roughly 8.388.608 atoms per node (or 116.508 atoms per CPU core).
Since the \emph{multinode} partition in Fritz has the maximum of 64 nodes (which results in 4608 CPU cores, no hyper-threading), this is the maximum amount we scale up our problem to.
It is possible to see that we achieve perfect weak-scaling in all the settings we use, and the \ac{HN} version is significantly faster than the \ac{FN} version.
Notice that atomic operations are not required because we only make use of parallelism through spatial decomposition and message communication with MPI, thus we use distributed-memory parallelism strategy even within the same node and there is no risk of race conditions.
Based on our results, it is clear that P4IRS is able to generate scalable code on modern CPU clusters, but problems that are not too homogeneous may need proper mechanisms for balancing the workload among different MPI ranks.
This is possible by using different domain-partitioning methods in P4IRS such as the waLBerla domain partitioning with block-forests, but for this example we can achieve perfect scaling even with regular domain-partitioning.

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.95]
    \tikzstyle{every node}=[font=\small]
    \begin{axis}[
            width=0.5\textwidth, height=8cm,
            xmode=log,log basis x={2},
            %xmin=0.75,   xmax=3072,
            xmin=1,   xmax=128,
            ymin=0,   ymax=5,
            log ticks with fixed point,
            xlabel={\# nodes}, xlabel style={yshift= 1ex},
            xticklabel={ % hack for precision issues with 1024 and 2048
                \pgfkeys{/pgf/fpu=true}
                \pgfmathparse{int(2^\tick)}
                \pgfmathprintnumber[fixed]{\pgfmathresult}
            },
            ylabel={time to solution (s)}, ylabel style={yshift=-1ex},
            grid=major,
            legend pos=north east,
            %legend style={at={(0.97,0.5)},anchor=east},
        ]
        \addplot table[x=nodes,y=pairsfn] {data/md-scaling-juwels.csv};
        \addplot table[x=nodes,y=pairshn] {data/md-scaling-juwels.csv};
        %\addplot table[x=nodes,y=pairsfna100] {data/md-scaling-alex.csv};
        %\addplot table[x=nodes,y=pairshna100] {data/md-scaling-alex.csv};
        \legend{P4IRS-FN, P4IRS-HN}
    \end{axis}
\end{tikzpicture}
\vspace{-2ex}
\caption{Weak-scaling results for P4IRS (FN and HN) in the JUWELS Booster GPU cluster, each node simulates 200 time-steps of a Copper FCC lattice system with $100^3$ unit cells, with approximately 500.000 atoms per GPU.}
\vspace{-2ex}
\label{fig:md_weak_scaling_juwels}
\end{figure}

\autoref{fig:md_weak_scaling_juwels} shows the weak-scaling runtimes for P4IRS in the JUWELS Booster cluster.
Each node simulates a system of $100^3$ unit cells, with roughly 500.000 atoms per GPU.
We scale up to 128 nodes, which is equivalent of 512 A100 GPUs.
Note that for the first node, the performance is slightly worse than the next ones, which can be explained as we only have 4 GPUs in one node, and domain-partitioning is only done in two dimensions.
This can lead to worse performance because the surface area for communication is likely higher for the dimension where the partitioning is not done, which culminates in more atoms to be communicated and therefore more time spent packing and transferring data.
Apart from this, the scaling is also perfect for all the settings, and it is possible to observe that the \ac{FN} version is slightly faster than \ac{HN}.
As we already discussed, the usage of \ac{HN} is not necessarily beneficial in GPUs due to atomics, and this can be worse especially for kernels which do not exhibit high arithmetic intensity.
The results demonstrate that P4IRS is in fact capable of generating scalable code for modern GPU clusters.

\subsection{Discrete Element Method}

For the \ac{DEM} case, we make use of the Linear Spring-Dashpot model to calculate the particle force contributions.
We also use the Linked Cells algorithm, as it is usually not common to use Verlet Lists for \ac{DEM} in general for contact detection, and also because we build the Linked Cells at every time-step of the simulation, which means that building the Verlet Lists do not provide any advantage.

% DEM
%   - Comparison with MESA-PD (figure + table)
%     - Linked Cells (ignore Verlet Lists)
%     - Full and half computations
%
%   - Weak-scaling on Alex (figure)
%     - CPU (128 cores) vs GPU (8)

\colorlet{force-cpu}{teal!80}
\colorlet{euler-cpu}{teal!60}
\colorlet{neigh-cpu}{teal!40}
\colorlet{other-cpu}{teal!20}

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.72]
    \pgfplotsset{
        ybar stacked, ymin=0, ymax=5, xmin=0.5, xmax=2.5, xtick=data,
        xtick={1,...,2},
        xticklabels={Ice Lake, Milan},
        xticklabel style={yshift=-10pt},
        ylabel={time to solution ([$\times 10^2$]s)}, ylabel style={yshift=-1ex},
        legend cell align={left},
        /pgf/bar width=12pt,% bar width
        scatter/position=absolute,
        node near coords style={
            font=\footnotesize,
            at={(axis cs:\pgfkeysvalueof{/data point/x},\pgfkeysvalueof{/pgfplots/ymin})},
            anchor=north,
            yshift={-\pgfkeysvalueof{/pgfplots/major tick length} + 4pt},
        },
    }
    \begin{axis}[bar shift=-24pt, nodes near coords style={xshift=-24pt},
        legend pos = outer north east, legend style = {name = pairs}]
        \addplot [fill=force-cpu, nodes near coords=A] table [x=arch, y=pairsfn_std] {data/dem-single-core-force.csv};
        \addplot [fill=euler-cpu] table [x=arch, y=pairsfn_std] {data/dem-single-core-euler.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=pairsfn_std] {data/dem-single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=pairsfn_std] {data/dem-single-core-other.csv};
        \legend{Force, Euler, Neigh, Other}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{A:} P4IRS}
        \addlegendimage{empty legend}
	\addlegendentry{\hspace{-.325cm}\textbf{B:} P4IRS (NPC)}
        \addlegendimage{empty legend}
	\addlegendentry{\hspace{-.325cm}\textbf{C:} P4IRS (2R)}
        \addlegendimage{empty legend}
	\addlegendentry{\hspace{-.325cm}\textbf{D:} P4IRS (NPC, 2R)}
        \addlegendimage{empty legend}
	\addlegendentry{\hspace{-.325cm}\textbf{E:} MESA-PD (2R)}
    \end{axis}
    \begin{axis}[bar shift=-12pt, nodes near coords style={xshift=-12pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-cpu, nodes near coords=B] table [x=arch, y=pairsfn_neighbins] {data/dem-single-core-force.csv};
        \addplot [fill=euler-cpu] table [x=arch, y=pairsfn_neighbins] {data/dem-single-core-euler.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=pairsfn_neighbins] {data/dem-single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=pairsfn_neighbins] {data/dem-single-core-other.csv};
    \end{axis}
    \begin{axis}[bar shift=0pt, nodes near coords style={xshift=0pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-cpu, nodes near coords=C] table [x=arch, y=pairsfn_std2] {data/dem-single-core-force.csv};
        \addplot [fill=euler-cpu] table [x=arch, y=pairsfn_std2] {data/dem-single-core-euler.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=pairsfn_std2] {data/dem-single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=pairsfn_std2] {data/dem-single-core-other.csv};
    \end{axis}
    \begin{axis}[bar shift=12pt, nodes near coords style={xshift=12pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-cpu, nodes near coords=D] table [x=arch, y=pairsfn_neighbins2] {data/dem-single-core-force.csv};
        \addplot [fill=euler-cpu] table [x=arch, y=pairsfn_neighbins2] {data/dem-single-core-euler.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=pairsfn_neighbins2] {data/dem-single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=pairsfn_neighbins2] {data/dem-single-core-other.csv};
    \end{axis}
    \begin{axis}[bar shift=24pt, nodes near coords style={xshift=24pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-cpu, nodes near coords=E] table [x=arch, y=mesapd] {data/dem-single-core-force.csv};
        \addplot [fill=euler-cpu] table [x=arch, y=mesapd] {data/dem-single-core-euler.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=mesapd] {data/dem-single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=mesapd] {data/dem-single-core-other.csv};
    \end{axis}
\end{tikzpicture}
\vspace{-1ex}
	\caption{Execution time for P4IRS and MD-Bench simulations in seconds (lower is better) of a Setting-Spheres benchmark with 18720 particles during 10000 time-steps on Ice Lake and Milan CPUs. Tests were performed in a single core (no parallelism) with fixed frequency. Results are shown for standard Linked Cells, Linked Cells with neighbor-lists created per cell (NPC) and for single and two ranks (2R).}
\vspace{-2ex}
\centering
\label{fig:dem_single_core_runtimes}
\end{figure}

\autoref{fig:dem_single_core_runtimes} displays the runtimes for the \ac{DEM} kernels in single-core (P4IRS) and two cores/ranks (P4IRS/MESA-PD), also with cases building the neighbor-lists per cell (NPC) in P4IRS to evaluate its impact on CPU performance.
It is noticeable that P4IRS outperforms MESA-PD even with a single rank in the overall runtime, and what draws the attention is the amount of time MESA-PD spends executing other procedures instead of the force calculation and building the neighbor-lists
This can be attributed to the overhead that MESA-PD has to manage its domain-partitioning with waLBerla (which cannot be switched) and calling its communication routines for next-neighbor communication.
A more in-depth analysis is required to actually understand why it takes a significant fraction of the time for MESA-PD to communicate data, but this is out of scope of this paper.

A more interesting insight from the results is the performance improvement for the force calculation when using P4IRS, with only a single rank it is only about 16-17\% worse the the force calculation for MESA-PD, while with 2 ranks it outperforms MESA-PD by about 76-82\% using \ac{FN} in both Ice Lake and Milan architectures.
The reason is that P4IRS code is able to perform several specializations in the generated code and thus reduce the amount of arithmetic instructions to be executed.
For instance, the code from MESA-PD needs parameters such as stiffness and damping to be set at every iteration because they depend on the masses and radii of the particles, which not only incurs in extra instructions to load and store such data in a lookup array, but also restricts specializations in the kernel since some parameters are not known at compile-time.
In P4IRS, such parameters are calculated directly in the kernel, and we can make use of Just-In-Time (JIT) compilation to emit tailored versions of our kernels for the specific problem we want to solve, which leads to significant speedup.
Some specialization can also be exposed by separating the interaction code between shapes as discussed in the \nameref{sec:shape_partitioning} section.
For more complex kernels, there may be even more opportunities for specializations, which can result in even higher improvements in performance when comparing to kernels with no specializations.

Another point to evaluate is the impact of building the neighbor-lists per cell (NPC) in the CPU.
Although there are considerable improvements in the force calculation in both CPUs (about 76\% faster in Ice Lake, and about 46-49\% in Milan), the significant amount of time building the neighbor-lists does not compensate the improvements.
At the end, it is also possible to use P4IRS for experimentation of such different strategies, which can help finding the most efficient version to solve a problem for a specific hardware.
It is also clear that P4IRS can generate more efficient DEM kernels for CPU in comparison to our previous solution, which reveals the advantages of using code generation for particle simulations.

\colorlet{force-gpu}{purple!80}
\colorlet{euler-gpu}{purple!60}
\colorlet{neigh-gpu}{purple!40}
\colorlet{other-gpu}{purple!20}

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.72]
    \pgfplotsset{
        ybar stacked, ymin=0, ymax=4, xmin=0.5, xmax=2.5, xtick=data,
        xtick={1,...,2},
        xticklabels={A40, A100},
        xticklabel style={yshift=-10pt},
        ylabel={time to solution ([$\times 10^2$]s)}, ylabel style={yshift=-1ex},
        legend cell align={left},
        /pgf/bar width=12pt,% bar width
        scatter/position=absolute,
        node near coords style={
            font=\footnotesize,
            at={(axis cs:\pgfkeysvalueof{/data point/x},\pgfkeysvalueof{/pgfplots/ymin})},
            anchor=north,
            yshift={-\pgfkeysvalueof{/pgfplots/major tick length} + 4pt},
        },
    }
    \begin{axis}[bar shift=-18pt, nodes near coords style={xshift=-24pt},
        legend pos = outer north east, legend style = {name = pairs}]
        \addplot [fill=force-gpu, nodes near coords=A] table [x=arch, y=pairsfn_std] {data/dem-single-gpu-force.csv};
        \addplot [fill=euler-gpu] table [x=arch, y=pairsfn_std] {data/dem-single-gpu-euler.csv};
        \addplot [fill=neigh-gpu] table [x=arch, y=pairsfn_std] {data/dem-single-gpu-neigh.csv};
        \addplot [fill=other-gpu] table [x=arch, y=pairsfn_std] {data/dem-single-gpu-other.csv};
        \legend{Force, Euler, Neigh, Other}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{A:} P4IRS}
        \addlegendimage{empty legend}
	\addlegendentry{\hspace{-.325cm}\textbf{B:} P4IRS (NPC)}
        \addlegendimage{empty legend}
	\addlegendentry{\hspace{-.325cm}\textbf{C:} P4IRS (MK)}
        \addlegendimage{empty legend}
	\addlegendentry{\hspace{-.325cm}\textbf{D:} P4IRS (NPC, MK)}
        %\addlegendimage{empty legend}
        %\addlegendentry{\hspace{-.325cm}\textbf{E:} MESA-PD+2R}
    \end{axis}
    \begin{axis}[bar shift=-6pt, nodes near coords style={xshift=-12pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-gpu, nodes near coords=B] table [x=arch, y=pairsfn_neighbins] {data/dem-single-gpu-force.csv};
        \addplot [fill=euler-gpu] table [x=arch, y=pairsfn_neighbins] {data/dem-single-gpu-euler.csv};
        \addplot [fill=neigh-gpu] table [x=arch, y=pairsfn_neighbins] {data/dem-single-gpu-neigh.csv};
        \addplot [fill=other-gpu] table [x=arch, y=pairsfn_neighbins] {data/dem-single-gpu-other.csv};
    \end{axis}
    \begin{axis}[bar shift=6pt, nodes near coords style={xshift=0pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-gpu, nodes near coords=C] table [x=arch, y=pairsfn_mk] {data/dem-single-gpu-force.csv};
        \addplot [fill=euler-gpu] table [x=arch, y=pairsfn_mk] {data/dem-single-gpu-euler.csv};
        \addplot [fill=neigh-gpu] table [x=arch, y=pairsfn_mk] {data/dem-single-gpu-neigh.csv};
        \addplot [fill=other-gpu] table [x=arch, y=pairsfn_mk] {data/dem-single-gpu-other.csv};
    \end{axis}
    \begin{axis}[bar shift=18pt, nodes near coords style={xshift=12pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-gpu, nodes near coords=D] table [x=arch, y=pairsfn_neighbins_mk] {data/dem-single-gpu-force.csv};
        \addplot [fill=euler-gpu] table [x=arch, y=pairsfn_neighbins_mk] {data/dem-single-gpu-euler.csv};
        \addplot [fill=neigh-gpu] table [x=arch, y=pairsfn_neighbins_mk] {data/dem-single-gpu-neigh.csv};
        \addplot [fill=other-gpu] table [x=arch, y=pairsfn_neighbins_mk] {data/dem-single-gpu-other.csv};
    \end{axis}
\end{tikzpicture}
\vspace{-3ex}
	\caption{Execution time for P4IRS simulations in seconds (lower is better) of a Setting-Spheres benchmark with 561.600 particles during 10000 time-steps on A40 and A100 GPUs. Results are shown for standard Linked Cells, Linked Cells with neighbor-lists created per cell (NPC) and for multi-kernel (MK) in both cases.}
\vspace{-2ex}
\centering
\label{fig:dem_single_gpu_runtimes}
\end{figure}

\autoref{fig:dem_single_gpu_runtimes} displays the runtimes for the DEM test-case in both A40 and A100 GPUs, also evaluating the neighbor-lists per cell (see the \nameref{sec:neighbor_lists_per_cell} section) and multi-kernel (see the \nameref{sec:shape_partitioning} section) strategies.
Most of the simulation time is spent on calculating the forces and executing other routines, which is likely transferring data into/from the GPUs.
Note that since particles contain significant more properties (and some of them with dynamic types such as vectors and matrices), the communication and/or PBC buffers are significantly larger especially when particles need to be exchanged.
It is also important to disclose that due to PBC particles are still exchanged and produce ghost versions of themselves (with proper PBC shifts) even when running in a single MPI rank, therefore the same routines for packing and unpacking data for communication are used.

It is noticeable that in this case, building the neighbor-lists per cell actually increases the overall performance, as the improved runtime for the force calculation compensates the time for building the neighbor-lists.
The force calculation with the neighbor-lists per cell is about 24\% faster the standard Linked Cells version in A40, and about 45\% faster in A100, hence the SIMD execution is improved as expected.
In the overall runtime, the NPC version is about 20\% faster for A40 and 12\% faster for A100 since the increase in time for building the neighbor-lists is relatively larger for the A100 GPU.

For the Multi-Kernel approach, there are no significant improvements observed for the A40 GPU, with a time difference less than 1\% in the force calculation kernels.
The time spent in other sections is significantly higher leading to worse performance when using the Multi-Kernel, but this can be happening due to impact caused by asynchronously launching more kernels into the GPU.
For the A100, the same behavior applies for the standard Linked Cells, but when combining both NPC and MK approaches, it is possible to see an improvement of 13\% in the force calculation performance in comparison to only using NPC, leading to about 6\% performance improvement overall.
This example also illustrates the usage of P4IRS for experimentation, as these different kernel versions can be generated with P4IRS without the effort of implementing each one of them for the specific interaction model used.

\begin{table*}[htb]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c}
        Variant & pairs-fn & pairs-fn-nb & pairs-fn-2r & pairs-fn-nb-2r & pairs-fn-mk & pairs-fn-nb-mk & mesapd \\
        \hline
        Ice Lake & 76.573 & 48.549 & 164.202 & 101.062 & --- & --- & 47.304 \\
        Milan & 113.229 & 68.008 & 225.374 & 134.969 & --- & --- & 64.347 \\
        A40 & 68.866 & 83.06 & --- & --- & 63.835 & 79.828 & --- \\
        A100 & 210.41 & 235.753 & --- & --- & 209.969 & 251.292 & --- \\
    \end{tabular}
    \caption{\RMchange{Summary of performance results (in millions of particle updates per second) for each P4IRS and MESA-PD simulation run of the DEM benchmark on a single core of Ice Lake CPU, on a single core of a Milan CPU, on a single A40 GPU, and on a single A100 GPU.}}
    \label{tab:dem_particle_updates_per_second_dem}
\end{table*}

\RMchange{\autoref{tab:dem_particle_updates_per_second_dem} summarizes the results running on one and two CPU cores, as well as on a single GPU, the results are shown in millions of particle updates per second.
The lower total number of overall particle interactions combined with a more complex kernel makes the Linear Spring-Dashpot method less suitable for GPUs when using the standard Linked Cells strategy.
Better performance is achieved using both the Neighbor-Lists per Cell and the Multi-Kernel approaches for GPUs, but there is still room for improvement.
Evaluating and designing new algorithms to efficiently compute this specific kernel on GPUs is one of our next goals, and code generation can be particularly helpful because we can experiment with a single high-level description of the problem.}

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.95]
    \tikzstyle{every node}=[font=\small]
    \begin{axis}[
            width=0.5\textwidth, height=8cm,
            xmode=log,log basis x={2},
            %xmin=0.75,   xmax=3072,
            xmin=1,   xmax=64,
            ymin=0.5,   ymax=1.5,
            log ticks with fixed point,
            xlabel={\# nodes}, xlabel style={yshift= 1ex},
            xticklabel={ % hack for precision issues with 1024 and 2048
                \pgfkeys{/pgf/fpu=true}
                \pgfmathparse{int(2^\tick)}
                \pgfmathprintnumber[fixed]{\pgfmathresult}
            },
            ylabel={time to solution ([$\times 10^2$]s)}, ylabel style={yshift=-1ex},
            grid=major,
            legend pos=north east,
            %legend style={at={(0.97,0.5)},anchor=east},
        ]
        \addplot table[x=nodes,y=pairsfn] {data/dem-scaling-fritz.csv};
        %\addplot table[x=nodes,y=pairshn] {data/dem-scaling-fritz.csv};
        \legend{P4IRS-FN}
    \end{axis}
\end{tikzpicture}
\vspace{-2ex}
\caption{Weak-scaling results for P4IRS (FN) on Fritz, each node simulates 10000 time-steps of the Settling-Spheres simulation, with approximately 7.800 particles per CPU core.}
\vspace{-2ex}
\label{fig:dem_weak_scaling_fritz}
\end{figure}

Finally, \autoref{fig:dem_weak_scaling_fritz} shows the weak-scaling for the DEM case on Fritz.
It is noticeable that the scaling degrades over successive executions, with about 91\% efficiency with 32 nodes (2304 CPU cores) and 87\% efficiency with 64 nodes (4608 CPU cores).
The reason is likely due to work imbalances caused by the Settling-Spheres example when extending the simulation domain and amount of particles.
Note that the example itself can present more heterogeneity when scaling up, as the spheres generated contain random radii and this can impact the amount of interactions computed, as well as the overall distribution of the particles into the sub-domains over the course of the simulation.
In practice, the presented weak-scaling results are still acceptable, but they demonstrate that such particle simulations may not necessarily scale well for more realistic and complex simulations.
The purpose of this work, however, is to demonstrate that we can achieve state-of-the-art performance and (currently in certain conditions) good scalability with P4IRS while providing simple and flexible means of modeling particle simulations, which is achieved via code generation techniques.

\subsubsection*{Acknowledgments}

The authors gratefully acknowledge the scientific support and HPC resources provided by the Erlangen National High Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU).
This project has received funding from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under project number 433735254.
\RMchange{Additionally, this work has also received funding from the European High Performance Computing Joint Undertaking (JU) and Sweden, Germany, Spain, Greece, and Denmark under grant agreement number 101093393.}

\section{Conclusion and Outlook}
\label{sec:conclusion}

This paper introduced P4IRS, an intermediate representation and compiler for particle simulations which is able to use a single problem description in Python and generate efficient C++ and CUDA parallel codes with OpenMP and MPI for shared- and distributed-memory parallelism.
We present and describe P4IRS features focusing on \ac{MD} and \ac{DEM}, illustrating how to setup simulations by providing the potential and/or contact model specification, the particle properties and even contact properties required for more complex DEM models such as the Linear Spring-Dashpot.
Apart from features to model particle simulations, we also display optimization strategies enabled with P4IRS, which are possible to be achieved through code generation.
P4IRS is currently able to generate the Linked Cells and Verlet Lists variants for each kernel specified, and these can also make use of specific optimization strategies such as the neighbor-lists per cell and multi-kernel approach to enhance SIMD parallelism on CPUs and GPUs.

Results for \ac{MD} kernels generated by P4IRS demonstrate that we can achieve state-of-the-art performance in comparison to our performance-focused prototyping harness MD-Bench, which contains hand-tuned implementations of \ac{MD} kernels in C and CUDA.
Also, results in modern clusters Fritz and JUWELS Booster are displayed to show P4IRS scaling capabilities, which are important for particle simulations in general.
For more complex and realistic cases, we also show that P4IRS is able to generate code to solve \ac{DEM} simulation with the Linear Spring-Dashpot contact model.
The performance results demonstrate that P4IRS significantly outperforms our previous framework (called MESA-PD), which can be attributed to aggressive specializations in the force calculation kernels of the contact model.
Besides, we also demonstrate how P4IRS can be used for experimentation, since contact models or potentials can be generated with different optimization strategies without the effort of implementing new kernels, highlighting the advantages of using code generation techniques.

In the future, we intend to use P4IRS particle simulations coupled with fluid simulations, which can be solved through the waLBerla framework.
For this, we still need to generate proper interfaces for the coupling and generate modular code for the particle simulations that can be seamlessly integrated into other applications.
Nevertheless, we also want to include more performance optimizations into P4IRS such as the Cluster Pair algorithm from GROMACS to enhance SIMD performance, as well as being able to generate SIMD intrinsics explicitly into P4IRS generated kernels to either enforce SIMD vectorization and also to be able to choose between different strategies (i.e. gather data manually instead of using gather instructions).
Another future plan is to support long-range interactions in P4IRS simulations, which could be achieved either by implementing methods in P4IRS to account for the long-range computation steps or coupling the generated code for short-range calculations with other existing long-range implementations.
At the end, the separation of concerns introduced by P4IRS together with its intermediate representation embedded in Python provide several opportunities to explore and experiment with performance on many types of particle simulations that can belong to distinct research fields.

\bibliographystyle{plain}

\begin{thebibliography}{99}

%\bibitem[Mittelbach and Goossens(2004)]{R3}
%Mittelbach~F and Goossens~M (2004) \textit{The \LaTeX\ Companion},
%2nd~edn. Addison-Wesley.

\bibitem[Anderson~J~A et al.(2020)]{hoomdblue}
Joshua~A. Anderson, Jens Glaser, and Sharon~C. Glotzer.
\newblock Hoomd-blue: A python package for high-performance molecular dynamics
  and hard particle monte carlo simulations.
\newblock {\em Computational Materials Science}, 173:109363, 2020.

\bibitem[Brendel~L and Dippel~S(1998)]{dem1}
L.~Brendel and S.~Dippel.
\newblock {\em Lasting Contacts in Molecular Dynamics Simulations}, pages
  313--318.
\newblock Springer Netherlands, Dordrecht, 1998.

\bibitem[Brooks~B~R et al.(2009)]{charmm}
B~R Brooks, C~L Brooks, A~D Mackerell, L~Nilsson, R~J Petrella, B~Roux, Y~Won,
  G~Archontis, C~Bartels, S~Boresch, A~Caflisch, L~Caves, Q~Cui, A~R Dinner,
  M~Feig, S~Fischer, J~Gao, M~Hodoscek, W~Im, K~Kuczera, T~Lazaridis, J~Ma,
  V~Ovchinnikov, E~Paci, R~W Pastor, C~B Post, J~Z Pu, M~Schaefer, B~Tidor, R~M
  Venable, H~L Woodcock, X~Wu, W~Yang, D~M York, and M~Karplus.
\newblock Charmm: the biomolecular simulation program.
\newblock {\em Journal of Computational Chemistry}, 30(10):1545--1614, 2009.

\bibitem[Brown~W et al.(2012)]{lammps2}
W.~Brown, Axel Kohlmeyer, Steven Plimpton, and Arnold Tharrington.
\newblock Implementing molecular dynamics on hybrid high performance computers
  - particle-particle particle-mesh.
\newblock {\em Computer Physics Communications}, 183:449--459, 03 2012.

\bibitem[{Carter Edwards}~H et al.(2014)]{kokkos}
H.~{Carter Edwards}, Christian~R. Trott, and Daniel Sunderland.
\newblock Kokkos: Enabling manycore performance portability through polymorphic
  memory access patterns.
\newblock {\em Journal of Parallel and Distributed Computing},
  74(12):3202--3216, 2014.
\newblock Domain-Specific Languages and High-Level Frameworks for
  High-Performance Computing.

\bibitem[Case~D~A. et al.(2023)]{amber1}
David~A. Case, Hasan~Metin Aktulga, Kellon Belfon, David~S. Cerutti, G.~Andrés
  Cisneros, Vinícius Wilian~D. Cruzeiro, Negin Forouzesh, Timothy~J. Giese,
  Andreas~W. Götz, Holger Gohlke, Saeed Izadi, Koushik Kasavajhala, Mehmet~C.
  Kaymak, Edward King, Tom Kurtzman, Tai-Sung Lee, Pengfei Li, Jian Liu, Tyler
  Luchko, Ray Luo, Madushanka Manathunga, Matias~R. Machado, Hai~Minh Nguyen,
  Kurt~A. O’Hearn, Alexey~V. Onufriev, Feng Pan, Sergio Pantano, Ruxi Qi, Ali
  Rahnamoun, Ali Risheh, Stephan Schott-Verdugo, Akhil Shajan, Jason Swails,
  Junmei Wang, Haixin Wei, Xiongwu Wu, Yongxian Wu, Shi Zhang, Shiji Zhao,
  Qiang Zhu, Thomas E.~III Cheatham, Daniel~R. Roe, Adrian Roitberg, Carlos
  Simmerling, Darrin~M. York, Maria~C. Nagan, and Kenneth M.~Jr. Merz.
\newblock Ambertools.
\newblock {\em Journal of Chemical Information and Modeling},
  63(20):6183--6191, 2023.
\newblock PMID: 37805934.

\bibitem[Eastman~P et al.(2017)]{openmm}
Peter Eastman, Jason Swails, John~D. Chodera, Robert~T. McGibbon, Yutong Zhao,
  Kyle~A. Beauchamp, Lee-Ping Wang, Andrew~C. Simmonett, Matthew~P. Harrigan,
  Chaya~D. Stern, Rafal~P. Wiewiora, Bernard~R. Brooks, and Vijay~S. Pande.
\newblock Openmm 7: Rapid development of high performance algorithms for
  molecular dynamics.
\newblock {\em PLOS Computational Biology}, 13(7):1--17, 07 2017.

\bibitem[Eibl~S and Rüde~U(2019)]{mesapd3}
Sebastian Eibl and Ulrich R{\"{u}}de.
\newblock A modular and extensible software architecture for particle dynamics.
\newblock {\em Proceedings of the 8\textsuperscript{th} International
  Conference on Discrete Element Methods (DEM8)}, 2019.

\bibitem[Eibl~S and Rüde~U(2018)]{mesapd1}
Sebastian Eibl and Ulrich Rüde.
\newblock A local parallel communication algorithm for polydisperse rigid body
  dynamics.
\newblock {\em Parallel Computing}, 80:36--48, 2018.

\bibitem[Eibl~S and Rüde~U(2019)]{mesapd2}
Sebastian Eibl and Ulrich Rüde.
\newblock A systematic comparison of runtime load balancing algorithms for
  massively parallel rigid particle dynamics.
\newblock {\em Computer Physics Communications}, 244:76--85, 2019.

\bibitem[Godenschwager~C et al.(2013)]{walberla1}
Christian Godenschwager, Florian Schornbaum, Martin Bauer, Harald K\"{o}stler,
  and Ulrich R\"{u}de.
\newblock A framework for hybrid parallel flow simulations with a trillion
  cells in complex geometries.
\newblock In {\em Proceedings of the International Conference on High
  Performance Computing, Networking, Storage and Analysis}, SC '13, New York,
  NY, USA, 2013. Association for Computing Machinery.

\bibitem[Khakhar~D~V et al.(1997)]{dem2}
D.~V. Khakhar, J.~J. McCarthy, and J.~M. Ottino.
\newblock {Radial segregation of granular mixtures in rotating cylinders}.
\newblock {\em Physics of Fluids}, 9(12):3600--3614, 12 1997.

\bibitem[Kloss~C et al.(2012)]{liggghts}
Christoph Kloss, Christoph Goniva, Alice König, Stefan Amberger, and Stefan
  Pirker.
\newblock Models, algorithms and validation for opensource dem and cfd-dem.
\newblock {\em Progress in Computational Fluid Dynamics}, 12:140 -- 152, 06
  2012.

\bibitem[Lei{\ss}a~R et al.(2018)]{anydsl1}
Roland Lei{\ss}a, Klaas Boesche, Sebastian Hack, Ars{\`{e}}ne
  P{\'{e}}rard{-}Gayot, Richard Membarth, Philipp Slusallek, Andr{\'{e}}
  M{\"{u}}ller, and Bertil Schmidt.
\newblock {AnyDSL}: A partial evaluation framework for programming
  high-performance libraries.
\newblock {\em {PACMPL}}, 2({OOPSLA}):119:1--119:30, 2018.

\bibitem[Lei{\ss}a~R et al.(2015)]{anydsl2}
Roland Lei{\ss}a, Marcel K{\"{o}}ster, and Sebastian Hack.
\newblock A graph-based higher-order intermediate representation.
\newblock In {\em Proceedings of the 13th Annual {IEEE/ACM} International
  Symposium on Code Generation and Optimization, {CGO} 2015, San Francisco, CA,
  USA, February 07 - 11, 2015}, pages 202--212, 2015.

\bibitem[Machado~R~R~L et al.(2021)]{tinymd}
Rafael Ravedutti~L. Machado, Jonas Schmitt, Sebastian Eibl, Jan Eitzinger,
  Roland Leißa, Sebastian Hack, Arsène Pérard-Gayot, Richard Membarth, and
  Harald Köstler.
\newblock tinymd: Mapping molecular dynamics simulations to heterogeneous
  hardware using partial evaluation.
\newblock {\em Journal of Computational Science}, 54:101425, 2021.

\bibitem[Mniszewski~S~M et al.(2021)]{ecpcopa}
Susan~M Mniszewski, James Belak, Jean-Luc Fattebert, Christian~FA Negre,
  Stuart~R Slattery, Adetokunbo~A Adedoyin, Robert~F Bird, Choongseok Chang,
  Guangye Chen, Stéphane Ethier, Shane Fogerty, Salman Habib, Christoph
  Junghans, Damien Lebrun-Grandié, Jamaludin Mohd-Yusof, Stan~G Moore, Daniel
  Osei-Kuffuor, Steven~J Plimpton, Adrian Pope, Samuel~Temple Reeve, Lee
  Ricketson, Aaron Scheinberg, Amil~Y Sharma, and Michael~E Wall.
\newblock Enabling particle applications for exascale computing platforms.
\newblock {\em The International Journal of High Performance Computing
  Applications}, 35(6):572–597, July 2021.

\bibitem[P{\'a}ll~S et al.(2015)]{gromacs2}
Szil{\'a}rd P{\'a}ll, Mark~James Abraham, Carsten Kutzner, Berk Hess, and Erik
  Lindahl.
\newblock Tackling exascale software challenges in molecular dynamics
  simulations with gromacs.
\newblock In Stefano Markidis and Erwin Laure, editors, {\em Solving Software
  Challenges for Exascale}, pages 3--27, Cham, 2015. Springer International
  Publishing.

\bibitem[Pennycook~J et al.(2013)]{mdsimd}
John Pennycook, Chris Hughes, M.~Smelyanskiy, and Stephen Jarvis.
\newblock Exploring simd for molecular dynamics, using intel® xeon®
  processors and intel® xeon phi coprocessors.
\newblock In {\em Proceedings - IEEE 27th International Parallel and
  Distributed Processing Symposium, IPDPS 2013}, pages 1085--1097, 05 2013.

\bibitem[Phillips~J~C et al.(2005)]{namd}
James~C. Phillips, Rosemary Braun, Wei Wang, James Gumbart, Emad Tajkhorshid,
  Elizabeth Villa, Christophe Chipot, Robert~D. Skeel, Laxmikant Kalé, and
  Klaus Schulten.
\newblock Scalable molecular dynamics with namd.
\newblock {\em Journal of Computational Chemistry}, 26(16):1781--1802, 2005.

\bibitem[Plimpton~S(1995)]{lammps1}
Steve Plimpton.
\newblock Fast parallel algorithms for short-range molecular dynamics.
\newblock {\em Journal of Computational Physics}, 117(1):1--19, 1995.

\bibitem[Páll~S and Hess~B(2013)]{gromacs1}
Szilárd Páll and Berk Hess.
\newblock A flexible algorithm for calculating pair interactions on simd
  architectures.
\newblock {\em Computer Physics Communications}, 184(12):2641--2650, 2013.

\bibitem[Quentrec~B and Brot~C(1973)]{linkedcells}
B~Quentrec and C~Brot.
\newblock New method for searching for neighbors in molecular dynamics
  computations.
\newblock {\em Journal of Computational Physics}, 13(3):430--432, 1973.

\bibitem[Ravedutti Lucio Machado~R et al.(2023a)]{mdbench1}
Rafael Ravedutti Lucio~Machado, Jan Eitzinger, Harald K{\"o}stler, and Gerhard
  Wellein.
\newblock MD-Bench: A generic proxy-app toolbox for state-of-the-art molecular
  dynamics algorithms.
\newblock In Roman Wyrzykowski, Jack Dongarra, Ewa Deelman, and Konrad
  Karczewski, editors, {\em Parallel Processing and Applied Mathematics}, pages
  321--332, Cham, 2023. Springer International Publishing.

\bibitem[Ravedutti Lucio Machado~R et al.(2023b)]{mdbench2}
Rafael Ravedutti Lucio Machado, Jan Eitzinger, Jan Laukemann, Georg Hager,
  Harald Köstler, and Gerhard Wellein.
\newblock MD-Bench: A performance-focused prototyping harness for
  state-of-the-art short-range molecular dynamics algorithms.
\newblock {\em Future Generation Computer Systems}, 149:25--38, 2023.

\bibitem[Schornbaum~F and R\"{u}de~U(2016)]{walberla2}
Florian Schornbaum and Ulrich R\"{u}de.
\newblock Massively parallel algorithms for the lattice boltzmann method on
  nonuniform grids.
\newblock {\em SIAM Journal on Scientific Computing}, 38(2):C96--C126, 2016.

\bibitem[Smilauer~V et al.(2021)]{yade}
Vaclav Smilauer, Vasileios Angelidakis, Emanuele Catalano, Robert Caulk, Bruno
  Chareyre, William Chèvremont, Sergei Dorofeenko, Jerome Duriez, Nolan Dyck,
  Jan Elias, Burak Er, Alexander Eulitz, Anton Gladky, Ning Guo, Christian
  Jakob, Francois Kneib, Janek Kozicki, Donia Marzougui, Raphael Maurin, Chiara
  Modenese, Gerald Pekmezi, Luc Scholtès, Luc Sibille, Jan Stransky, Thomas
  Sweijen, Klaus Thoeni, and Chao Yuan.
\newblock {\em Yade documentation}.
\newblock The Yade Project, November 2021.

\bibitem[Treibig~J et al.(2010)]{likwid}
Jan Treibig, Georg Hager, and Gerhard Wellein.
\newblock Likwid: A lightweight performance-oriented tool suite for x86
  multicore environments.
\newblock In {\em 2010 39th International Conference on Parallel Processing
  Workshops}, pages 207--216, 2010.

\bibitem[Verlet~L(1967)]{verletlists}
Loup Verlet.
\newblock Computer "experiments" on classical fluids. i. thermodynamical
  properties of lennard-jones molecules.
\newblock {\em Phys. Rev.}, 159:98--103, Jul 1967.

\bibitem[Weinhart~T et al.(2020)]{mercurydpm}
Thomas Weinhart, Luca Orefice, Mitchel Post, Marnix~P. {van Schrojenstein
  Lantman}, Irana~F.C. Denissen, Deepak~R. Tunuguntla, J.M.F. Tsang, Hongyang
  Cheng, Mohamad~Yousef Shaheen, Hao Shi, Paolo Rapino, Elena Grannonio, Nunzio
  Losacco, Joao Barbosa, Lu~Jing, Juan~E. {Alvarez Naranjo}, Sudeshna Roy,
  Wouter~K. {den Otter}, and Anthony~R. Thornton.
\newblock Fast, flexible particle simulations — an introduction to
  mercurydpm.
\newblock {\em Computer Physics Communications}, 249:107129, 2020.

\end{thebibliography}

\end{document}
