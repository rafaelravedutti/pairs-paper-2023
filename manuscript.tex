%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}
\usepackage{color}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage[hidelinks]{hyperref}
\usepackage[printonlyused]{acronym}
\usepackage{listings}
\usepackage{algorithm,algpseudocode}
%\usepackage{math}
\usepackage{tikz}
\usepackage{pgfplots}

\journal{Computer Physics Communications}

\lstset{
    showstringspaces=false,
    extendedchars=true,
    basicstyle=\footnotesize\ttfamily,
    commentstyle=\slshape,
    stringstyle=\ttfamily,
    breaklines=true,
    breakatwhitespace=true,
    columns=flexible,
    numbers=left,
    numberstyle=\tiny,
    basewidth=.5em,
    xleftmargin=.5cm,
    captionpos=b,
    frame=lines
}

%Predefined commands
\newcommand{\bq}{\begin{equation}}
\newcommand{\eq}{\end{equation}}
\newcommand{\bytes}{\mbox{bytes}}
\newcommand{\byte}{\mbox{byte}}
\newcommand{\second}{\mbox{s}}
\newcommand{\seconds}{\mbox{s}}
\newcommand{\flop}{\mbox{flop}}
\newcommand{\flops}{\mbox{flops}}
\newcommand{\NJFLOP}{\mbox{nJ/\flop}}
\newcommand{\instr}{\mbox{instr}}
\newcommand{\cycle}{\mbox{cy}}
\newcommand{\iter}{\mbox{it}}
\newcommand{\cycles}{\mbox{cy}}
\newcommand{\FCY}{\mbox{\flop/\cycle}}
\newcommand{\FIT}{\mbox{\flop/\iter}}
\newcommand{\BIT}{\mbox{\byte/\iter}}
\newcommand{\FR}{\mbox{\flops/\mbox{row}}}
\newcommand{\BR}{\mbox{\byte/\mbox{row}}}
\newcommand{\CYF}{\mbox{\cycles/\flop}}
\newcommand{\CS}{\mbox{\cycle/\second}}
\newcommand{\GCS}{\mbox{G\cycle/\second}}
\newcommand{\word}{\mbox{Word}}
\newcommand{\words}{\mbox{Words}}
\newcommand{\order}[1]{\mbox{${\cal O}\left(\mbox{#1}\right)$}}
\newcommand{\bit}{\mbox{bit}}
\newcommand{\bits}{\mbox{bits}}
\newcommand{\GBPS}{\mbox{G\bit/\second}}
\newcommand{\MBPS}{\mbox{M\bit/\second}}
\newcommand{\FS}{\mbox{\flop/\second}}
\newcommand{\BS}{\mbox{\byte/\second}}
\newcommand{\BC}{\mbox{\byte/\cycle}}
\newcommand{\GBS}{\mbox{G\byte/\second}}
\newcommand{\MBS}{\mbox{M\byte/\second}}
\newcommand{\GWS}{\mbox{G\word/\second}}
\newcommand{\GFS}{\mbox{G\flop/\second}}
\newcommand{\MFS}{\mbox{M\flop/\second}}
\newcommand{\lup}{\mbox{LUP}}
\newcommand{\lups}{\mbox{LUPs}}
\newcommand{\LUPCY}{\mbox{\lup/\cycle}}
\newcommand{\LUPS}{\mbox{\lup/\second}}
\newcommand{\MLUPS}{\mbox{M\lup/\second}}
\newcommand{\GLUPS}{\mbox{G\lup/\second}}
\newcommand{\GHZ}{\mbox{GHz}}
\newcommand{\ns}{\mbox{ns}}
\newcommand{\WF}{\mbox{\word/\flop}}
\newcommand{\BF}{\mbox{\byte/\flop}}
\newcommand{\FB}{\mbox{\flop/\byte}}
\newcommand{\BL}{\mbox{\byte/\lup}}
\newcommand{\GB}{\mbox{GB}}
\newcommand{\KB}{\mbox{kB}}
\newcommand{\MB}{\mbox{MB}}
\newcommand{\GiB}{\mbox{GiB}}
\newcommand{\MiB}{\mbox{MiB}}
\newcommand{\KiB}{\mbox{KiB}}
\newcommand{\W}{\mbox{W}}
\newcommand{\muarch}{\mbox{$\mu$-arch}}
\newcommand{\muop}{\mbox{$\mu$-op}}
\newcommand{\muops}{\mbox{$\mu$-ops}}
\newcommand{\eos}{\;.}
\newcommand{\cma}{\;,}
\newcommand{\rlm}{roof{}line model}
\newcommand{\rl}{roof{}line}
\newcommand{\Rlm}{Roof{}line model}
\newcommand{\Rl}{Roof{}line}
\newcommand{\olsep}{\|}
\newcommand{\nolsep}{|}
\newcommand{\ecmspace}{\,}
\newcommand{\TOL}{$T_{\mathrm{c}\_\mathrm{OL}}$}
\newcommand{\NNZR}{$N_\mathrm{nzr}$}
\newcommand{\NR}{$N_\mathrm{r}$}
\newcommand{\NNZ}{$N_\mathrm{nz}$}
\newcommand{\ecm}[6]{\mbox{$\left\{{#1}\ecmspace\olsep\ecmspace {#2}\ecmspace\nolsep\ecmspace {#3}\ecmspace\nolsep\ecmspace {#4}\ecmspace\nolsep\ecmspace {#5}\right\}\ecmspace{#6}$}}
\newcommand{\epsep}{\rceil}
\newcommand{\ecmp}[4]{\mbox{$\left\{{#1}\ecmspace\epsep\ecmspace {#2}\ecmspace\epsep\ecmspace {#3}\right\}\ecmspace{#4}$}}
\newcommand{\ecme}[4]{\mbox{$\left({#1}\ecmspace\epsep\ecmspace {#2}\ecmspace\epsep\ecmspace {#3}\right)\ecmspace{#4}$}}
\newcommand{\sellcs}{SELL-\texorpdfstring{$C$-$\sigma$}{C-sigma}}
\newcommand{\likwid}{\texttt{LIKWID}}
\newcommand{\likwidperfctr}{\texttt{likwid-perfctr}}
\newcommand{\likwidpin}{\texttt{likwid-pin}}
\newcommand{\likwidbench}{\texttt{likwid-bench}}
\newcommand{\lmbench}{\texttt{lmbench}}

% Acronyms
\newacro{AoS}{array of structures}
\newacro{AoSoA}{array of structures of arrays}
\newacro{CPI}{cycles per instruction}
\newacro{DEM}{discrete element method}
\newacro{EAM}{embedded atom method}
\newacro{FCC}{face-centered cubic}
\newacro{FN}{full neighbor-lists}
\newacro{HN}{half neighbor-lists}
\newacro{ILP}{instruction-level parallelism}
\newacro{IR}{intermediate representation}
\newacro{AST}{abstract syntax tree}
\newacro{ISA}{instruction set architecture}
\newacro{LJ}{Lennard-Jones}
\newacro{MD}{Molecular dynamics}
\newacro{MPI}{message passing interface}
\newacro{SIMD}{single instruction, multiple data}
\newacro{SoA}{structure of arrays}
\newacro{PBC}{periodic boundary conditions}
\newacro{HPM}{hardware performance monitoring}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{P4IRS: An Intermediate Representation and Compiler for Parallel and Performance-Portable Particle Simulations}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[nhratfau]{Rafael Ravedutti Lucio Machado}
\author[nhratfau]{Jan Eitzinger}
\author[nhratfau]{Harald Köstler}

\affiliation[nhratfau]{
	organization={Erlangen National High Performance Computing Center}, %(NHR@FAU)},
	addressline={Martensstraße 1},
	city={Erlangen},
	postcode={91058},
	state={Bayern},
	country={Germany}}

\begin{abstract}
Various physics simulations today rely on simulating particle interactions, where particles can represent point masses (as in Molecular Dynamics), rigid bodies (as in Discrete Element Method) or even massive bodies such as planets.
Evaluating and calculating the required particle interactions in a simulation is computationally expensive, hence suitable algorithms and proper optimizations to exploit available parallelism in the target hardware are important to reach good performance.
However, it is difficult to maintain flexible implementations while keeping state-of-the-art performance, as most packages are developed individually and have their own hard-coded, fine-tuned implementations.
To combine flexibility and optimal performance, we introduce P4IRS, an intermediate representation and compiler for particle simulations which is aimed at delivering good performance.
We describe P4IRS, display usage examples and evaluate the performance from its generated code in both MD and DEM fields.
\end{abstract}

%%Graphical abstract
\begin{graphicalabstract}
%\includegraphics{grabs}
\end{graphicalabstract}

%%Research highlights
\begin{highlights}
\item Research highlight 1
\item Research highlight 2
\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
code generation \sep particle simulations \sep molecular dynamics \sep discrete element method \sep HPC
\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:introduction}

The computation of particle-pair interactions is widely used in physics simulations, especially in the fields of Molecular Dynamics and Discrete Element Method.
To efficiently compute the inter-particle contributions, different strategies can be used to increase efficiency and make use of the parallelism offered by the target hardware.
Most simulation packages, however, have their own hard-coded implementations, which are strictly tighted to their data structures and programming artifacts.
Also, several kernel implementations for the same potential or force field are present in a single package to target different hardwares such as GPUs and to exploit different optimization strategies, which leads to higher maintainability and substantially increases the portability effort.

To tackle these limitations, we developed P4IRS --- Parallel and Performance-Portable Particles Intermediate Representation and Simulator, an intermediate representation and compiler tool where particle interactions can be simply expressed as Python methods, and efficient and parallel code for the interaction description can be generated for multi-CPU and multi-GPU targets.
P4IRS provide its own intermediate representation, which allows it to perform compiler analysis and transformations, to then generate code for the proper backend such as C++ and CUDA with OpenMP/MPI.

P4IRS can generate several kernel flavours of the same potential such as Linked Cells and Verlet Lists, where data layouts for properties and specific arrays (such as the neighbor-lists) can be switched to the case with best performance.
A runtime interface in C++ is also available and can be used to integrate P4IRS with other simulation tools, which is particularly important for us to be able to integrate P4IRS simulations with our multi-physics framework waLBerLa.
%as an example we show how P4IRS can use the load-balancing communication algorithm from waLBerLa to compute simulations with a more heterogeneous distribution of particles.

Most of P4IRS internal routines such as building Linked Cells and Verlet Lists are implemented in Python by constructing the intermediate representation for them with the assistance of operator overloading.
In this way, we leverage P4IRS analysis and transformations to achieve performance-portability for these routines as well, without requiring multiple tailored implementations.

The goals of this paper are summarized as follows:
\begin{itemize}
	\item We present our code generator tool P4IRS, describing how it works and how it can be used to generate efficient particle simulation codes.
	\item We discuss the advantages of using P4IRS to achieve performance-portability, also comparing it with other previous approaches such as tinyMD and MESA-PD.
	\item We describe P4IRS interfaces to allow its generated code to be integrated with other simulation tools, especifically in the context of domain-partitioning and communication.
	\item We provide performance and scaling results for Intel Ice Lake, AMD Milan and NVIDIA A40 and A100 hardwares, to demonstrate the state-of-the-art performance and scaling capabilities of the applications generated by P4IRS.
\end{itemize}

This paper is structured as follows: In \autoref{sec:related_work} we list related work for MD and DEM simulation packages, code generation tools for particle simulations and performance evaluation of such simulations. In \autoref{sec:background} we explain the basic theory for particle simulations focusing on MD and DEM methods, also covering optimization strategies and choices. In \autoref{sec:pairs} we present P4IRS with its current features and application design. In \autoref{sec:evaluation} we evaluate the performance and scalability from P4IRS applications using \ac{MD} and \ac{DEM} experiments. Finally, \autoref{sec:conclusion} presents our conclusion and outlook for P4IRS, also covering our future work.

\section{Related Work}
\label{sec:related_work}

\subsection{Simulation Packages}
\label{sec:packages}

Diverse simulation fields can be described by updating particle trajectories based on their interactions with other particles.
In this paper, we mainly focus on \ac{MD} and \ac{DEM}, where in MD particles represent point masses (i.e. atoms) in the simulation, and in \ac{DEM} particles represent rigid bodies with specific shapes (i.e. spheres, hyperplanes, ellipses).
Most packages available use similar techniques to compute trajectories, but each of them have their own specific algorithms, parallelism strategies and hand-tuned implementations for specific hardware.
With P4IRS, we focus on using a single description of the problem, and then leverage its implemented routines and code transformations to provide the high-end implementation for the specific target, which currently can be C++ with OpenMP and CUDA, also supporting MPI for multiple computing nodes.

In MD, there exist many packages with distinct strategies and simulation fields.
LAMMPS is a molecular package for the material-modeling field which uses the standard Verlet Lists algorithm to optimize the non-bonded short-range force calculations.
It offers many interatomic potentials, each one with different kernel implementations for a specific target (like CPU, GPU, Kokkos).
GROMACS is another package for MD with focus on bio-sciences simulation which uses its own Cluster Pair algorithm to optimize the short-range forces computation.
It does not support as many potentials as LAMMPS, but its algorithm tends to be significantly faster than the Verlet Lists since it employs SIMD parallelism more efficiently.
Furthermore, the optimal GROMACS kernel implementations rely on SIMD intrinsics to attain higher performance, which makes them harder to be maintained and ported to other architectures.
Also, the GPU algorithm in GROMACS differs from the CPU, since it relies on a strategy named ``super-clustering'' to reduce the amount of redundant computations caused by adjusting the cluster sizes to fit the SIMD width in the GPU.
Other MD packages such as HOOMD-blue, NAMD, AMBER and CHARMM also contain dedicated and hand-tuned implementations in general purpose languages for each supported backend.
Each version is tested, maintained, debugged and optimized independently.

Packages that support DEM simulations are in general more flexible (since simulation with more particle properties such as rotations and shapes are needed), however they usually do not have the same optimization techniques as the MD simulation packages.
The reason is most likely that it is not so feasible to support flexibility with sophisticated optimization algorithms as the complexity of the code grows considerably.
Examples of such packages are the LIGGGHTS DEM (based on LAMMPS), GranOO and YADE.
To mitigate such limitations, a few code generation approaches were developed for both MD and DEM.

\subsection{Code Generation Frameworks}
\label{sec:codegen}

% TODO: OpenMM, MDL, ppmd, OpenFPM, PPME
% TODO: MESA-PD, tinyMD

Solutions based on code generation are also available for specific fields of particle simulations, and these tend to provide a higher level of extensibility while keeping pace with of performance in modern accelerators.

OpenMM is a \ac{MD} package that makes use of Python as a front-end for simulation protocols and file I/O, it contains a C++ API for forces and integrators as well as optimized kernel implementations for CUDA, C++ and OpenCL.
Integrators and forces can be implemented by using mathematical descriptions in OpenMM, but its use cases are restricted to \ac{MD} simulations and several features required for a DEM simulation or other types of particles simulation (such as contact history and different shapes) are not available.

MESA-PD is a general particle dynamics framework that is built-in the waLBerla framework for the simulation of particles and fluids.
Its design principle of separation of code and data allows to introduce arbitrary interaction models with ease, which makes it also useful for \ac{MD}.
By using its code generation approach using Jinja templates it can be adapted very effectively to any simulation scenario.
As successor of the pe rigid particle dynamics framework it inherits its scalability and advanced load balancing functionalities.
One of the limitations of MESA-PD is that it does not support simulations in the GPU, and since its code generation approach is based on Jinja templates, the effort for implementing new functionalities may not be lower compared to other simulation packages without code generation techniques (for instance, most of its force kernels are not generated but rather implemented in C++).

In previous work we developed tinyMD, a proxy-app (also based on miniMD) created to evaluate the portability of MD and DEM applications with the AnyDSL framework.
TinyMD uses higher-order functions to abstract device iteration loops, data layouts and communication strategies, providing a domain-specific library to implement pair-wise interaction kernels that execute efficiently on multi-CPU and multi-GPU targets.
One of tinyMD's limitation is that its flexibility and optimization settings are restricted to what can be achieve through higher-order functions and the AnyDSL compilation framework (the generated LLVM IR restricts the backend compilation to the Clang compiler), overcoming such limitation is one of the reasons we develop P4IRS.


\subsection{Performance Evaluation}
\label{sec:perfeval}

Several performance investigation researches for MD systems exist, most of them using MD packages as a black-box and adjusting different simulation parameters to analyse their implications in the overall performance.
In some cases, however, in-depth performance studies were made to gain a broader understanding on efficient ways to achieve SIMD vectorization with SSE/AVX instructions.
Most in-depth studies are performed using mini-apps or proxy-apps, with miniMD being worth to mention as it provides a simple copper face-cubic-centered lattice simulation using the kernels extracted from LAMMPS.
Our performance-focused prototyping harness MD-Bench is a toolbox developed for performance-engineering \ac{MD} non-bonded short-range force calculation algorithms, with focus on in-core execution on CPUs and GPUs.
It currently supports the Verlet Lists and Cluster Pair algorithms from LAMMPS and GROMACS, respectively.
One of our goals is to use the knowledge obtained with MD-Bench and integrate it into P4IRS to achieve state-of-the-art performance and flexibilty altogether in particle simulations.

DEM simulations, however, do not contain many in-depth performance studies, and most of them focus on evaluating distributed-memory parallelism in weak-scaling settings.
With P4IRS, we intend to bridge the gap between performance strategies across different simulation domains, and use the most efficient algorithm whenever it is possible, without requiring significant efforts such as re-implementing kernels and re-designing applications with tailored data structures for improving performance.

To evaluate performance in a single CPU core or GPU in this paper, we use MD-Bench and miniMD as the baseline performance, as they are well-known implementations that reach state-of-the-art performance for MD simulations.
For DEM cases, we use MESA-PD as the comparison target, since our focus is to show that we can achieve performance-portability with the flexibility that MESA-PD provides. 

\section{Background and Theory}
\label{sec:background}

Particle simulations consist of solving N-body problems and has a wide range of applications as many fields can be represented as a system of interacting particles.
In this sense, particles can be either atoms, rigid bodies or even planets, stars and galaxies.
Interaction contributions can come either from inter-atomic van-der-Walls forces based on the Lennard-Jones potential, or even from gravitational forces generated by super-massive particles.
In order to simulate such systems, however, we need well-defined and closed system of particles with known boundary conditions.
In the scope of this paper, particle contributions are computed for particles that lie within a range or ``cutoff radius''. 
Strategies to compute approximations from particles beyond this range (also known as long-range contributions) also exist, but are not disclosed.
To provide distinct examples of simulations and display a certain level of flexibility from P4IRS, we focus on MD and DEM simulations in this work.

\subsection{Molecular Dynamics}
\label{sec:md}

\ac{MD} simulations are widely used today to study the behavior of microscopic structures.
These simulations reproduce the interactions among atoms in these structures on a macroscopic scale while allowing us to observe the evolution in the system of particles in a time-scale that is simpler to analyze.

Different areas such as material science to study the evolution of the system for specific materials, chemistry to analyze the evolution of chemical processes, or biology to reproduce the behavior of certain proteins and bio-molecules resort to simulations of \ac{MD} systems.

A \ac{MD} system to be simulated constitutes of a number of atoms, the initial state (such as the atoms' position or velocities), and the boundary conditions of the system.
Here, we use \ac{PBC} in all directions, hence when one atom crosses the domain, it reappears on the opposite side with the same velocity.
%These simulations are defined by establishing the number of atoms in a system, the initial state of the system (such as the atoms' positions or velocities), and the boundary conditions for the system.
Fundamentally, atom trajectories in classical \ac{MD} systems are computed by integrating Newton-Euler equations for every atom $i$:
\begin{equation}
    \hat F_i = m \dot{\hat v}_i \label{eq:newton_force}
\end{equation}
\begin{equation}
    \hat v_i = m \dot{\hat x}_i \label{eq:newton_velocity}
\end{equation}
The forces $F_i$ are described by the negative gradient of the potential of interest.
The mutual force between atom $i$ and $j$ as governed by the \ac{LJ} potential
is given as:
\begin{equation}
    \hat{F}_{LJ}(\hat{x_i}, \hat{x_j}) = -\frac{24\varepsilon}{x_{ij}} \left( \frac{\sigma}{x_{ij}} \right)^{6} \left[ 2\left(\frac{\sigma}{x_{ij}}\right)^{6} - 1\right] \hat{x}_{ij}~,
    %V_{LJ} = 4\varepsilon \left[ \left(\frac{\sigma}{r}\right)^{12} - \left(\frac{\sigma}{r}\right)^6 \right]
    \label{eq:lennard_jones}
\end{equation}
Where $\hat{x}_{ij}$ is the distance vector between atoms $i$ and $j$, $x_{ij}$ is the distance between atoms $i$ and $j$, $\varepsilon$ is the width of the potential well, and $\sigma$ specifies at which distance the potential is~$0$.

\subsection{Discrete Element Method}
\label{sec:dem}

Besides computing trajectories for zero-dimensional point masses as done in MD, we can model particles as objects with specific shapes and spatial extension using the Discrete Element Method (DEM).
This requires the simulation tool to support more degrees of freedom for particles, since spatial properties (such as rotation) need to be taken into consideration.
In this work, we consider particles with homogeneous density, and then use the center of mass for each particle as its position, which due to its density properties coincides with its geometrical center.
Equations of motions are extended with the following terms for the torque and angular velocity:

\begin{equation}
    \hat{\tau}{_i} = I \dot{\hat \omega}_i + {\hat \omega}_i \times I {\hat \omega}_i \label{eq:newton_torque}
\end{equation}
\begin{equation}
    \hat{\omega}{_i} = \dot{\hat{\varphi}}_i \label{eq:newton_angular_velocity}
\end{equation}

Also, a collision detection step is also needed to check when two particles overlap.
When particles overlap, a collision is detected and information regarding the collision such as the contact point and penetration depth are calculated.
Note that there may be penetration when a collision happens because there is no guarantee that it is detect right when it starts, as the simulation is discretized by the timestep size.
For simplification purposes, we only calculate collision detections analytically in this work since there are no particles with complex shapes involved.

In this work, we use the penalty-based Linear Spring-Dashpot model to calculate forces among particles for the DEM cases.
The collision force is splitted into two components, the force in the normal direction and in the tangential direction.

\begin{equation}
    \hat{F}_{SD}^{n}(\hat{x_i}, \hat{x_j}) = k^{n}\delta_{ij}\hat{n}_{ij} - \xi^{n}\hat{u}_{ij}^{n}
    \label{eq:linear_spring_dashpot_normal}
\end{equation}

\begin{equation}
    \hat{F}_{SD}^{t}(\hat{x_i}, \hat{x_j}) = -\min \left\{ k^{t}\left| \delta^{t}_{ij} \right|,\mu\left|\hat{F}_{SD}^{n}(\hat{x_i}, \hat{x_j}) \right| \right\} \hat{\delta}^{t}_{ij}
    \label{eq:linear_spring_dashpot_tangential}
\end{equation}

\begin{equation}
    \hat{\delta}^{t}_{ij} \gets \hat{\delta}^{t}_{ij} + \hat{u}_{ij}^{t}dt
    \label{eq:update_rule}
\end{equation}

Where $k^{n}$ is the spring stiffness, $\xi^{n}$ the normal velocity damping and $\mu$ the friction.
The update rule \autoref{eq:update_rule} and modifications in \autoref{eq:linear_spring_dashpot_tangential} were properly included to fix frictional contact accordingly.
The update rule is carried out every time-step the contact persists, and therefore a contact history data structure is needed to properly compute it from the algorithmic point of view.

\subsection{Optimization Strategies}
\label{sec:opts}

To obtain the force for a particle in a specific time-step, it is necessary to compute its partial contributions from each of the other particles present in the system.
In a naive strategy, this results in a quadractic complexity and severely affects the application performance, which limits the system sizes and time-scales of such simulations.

When force contributions become negligible at long-distance interactions, different algorithm strategies can be employed to perform computations only for particles within a specific cut-off region.
There are two main data structures which are commonly used for efficiently computing particle interactions in state-of-the-art simulation packages: the Linked Cells and the Verlet Lists.

Linked Cells strategy sorts atoms spatially according to their position in cells (also called bins), where cells have a fixed size, and mapping particles into cells can be done directly by computing in which cell a particle is contained within.
After, forces can be computed for each particle by traversing its current and neighbor cells, and only adding contributions for particles within these cells.

The Linked Cells results in computing only forces for particles within a region that is discretized by their current and neighbor cells/bins, which can still results in a significant amount of redundant computations.
In order avoid even more redundant computations, the Verlet Lists strategy can be used jointly with Linked Cells.
The Verlet Lists constructs a list of neighbor particles for every particle, and this list can be created by using the Linked Cells structure, and doing particle-particle distance comparison to evaluate wether a particle should be added into the list.
At the end, the region for each particle search is defined by a sphere.
Note that for the neighbor-lists construction to be worthwhile, it should be used more than once force calculation, hence neighbor-lists are not created at every simulation time-step, and a "skin" can be added to the cutoff radius to extend the neighbor-search and avoid missing computations from atoms that end up entering the cut-off region.

Despite avoiding computations from long-distance particles, another common optimization is to take advantage of Newton's Third Law and computing forces in only one direction.
This means that only half of particle-pairs are traversed during force computation, and forces are decreased for the neighbor atom during their computation.
Ideally, this should lead to a factor of two speedup, but this strategy has a few drawbacks: (a) parallelism is harmed since decreasing force of neighbor atoms (j) requires atomic operations to avoid race conditions and (b) gather and scatter instructions are needed to build up the SIMD registers with forces of the neighbor atoms (j), which results in scattered accesses and extra instructions.
At the end, this strategy is likely to be beneficial for more arithmetic-intensive kernels, and the trade-off between parallelism and less arithmetic operations must be assessed.

\section{P4IRS}
\label{sec:pairs}

% Show examples (LJ, EAM)
% Linked Cells / Verlet Lists
% Can generate CPU/GPU code
% Vectors, quaternions and matrices
% Properties, feature properties and contact properties
% Contact history
% Shape partitioning

\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{pairs_overview.png}
  \caption{Overview of P4IRS architecture.}
  \label{fig:features}
\end{figure*}

P4IRS is a standalone compiler and domain-specific language for particle simulations which aims at generating optimized code for different target hardwares.
It is released as a Python package and allows users to define kernels, integrators and other particle routines in a high-level and straightforward fashion.

\autoref{fig:features} displays an overview of P4IRS.
In the left side, the front-end library from P4IRS is displayed with the available features, all these are defined through a Python interface by the user for modeling the simulation and determining the optimization schedule.
In the middle, the backend with the internal methods (deep-embedded in Python) with the compiler transformations and analyses are shown, these make use of the front-end representation to produce the final backend code.
In the right side, the final generated code is integrated and compiled together with the runtime modules and functionalities to achieve parallelism and accesses to external features.
The next subsections explore the different P4IRS features in more detail.

\subsection{Frontend and Features}
\label{sec:frontend}

\autoref{lst:pairslj} shows a simple Lennard-Jones kernel implemented in P4IRS.
Functions can be provided through a method (line 1), and the parameters in the method refer to different particles in the system.
In this way, many-body potentials can also be described using methods with more than two parameters.
Some keywords are also provided by P4IRS for writing the kernels, in line 2 the \emph{squared\_distance} method is used for obtaining the squared distance for the atoms \emph{i} and \emph{j}, and in line 4 the \emph{delta} method is used for obtaining the delta vector.
Also, in lines 3 and 4 it is possible to see accesses to feature properties called \emph{sigma6} and \emph{epsilon}, which are defined during the simulation setup as properties that depends on the atom types feature.
In line 4, the \emph{apply} method is used for applying the computed term into the \emph{force} property of the particles, the method allows the identification of particle properties reductions and is also used when generating the kernel variant using the half neighbor-lists.

\begin{lstlisting}[language=Python,
		   label={lst:pairslj},
		   caption={Lennard-Jones force description in P4IRS.}]
def lj(i, j):
    sr2 = 1.0 / squared_distance(i, j)
    sr6 = sr2 * sr2 * sr2 * sigma6[i, j]
    apply(force, delta(i, j) * (48.0 * sr6 * (sr6 - 0.5) * sr2 * epsilon[i, j]))
\end{lstlisting}

Functions that update every particle separately can also be described with a single parameter.
\autoref{lst:euler} displays how to implement a simple Euler integrator for \ac{MD} (see line 1).
In lines 2-3, there are also property accesses for the particles' linear velocities, forces, masses and positions.
Note that vector operations are intrinsically supported in P4IRS, which is also the case for matrices and quaternions data structures.
P4IRS methods also allow the specification of parameters such as the \emph{dt} specified in line 3, these must be given when setting up the simulation.

\begin{lstlisting}[language=Python,
		   label={lst:euler},
		   caption={Euler integrator description in P4IRS.}]
def euler(i):
    linear_velocity[i] += dt * force[i] / mass[i]
    position[i] += dt * linear_velocity[i]
\end{lstlisting}

\autoref{lst:pairs_md_setup} shows the simulation setup for the \ac{MD} case. In the first part (lines 6-10), a \emph{simulation} instance is created, and its identifier, the list of particle shapes used, the number of time-steps and the floating-point precision are specified.
Further, the properties needed for each particle in the simulation are specified (lines 13-16), together with their data types and (if needed) initial values.
Some properties are also defined as volatile (in this case the \emph{force} at line 16) property, which means they are reset after every time-step.
To add feature properties, it is first required to add the specified feature as shown in line 19 (in this example, each atom has a feature named \emph{type}), and then specifiy the feature name for these properties (see lines 20-21).
Next, the simulation domain is specified (line 24) and the initial state of the system is defined by reading data from a file (lines 27-30), a list of properties must be given in the same order as they appear in the file, and the particles shape must also be included.
Different entries can be used to read particles with different shapes in the same simulation.

\begin{lstlisting}[language=Python,
		   label={lst:pairs_md_setup},
		   caption={Simple example for MD simulation setup in P4IRS.}]
import pairs

# ...

# Simulation setup
psim = pairs.simulation(
	"md", 			# Simulation identifier
	[pairs.point_mass()], 	# List of shapes
	timesteps=200,		# Number of time-steps
	double_prec=True)	# Use double-precision

# Particle properties
psim.add_position('position')
psim.add_property('mass', pairs.real(), 1.0)
psim.add_property('linear_velocity', pairs.vector())
psim.add_property('force', pairs.vector(), volatile=True)

# Features and their properties
psim.add_feature('type', ntypes)
psim.add_feature_property('type', 'epsilon', pairs.real(), [...])
psim.add_feature_property('type', 'sigma6', pairs.real(), [...])

# Simulation domain
psim.set_domain([xmin, ymin, zmin, xmax, ymax, zmax])

# Initial state
psim.read_particle_data(
	"data/copper_fcc_lattice.input",
	['type', 'mass', 'position', 'linear_velocity', 'flags'],
	pairs.point_mass())

# Optimization settings
psim.reneighbor_every(20)
psim.compute_half()
psim.build_neighbor_lists(cutoff_radius + skin)
psim.vtk_output(f"output/lj_{target}", every=20)

# Kernels to compute
psim.compute(lj, cutoff_radius)
psim.compute(euler, symbols={'dt': dt})

# Target hardware
if target == 'gpu':
    psim.target(pairs.target_gpu())
else:
    psim.target(pairs.target_cpu())

psim.generate()
\end{lstlisting}

After the properties and simulation settings, the optimization settings (i.e. whether Verlet Lists are used, the reneighboring frequency and if half-neighbor-lists should be used) are specified (see lines 33-35), followed by the directive (line 36) to write data to VTK files at every 20 time-steps.
Finally, the kernels to be computed must be specified using the \emph{compute} method (lines 39-40).
The cutoff radius must be specified for kernels with more than one parameter/particle and other parameters used in such kernels (such as the \emph{dt} in the Euler kernel) must also be provided as a Python dictionary.
The final steps define the target and call the \emph{generate} method to then trigger the code generator.
P4IRS provide targets with default settings via the \emph{target\_cpu} and \emph{target\_gpu} settings, but custom targets can be defined with their own options such as the thread and blocks configuration.

%\begin{lstlisting}[language=Python,
%		   label={lst:pairslj},
%		   caption={Lennard-Jones force description in P4IRS.}]
%\end{lstlisting}

\subsection{Contact History and Contact Properties}
\label{sec:contact_history}

When implementing DEM kernels such as the Linear Spring-Dashpot, properties computed during two contacted particles must be used across subsequent time-step interactions whenever they happen.
For this reason, P4IRS supports contact properties, which are properties tighted to a contact in the simulation.
These can be simply added through the \emph{add\_contact\_property} method as can be seen in \autoref{lst:contactprops1} for the sticking, tangential spring displacement and the impact velocity magnitude:

\begin{lstlisting}[language=Python,
		   label={lst:contactprops1},
		   caption={Setup example for contact properties.}]
psim.add_contact_property('is_sticking', pairs.int32(), 0)
psim.add_contact_property('tangential_spring_displacement', pairs.vector(), [0.0, 0.0, 0.0])
psim.add_contact_property('impact_velocity_magnitude', pairs.real(), 0.0)
\end{lstlisting}

These properties can be used directly in kernels in the same way as feature properties.
\autoref{lst:contactprops2} shows an example of their usage in the Linear Spring-Dashpot kernel.
Note that when such properties are not stored for the particle pair, then the default values are always used.

\begin{lstlisting}[language=Python,
		   label={lst:contactprops2},
		   caption={Setup example for contact properties.}]
def linear_spring_dashpot(i, j):
    # ...
    tan_spring_disp = tangential_spring_displacement[i, j]
    impact_vel_magnitude = impact_velocity_magnitude[i, j]
    impact_magnitude = select(impact_vel_magnitude > 0.0, impact_vel_magnitude, length(rel_vel))
    # ...
    tangential_spring_displacement[i, j] = new_tan_spring_disp
    impact_velocity_magnitude[i, j] = impact_magnitude
    is_sticking[i, j] = n_sticking
\end{lstlisting}

To provide fast and efficient accesses of contact properties when using neighbor-lists, P4IRS keep the particles in the same indexes in both data structures.
This prevents a lookup operation within the kernel for finding the contact index, and also keeps data access contiguous for the contact history data structure.
When Linked Cells are used without a Verlet Lists, such optimizations are not possible, which should significantly impact the performance for the kernel.

%\begin{lstlisting}[language=Python,
%		   label={lst:pairslj},
%		   caption={Lennard-Jones force description in P4IRS.}]
%\end{lstlisting}

\subsection{Backend and Code Generation}
\label{sec:backend}

Generating the final backend code for the specified kernels with proper parallelism, optimization strategies and GPU offloading is challenging.
P4IRS uses its own \ac{IR} to facilite this process, where internal routines to build the cell lists, neighbor lists and generate the communication buffers are deep-embedded into Python to further be mapped into the P4IRS \ac{AST}.
Afterwards, compiler analyses and transformations are performed to properly add parallelism and optimizations into the code.
In its current state, P4IRS generates C++ and CUDA code with MPI for distributed-memory parallelism, but new backend languages, intermediate representations and technologies can be included without difficulties by extending its code generator.

Functions that require external libraries and usually run on the CPU (such as reading particle data from file and writing data to VTK files) can be implemented using P4IRS runtime routines.
P4IRS provide a C++ instance that contains all the meta-data for the simulation, which can be useful for such routines and provide a simple way to integrate functionalities that do not need extensive optimizations and may rely on external libraries.

\begin{lstlisting}[language=C++,
		   label={lst:pairsruntime},
	   	   caption={P4IRS runtime routine example for writing VTK data into a file.}]
void vtk_write_data(PairsSimulation *ps, const char *filename, int start, int end, int timestep) {
    std::string output_filename(filename);
    auto position = ps->getAsVectorProperty(ps->getPropertyByName("position"));
    // ...
    filename_oss << filename << "_";
    if(ps->getDomainPartitioner()->getWorldSize() > 1) {
        filename_oss << ps->getDomainPartitioner()->getRank() << "_";
    }
    // ...
    filename_oss << timestep << ".vtk";
    std::ofstream out_file(filename_oss.str());
    // ...
    ps->copyPropertyToHost(position);
    for(int i = start; i < end; i++) {
        out_file << position(i, 0) << " ";
        out_file << position(i, 1) << " ";
        out_file << position(i, 2) << "\n";
    }
    // ...
}
\end{lstlisting}

\autoref{lst:pairsruntime} shows an example for the routine to write data into VTK files, note that the first parameter is a \emph{PairsSimulation} object, and can be used to check whether properties exist, check if they are synced with the host device and (in case not) copy the data from the host.
The meta-data object is feed at the start of the simulation with the code generated by P4IRS, and it is therefore dependent on each simulation.

\subsection{Shape Partitioning for SIMD Improvement}
\label{sec:shape_partitioning}

When computing forces for DEM simulations that contain different shapes, different calculation formulas are needed for some of the contact elements such as the penetration depth, contact point and distance.
From a computational point of view, this introduces branching in the inner-most loop where each path leads to different arithmetic instructions be executed and therefore difficults SIMD vectorization.
Algorithm \autoref{alg:branching} displays an example of such issue for a kernel with spheres and half-spaces.

\begin{algorithm}[H]
  \caption{Example kernel with branching for different shapes.}
  \label{alg:branching}
  \begin{algorithmic}[1]
    \For{$i$ \textbf{in} \texttt{sphere\_particles()}}
      \For{$j$ \textbf{in} \texttt{neighbors($i$)}}
        \If{\texttt{shape($j$) == SPHERE}}
          \State \texttt{// Sphere-Sphere interaction}
        \EndIf
        \If{\texttt{shape($j$) == HALFSPACE}}
          \State \texttt{// Sphere-Halfspace interaction}
        \EndIf
        \State \texttt{// ...}
      \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

To provide a simpler way to achieve vectorized code for such DEM kernels, we partition particles by shape in our acceleration data structures (Linked Cells and Verlet Lists), which allows distinguishing them during the force computation.
After, we use code generation to apply a loop-splitting in the generated code for every pair of shapes, these resulting loops should be then easier to vectorize since no branches are needed to calculate different contact elements for the particles being computed.
Algorithm \autoref{alg:no_branching} displays the same kernel as Algorithm \autoref{alg:branching} without the branching issue.
Note that the outer-most loop can also be splitted into two parts as well, which leads to two different kernels for each pair of shapes.
For GPUs, this can be beneficial since different threads will be used for different pair of shapes (parallelism is done at the level of the outer-most loop) and no threads will be disabled due to traversing a different code path.

\begin{algorithm}[H]
  \caption{Example kernel without branching for different shapes.}
  \label{alg:no_branching}
  \begin{algorithmic}[1]
    \For{$i$ \textbf{in} \texttt{sphere\_particles()}}
      \For{$j$ \textbf{in} \texttt{neighbor\_spheres($i$)}}
        \State \texttt{// Sphere-Sphere interaction}
      \EndFor
      \For{$j$ \textbf{in} \texttt{neighbor\_halfspaces($i$)}}
        \State \texttt{// Sphere-Halfspace interaction}
      \EndFor
      \State \texttt{// ...}
    \EndFor
  \end{algorithmic}
\end{algorithm}

Note that there is no guarantee that the internal loop will be vectorized, since there may still be branches and instructions that make it not possible.
Besides, the backend compiler must be able to properly analyze and apply the transformations to actually generate the vectorized code, which may not happen even when the code is amenable to vectorization.

\subsection{Domain Partitioning and Communication}
\label{sec:domain_partitioning}

In order to attain distributed-memory parallelism and software flexibility without injuring performance, P4IRS provides a customizable and robust communication module that facilitates integrating new domain partitioning schemes into its generated simulations.
This is done by combining its runtime functions and code generation techniques, and allows the usage of the waLBerla domain-partitioning scheme based on a block-forest data structure.

Essentially, P4IRS communication is split into three routines:

\begin{itemize}
    \item \emph{borders:} define particles within the halo region which must be send to neighbor ranks as "ghost" particles; only non-volatile properties that are accessed in a neighbor particle within a kernel are sent.
    \item \emph{synchronize:} uses the defined particle in the borders routine to exchange data; only non-volatile properties that are accessed in neighbor particles within a kernel and that change across time-steps are sent.
    \item \emph{exchange:} send particles that overlap the current rank's domain, it becomes a local particle in the neighbor rank; all properties except volatiles are sent; contact history data is also exchanged.
\end{itemize}

These methods make use of a domain-partitioner class that can be implemented in P4IRS runtime to determine which particles must be sent.
The method is implemented in C++ and fill-in domain buffers that are used for evaluating if a particle is outside the domain or within the border using its position.
This means that to include new domain partitioning schemes in P4IRS, it is only necessary to define a C++ runtime class, which can make use of external libraries and therefore can use the waLBerla block-forest domain-partitioning.
Note that the reason for using buffers instead of evaluating the particle positions directly in the runtime code is performance, since particle packing and unpacking kernels should be executed in the target processor to achieve parallelism and to avoid copying data back and forth from the accelerator and host processors.

\section{Evaluation}
\label{sec:evaluation}

% Testbed
%   - Ice Lake, Milan, A40, A100
%   - Fritz: Intel(R) oneAPI DPC++/C++ Compiler 2022.1.0 (2022.1.0.20220316)
%     - -Ofast -xCORE-AVX512 -qopt-zmm-usage=high (Ice Lake)
%   - Alex: icpc (ICC) 2021.6.0 20220226
%     - -Ofast -xHost -qopt-zmm-usage=high (Milan) 

\subsection{Materials and Methods}

In this paper we conducted experiments in FAU's clusters Fritz (CPU) and Alex (GPU), these systems have the following configurations:

\begin{itemize}
  \item \textbf{Fritz:} 992 compute nodes, each one with two Intel Xeon Platinum 8360Y “Ice Lake” processors (36 cores per chip) running at a base frequency of 2.4 GHz and 54 MB Shared L3 cache per chip, 256 GB of DDR4 RAM.
  \item \textbf{Alex:} 44 GPGPU nodes, each with two AMD EPYC 7713 “Milan” processors (64 cores per chip) running at 2.0 GHz with 256 MB Shared L3Cache per chip, 512 GB of DDR4-RAM, eight Nvidia A40 (each with 48 GB DDR6 @ 696 GB/s; 37.42 TFlop/s in FP32), 25 GbE, and 7 TB on local NVMe SSDs.
\end{itemize}

We perform single-core CPU experiments in both clusters, and refer to both CPUs listed above as Ice Lake and Milan, respectively.
For single GPU experiments, we use the referred A40 Nvidia GPUs, as well as the Nvidia A100 also available in the Alex cluster.

Experiments for \ac{MD} use the standard Copper FCC lattice testcase from miniMD/MD-Bench, and we use our results from MD-Bench as a performance reference since it is a well optimized code for \ac{MD}.
The \ac{DEM} test case is a "Settling Spheres" benchmark, where several spheres fall into a "ground" (half-space), thus generating a "bed" of spherical particles.
Due to its multiple shapes and dynamics that consists of many interactions among spheres and half-spaces, it should provide a good reference for DEM simulations.
We compare our single-core results from P4IRS with MESA-PD using 2 MPI ranks since it is not possible to run this setup in MESA-PD using a single rank.
GPU results are not available for MESA-PD as it is not capable of generating GPU code, but we show the performance benefits when using GPUs for P4IRS generated codes.

Scaling results are also shown for demostrating P4IRS scaling capabilities.
For this we use regular domain-partitioning since these experiments do not require sophisticated load-balancing algorithms (Copper lattice is an homogeneous testcase in all dimensions, and Settling Spheres can obtain good weak-scaling by regularly partitioning the domain in X and Y dimensions).
We use double floating-point precision and display results when enabling and disabling Newton's 3rd Law optimization (half neighbor-lists).
For \ac{MD} we show results for the Verlet Lists case since it is widely employed in such simulations, and for \ac{DEM} we stick to Linked Cells as this is the algorithm used in MESA-PD.

\subsection{Molecular Dynamics}

% MD
%   - Single core/GPU (vs MD-Bench) --- one figure for CPU and one for GPU
%     - Full and half-neighbor lists
%     - Runtime, MFLOP/s, Vectorization ratio, # instructions, ... (table)
% Weak-scaling
%   Fritz (figure) - base = 128x128x128 per node
%     - Full and half-neighbor lists
%   Alex (figure)
%     - Full and half-neighbor lists
%     - A40 and A100

\colorlet{force-cpu}{teal!80}
\colorlet{neigh-cpu}{teal!50}
\colorlet{other-cpu}{teal!20}

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.95]
    \pgfplotsset{
        ybar stacked, ymin=0, ymax=32, xmin=0.5, xmax=2.5, xtick=data,
        xtick={1,...,2},
        xticklabels={Ice Lake, Milan},
        xticklabel style={yshift=-10pt},
        ylabel={time to solution (s)}, ylabel style={yshift=-1ex},
        legend cell align={left},
        /pgf/bar width=12pt,% bar width
        scatter/position=absolute,
        node near coords style={
            font=\footnotesize,
            at={(axis cs:\pgfkeysvalueof{/data point/x},\pgfkeysvalueof{/pgfplots/ymin})},
            anchor=north,
            yshift={-\pgfkeysvalueof{/pgfplots/major tick length} + 4pt},
        },
    }
    \begin{axis}[bar shift=-18pt, nodes near coords style={xshift=-18pt},
        legend pos = outer north east, legend style = {name = pairs}]
        \addplot [fill=force-cpu, nodes near coords=A] table [x=arch, y=pairsfn] {data/single-core-force.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=pairsfn] {data/single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=pairsfn] {data/single-core-other.csv};
        \legend{Force, Neigh, Other}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{A:} P4IRS (FN)}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{B:} MD-Bench (FN)}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{C:} P4IRS (HN)}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{D:} MD-Bench (HN)}
    \end{axis}
    \begin{axis}[bar shift= -6pt, nodes near coords style={xshift=-6pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-cpu, nodes near coords=B] table [x=arch, y=mdbenchfn] {data/single-core-force.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=mdbenchfn] {data/single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=mdbenchfn] {data/single-core-other.csv};
    \end{axis}
    \begin{axis}[bar shift= 6pt, nodes near coords style={xshift=6pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-cpu, nodes near coords=C] table [x=arch, y=pairshn] {data/single-core-force.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=pairshn] {data/single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=pairshn] {data/single-core-other.csv};
    \end{axis}
    \begin{axis}[bar shift= 18pt, nodes near coords style={xshift=18pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-cpu, nodes near coords=D] table [x=arch, y=mdbenchhn] {data/single-core-force.csv};
        \addplot [fill=neigh-cpu] table [x=arch, y=mdbenchhn] {data/single-core-neigh.csv};
        \addplot [fill=other-cpu] table [x=arch, y=mdbenchhn] {data/single-core-other.csv};
    \end{axis}
\end{tikzpicture}
\vspace{-3ex}
	\caption{Execution time for P4IRS and MD-Bench simulations in seconds (lower is better) of a Copper FCC lattice system with $32^{3}$ unit-cells (4 atoms per unit-cell) during 200 time-steps on Ice Lake and Milan CPUs. Tests were performed in a single core (no parallelism) with fixed frequency. Results are shown for both full and half neighbor-lists (FN/HN).}
\vspace{-2ex}
\centering
\label{fig:md_single_core_runtimes}
\end{figure}

\autoref{fig:md_single_core_runtimes} display the runtime measurements for P4IRS and MD-Bench simulations of the Copper FCC lattice case with $32^{3}$ unit cells (4 atoms per unit-cell).
It is noticeable that the kernels generated by P4IRS achieve competitive results to MD-Bench, a hand-tuned implementation for MD cases, which demonstrates P4IRS capabitilies to generate fast code.
In Ice Lake, The performance is slightly better for MD-Bench using \ac{FN}, but P4IRS provide a slightly faster result when using \ac{HN}.
P4IRS code tend to be build neighbor-lists faster but the force computation is slower, but runtime differences vary in the range of 10\% for the measured runtime, which is still not too significant and can vary when using different compilers and flags.
In Milan, P4IRS is faster than MD-Bench for the \ac{FN}case, and results are pretty similar when using \ac{HN} case.
The force calculation is still slower for P4IRS with \ac{HN} when compared to MD-Bench, but this is compensated by the less time spent building the neighbor lists.

\begin{table*}[htb]
    \centering
    \begin{tabular}{c|c|c|c|c}
%        \hline
        Variant & pairs-fn & pairs-hn & mdbench-fn & mdbench-hn \\
        \hline
        Runtime [\seconds] & 23.26 & 12.36 & 20.89 & 12.25 \\
        AVX512 Perf. [\GFS] & 2.866 & --- & 3.192 & --- \\
        Scalar Perf. [GUOPS/s] & --- & 1.777 & --- & 1.794 \\
        CPI & 3.597 & 0.8292 & 2.9961 & 0.7211 \\
        Total instructions $(\times 10^9)$ & $15.442$ & $41.668$ & $16.666$ & $40.587$ \\
        FP instructions $(\times 10^9)$ & $8.416$ & $21.983$ & $8.416$ & $21.985$ \\
        Arithmetic ratio [\%] & 54.49 & 52.75 & 50.50 & 54.16 \\
        Vectorization ratio [\%] & 99.0574 & 0.0 & 99.0609 & 0.0 \\
%        \hline
    \end{tabular}
    \caption{Measurements from P4IRS and MD-Bench Lennard-Jones force kernels for both full and half neighbor-lists cases on a single Ice Lake CPU core.}
    \label{tab:md_single_core_stats}
\end{table*}

\autoref{tab:md_single_core_stats} displays HPM measurements for the force calculation kernels in Ice Lake with AVX512.
The important thing to notice is that the compiler was not able to vectorize the cases with \ac{HN}, but the runtimes are still smaller for these cases.
The performance with \ac{HN} is significantly worse but it is compensated by the fact it computes roughly half the interactions.
The arithmetic instructions ratio for all cases lie between 50-55\%, which means that about half of the instructions are used for organizing data into the vector registers.
For more computational-intensive kernels, the arithmetic ratio should increase and the measured AVX512/Scalar performance should have a more significant impact in the runtime.


\colorlet{force-gpu}{purple!80}
\colorlet{neigh-gpu}{purple!50}
\colorlet{other-gpu}{purple!20}

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.95]
    \pgfplotsset{
        ybar stacked, ymin=0, ymax=1.5, xmin=0.5, xmax=2.5, xtick=data,
        xtick={1,...,2},
        xticklabels={A40, A100},
        xticklabel style={yshift=-10pt},
        ylabel={time to solution (s)}, ylabel style={yshift=-1ex},
        legend cell align={left},
        /pgf/bar width=12pt,% bar width
        scatter/position=absolute,
        node near coords style={
            font=\footnotesize,
            at={(axis cs:\pgfkeysvalueof{/data point/x},\pgfkeysvalueof{/pgfplots/ymin})},
            anchor=north,
            yshift={-\pgfkeysvalueof{/pgfplots/major tick length} + 4pt},
        },
    }
    \begin{axis}[bar shift=-12pt, nodes near coords style={xshift=-12pt},
        legend pos = outer north east, legend style = {name = pairs}]
        \addplot [fill=force-gpu, nodes near coords=A] table [x=arch, y=pairsfn] {data/single-gpu-force.csv};
        \addplot [fill=neigh-gpu] table [x=arch, y=pairsfn] {data/single-gpu-neigh.csv};
        \addplot [fill=other-gpu] table [x=arch, y=pairsfn] {data/single-gpu-other.csv};
        \legend{Force, Neigh, Other}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{A:} P4IRS (FN)}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{B:} MD-Bench (FN)}
        \addlegendimage{empty legend}
        \addlegendentry{\hspace{-.325cm}\textbf{C:} P4IRS (HN)}
    \end{axis}
    \begin{axis}[bar shift= -0pt, nodes near coords style={xshift=-0pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-gpu, nodes near coords=B] table [x=arch, y=mdbenchfn] {data/single-gpu-force.csv};
        \addplot [fill=neigh-gpu] table [x=arch, y=mdbenchfn] {data/single-gpu-neigh.csv};
        \addplot [fill=other-gpu] table [x=arch, y=mdbenchfn] {data/single-gpu-other.csv};
    \end{axis}
    \begin{axis}[bar shift= 12pt, nodes near coords style={xshift=12pt},
        legend style = {at = {([yshift = -1mm]pairs.south west)},
        anchor = north west}]
        \addplot [fill=force-gpu, nodes near coords=C] table [x=arch, y=pairshn] {data/single-gpu-force.csv};
        \addplot [fill=neigh-gpu] table [x=arch, y=pairshn] {data/single-gpu-neigh.csv};
        \addplot [fill=other-gpu] table [x=arch, y=pairshn] {data/single-gpu-other.csv};
    \end{axis}
\end{tikzpicture}
\vspace{-3ex}
	\caption{Execution time for P4IRS and MD-Bench simulations in seconds (lower is better) of a Copper FCC lattice system with $50^{3}$ unit-cells (4 atoms per unit-cell) during 200 time-steps on A40 and A100 GPUs. Results are shown for both full and half neighbor-lists (FN/HN) cases, and atomic operations affect the performance for the HN case.}
\vspace{-2ex}
\centering
\label{fig:md_single_gpu_runtimes}
\end{figure}

\autoref{fig:md_single_gpu_runtimes} displays the runtime measurements for P4IRS and MD-Bench simulations in the A40 and A100 GPUs from the Alex cluster.
MD-Bench still do not have a GPU version with \ac{HN}, so we just display it using \ac{FN}.
For A40, there is no significant difference for all runs, with MD-Bench being slightly faster.
The performance difference is larger for A100, where MD-Bench is significantly faster.
The usage of atomics indeed harms the performance since \ac{HN} computes half the interactions in the same amount of time in A40, and this impact is significant worse in A100 since it takes about twice the amount of time to compute half the interactions.
Note again that this is strictly dependent on the arithmetic intensity of the kernel, which is relatively small for the used \ac{LJ} kernels, for other cases it may be worth to make use of \ac{HN}.

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.80]
    \tikzstyle{every node}=[font=\small]
    \begin{axis}[
            width=\textwidth, height=8cm,
            xmode=log,log basis x={2},
            %xmin=0.75,   xmax=3072,
            xmin=1,   xmax=64,
            ymin=10,   ymax=30,
            log ticks with fixed point,
            xlabel={\# nodes}, xlabel style={yshift= 1ex},
            xticklabel={ % hack for precision issues with 1024 and 2048
                \pgfkeys{/pgf/fpu=true}
                \pgfmathparse{int(2^\tick)}
                \pgfmathprintnumber[fixed]{\pgfmathresult}
            },
            ylabel={time to solution (s)}, ylabel style={yshift=-1ex},
            grid=major,
            legend pos=north east,
            %legend style={at={(0.97,0.5)},anchor=east},
        ]
        \addplot table[x=nodes,y=pairsfn] {data/md-scaling-fritz.csv};
        \addplot table[x=nodes,y=pairshn] {data/md-scaling-fritz.csv};
        \legend{P4IRS-FN, P4IRS-HN}
    \end{axis}
\end{tikzpicture}
\vspace{-3ex}
\caption{Weak-scaling results for P4IRS (FN and HN) on Fritz, each node simulates 200 time-steps of a Copper FCC lattice system with $128^3$ unit cells, with approximately 116.508 atoms per CPU core.}
\vspace{-2ex}
\label{fig:md_weak_scaling_fritz}
\end{figure}

\autoref{fig:md_weak_scaling_fritz} shows the results for P4IRS weak-scaling in the Fritz supercomputer.
For each node, a system of $128^3$ unit cells is included, with roughly 8.388.608 atoms per node (or 116.508 atoms per CPU core).
Results demonstrate that P4IRS achieve near perfect weak-scaling in all 64 nodes of the Fritz cluster (4608 CPU cores), which suggests it is fully capable of generating scalable applications to run on modern supercomputers.

\begin{figure}[t]
\centering
\begin{tikzpicture}[scale=0.80]
    \tikzstyle{every node}=[font=\small]
    \begin{axis}[
            width=\textwidth, height=8cm,
            xmode=log,log basis x={2},
            %xmin=0.75,   xmax=3072,
            xmin=1,   xmax=8,
            ymin=4,   ymax=20,
            log ticks with fixed point,
            xlabel={\# nodes}, xlabel style={yshift= 1ex},
            xticklabel={ % hack for precision issues with 1024 and 2048
                \pgfkeys{/pgf/fpu=true}
                \pgfmathparse{int(2^\tick)}
                \pgfmathprintnumber[fixed]{\pgfmathresult}
            },
            ylabel={time to solution (s)}, ylabel style={yshift=-1ex},
            grid=major,
            legend pos=north east,
            %legend style={at={(0.97,0.5)},anchor=east},
        ]
        \addplot table[x=nodes,y=pairsfna40] {data/md-scaling-alex.csv};
        \addplot table[x=nodes,y=pairshna40] {data/md-scaling-alex.csv};
        \addplot table[x=nodes,y=pairsfna100] {data/md-scaling-alex.csv};
        \addplot table[x=nodes,y=pairshna100] {data/md-scaling-alex.csv};
        \legend{P4IRS-FN-A40, P4IRS-HN-A40, P4IRS-FN-A100, P4IRS-HN-A100}
    \end{axis}
\end{tikzpicture}
\vspace{-3ex}
\caption{Weak-scaling results for P4IRS (FN and HN) on Alex on both A40 and A100 GPUs, each node simulates 200 time-steps of a Copper FCC lattice system with $100^3$ unit cells, with approximately 500.000 atoms per GPU.}
\vspace{-2ex}
\label{fig:md_weak_scaling_alex}
\end{figure}

\autoref{fig:md_weak_scaling_alex} shows the results for P4IRS weak-scaling in the Alex supercomputer for both nodes with A40 and A100 GPUs.
Each node simulates a system of $100^3$ unit cells, with roughly 500.000 atoms per GPU.
Note there is a significant overhead for communication when using GPUs (compared to solving the problem in a single GPU), which can be attributed to the time do transfer data from the GPUs through the communication channels.
The results for scaling up to 8 nodes (64 GPUs) is good but not perfect (93\% efficiency for \ac{HN} in A100, and 83-85\% efficiency across other cases), one reason for this is that the Alex cluster has significant impact with its interconnect interface among different nodes, which significantly increases the time for the communication.
Running simulations with higher workload on GPUs for the force computation or using different GPU clusters with better interconnection interfaces should improve the weak-scaling results.

\subsection{Discrete Element Method}

% DEM
%   - Comparison with MESA-PD (figure + table)
%     - Linked Cells (ignore Verlet Lists)
%     - Full and half computations
%
%   - Weak-scaling on Alex (figure)
%     - CPU (128 cores) vs GPU (8)

\section{Conclusion and Outlook}
\label{sec:conclusion}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% For citations use: 
%%       \citet{<label>} ==> Jones et al. [21]
%%       \citep{<label>} ==> [21]
%%

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-num-names} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

\begin{thebibliography}{00}

%% \bibitem[Author(year)]{label}
%% Text of bibliographic item

\bibitem[ ()]{}

\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.
