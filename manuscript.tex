%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}
\usepackage{color}
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage[hidelinks]{hyperref}
\usepackage[printonlyused]{acronym}
\usepackage{listings}
\usepackage{algorithm,algpseudocode}
%\usepackage{math}

\journal{Computer Physics Communications}

\lstset{
    showstringspaces=false,
    extendedchars=true,
    basicstyle=\footnotesize\ttfamily,
    commentstyle=\slshape,
    stringstyle=\ttfamily,
    breaklines=true,
    breakatwhitespace=true,
    columns=flexible,
    numbers=left,
    numberstyle=\tiny,
    basewidth=.5em,
    xleftmargin=.5cm,
    captionpos=b,
    frame=lines
}

% Acronyms
\newacro{AoS}{array of structures}
\newacro{AoSoA}{array of structures of arrays}
\newacro{CPI}{cycles per instruction}
\newacro{DEM}{discrete element method}
\newacro{EAM}{embedded atom method}
\newacro{FCC}{face-centered cubic}
\newacro{FN}{full neighbor-lists}
\newacro{HN}{half neighbor-lists}
\newacro{ILP}{instruction-level parallelism}
\newacro{IR}{intermediate representation}
\newacro{AST}{abstract syntax tree}
\newacro{ISA}{instruction set architecture}
\newacro{LJ}{Lennard-Jones}
\newacro{MD}{Molecular dynamics}
\newacro{MPI}{message passing interface}
\newacro{SIMD}{single instruction, multiple data}
\newacro{SoA}{structure of arrays}
\newacro{PBC}{periodic boundary conditions}
\newacro{HPM}{hardware performance monitoring}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{P4IRS: An Intermediate Representation and Compiler for Parallel and Performance-Portable Particle Simulations}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[nhratfau]{Rafael Ravedutti Lucio Machado}
\author[nhratfau]{Jan Eitzinger}
\author[nhratfau]{Harald Köstler}

\affiliation[nhratfau]{
	organization={Erlangen National High Performance Computing Center}, %(NHR@FAU)},
	addressline={Martensstraße 1},
	city={Erlangen},
	postcode={91058},
	state={Bayern},
	country={Germany}}

\begin{abstract}
Various physics simulations today rely on simulating particle interactions, where particles can represent point masses (as in Molecular Dynamics), rigid bodies (as in Discrete Element Method) or even massive bodies such as planets.
Evaluating and calculating the required particle interactions in a simulation is computationally expensive, hence suitable algorithms and proper optimizations to properly exploit available parallelism in the target hardware are important to reach good performance.
However, it is difficult to maintain flexible implementations while keeping state-of-the-art performance, as most packages are developed individually and have their own hard-coded, fine-tuned implementations.
To combine flexibility and optimal performance, we introduce P4IRS, an intermediate representation and compiler for particle simulations which is aimed at delivering good performance.
We describe P4IRS, display usage examples and evaluate the performance from its generated code in both MD and DEM fields.
\end{abstract}

%%Graphical abstract
\begin{graphicalabstract}
%\includegraphics{grabs}
\end{graphicalabstract}

%%Research highlights
\begin{highlights}
\item Research highlight 1
\item Research highlight 2
\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
code generation \sep particle simulations \sep molecular dynamics \sep discrete element method \sep HPC
\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:introduction}

The computation of particle-pair interactions is widely used in physics simulations, especially in the fields of Molecular Dynamics and Discrete Element Method.
To efficiently compute the inter-particle contributions, different strategies can be used to increase efficiency and make use of the parallelism offered by the target hardware.
Most simulation packages, however, have their own hard-coded implementations, which are strictly tighted to their data structures and programming artifacts.
Also, several kernel implementations for the same potential or force field are present in a single package to target different hardwares such as GPUs and to exploit different optimization strategies, which leads to higher maintainability and substantially increases the portability effort.

To tackle these limitations, we developed P4IRS --- Parallel and Performance-Portable Particles Intermediate Representation and Simulator, an intermediate representation and compiler tool where particle interactions can be simply expressed as Python methods, and efficient and parallel code for the interaction description can be generated for multi-CPU and multi-GPU targets.
P4IRS provide its own intermediate representation, which allows it to perform compiler analysis and transformations, to then generate code for the proper backend such as C++ and CUDA with OpenMP/MPI.

P4IRS can generate several kernel flavours of the same potential such as Linked Cells and Verlet Lists, where data layouts for properties and specific arrays (such as the neighbor-lists) can be switched to the case with best performance.
A runtime interface in C++ is also available to integrate P4IRS with other simulation tools, as an example we show how P4IRS can use the load-balancing communication algorithm from waLBerLa to compute simulations with a more heterogeneous distribution of particles.

Most of P4IRS internal routines such as building Linked Cells and Verlet Lists are implemented in Python by constructing the intermediate representation for them with the assistance of operator overloading.
In this way, we leverage P4IRS analysis and transformations to achieve performance-portability for these routines as well, without requiring multiple tailored implementations.

The goals of this paper are summarized as follows:
\begin{itemize}
	\item We present our code generator tool P4IRS, describing how it works and how it can be used to generate efficient particle simulation codes.
	\item We discuss the advantages of using P4IRS to achieve performance-portability, also comparing with other previous approaches such as tinyMD and MESA-PD.
	\item We describe P4IRS interfaces to allow its generated code to be integrated with other simulation tools, and provide an example with the waLBerLa framework.
	\item We provide performance results for several CPU and GPU machines, as well as scaling results for multi-CPU and multi-GPU in our in-house supercomputers Fritz and Alex.
\end{itemize}

This paper is structured as follows: In \autoref{sec:related_work} we list related work for MD and DEM simulation packages, code generation tools for particle simulations and performance evaluation of such simulations. In \autoref{sec:background} we explain the basic theory for particle simulations focusing on MD and DEM methods, also covering optimization strategies and choices. In \autoref{sec:pairs} we present P4IRS with its current features and application design. In \autoref{sec:evaluation} we evaluate the performance from P4IRS comparing it to other tools. Finally, \autoref{sec:conclusion} presents our conclusion and outlook for P4IRS, also covering our future work.

\section{Related Work}
\label{sec:related_work}

\subsection{Simulation Packages}
\label{sec:packages}

Diverse simulation fields can be described by updating particle trajectories based on their interactions with other particles.
In this paper, we mainly focus on MD and DEM, where in MD particles represent point masses (i.e. atoms) in the simulation, and in DEM particles represent rigid bodies with specific shapes (i.e. spheres, hyperplanes, ellipses).
Most packages available use similar techniques to compute trajectories, but each of them have their own specific algorithms, parallelism strategies and hand-tuned implementations for specific hardware.
With P4IRS, we focus on using a single description of the problem, and then leverage its implemented routines and code transformations to provide the high-end implementation for the specific target, which currently can be C++ with OpenMP and CUDA, also supporting MPI for multiple computing nodes.

In MD, there exist many packages with distinct strategies and simulation fields.
LAMMPS is a molecular package for the material-modeling field which uses the standard Verlet Lists algorithm to optimize the non-bonded short-range force calculations.
It offers many interatomic potentials, each one with different kernel implementations for a specific target (like CPU, GPU, Kokkos).
GROMACS is another package for MD with focus on bio-sciences simulation which uses its own Cluster Pair algorithm to optimize the short-range forces computation.
It does not support as many potentials as LAMMPS, but its algorithm tends to be significantly faster than the Verlet Lists since it employs SIMD parallelism more efficiently.
Furthermore, the optimal GROMACS kernel implementations rely on SIMD intrinsics to attain higher performance, which makes them harder to be maintained and ported to other architectures.
Also, the GPU algorithm in GROMACS differs from the CPU, since it relies on a strategy named ``super-clustering'' to reduce the amount of redundant computations in the GPU.
Other MD packages such as NAMD, AMBER and CHARMM also contain dedicated and hand-tuned implementations in general purpose languages for each supported backend.
Each version is tested, maintained, debugged and optimized independently.

Packages that support DEM simulations are in general more flexible (since simulation with more particle properties such as rotations and shapes are needed), however they usually do not have the same optimization techniques as the MD simulation packages.
The reason is most likely that it is not so feasible to support flexibility with sophisticated optimization algorithms as the complexity of the code grows considerably.
Examples of such packages are the LIGGGHTS DEM (based on LAMMPS), GranOO and YADE.
To mitigate such limitations, a few code generation approaches were developed for both MD and DEM.

\subsection{Code Generation Frameworks}
\label{sec:codegen}

% TODO: HOOMD-blue, OpenMM, MDL, ppmd, OpenFPM, PPME
% TODO: MESA-PD, tinyMD

\subsection{Performance Evaluation}
\label{sec:perfeval}

Several performance investigation researches for MD systems exist, most of them using MD packages as a black-box and adjusting different simulation parameters to analyse their implications in the overall performance.
In some cases, however, in-depth performance studies were made to gain a broader understanding on efficient ways to achieve SIMD vectorization with SSE/AVX instructions.
Most in-depth studies are performed using mini-apps or proxy-apps, with miniMD being worth to mention as it provides a simple copper face-cubic-centered lattice simulation using the kernels extracted from LAMMPS.
Our performance-focused prototyping harness MD-Bench is a toolbox developed for performance-engineering MD non-bonded short-range force calculation algorithms, with focus on in-core execution on CPUs and GPUs.
It currently supports the Verlet Lists and Cluster Pair algorithms from LAMMPS and GROMACS, respectively.
One of our goals is to use the knowledge obtained with MD-Bench and integrate it into P4IRS to achieve state-of-the-art performance and flexibilty altogether, which is the main reason we develop P4IRS.

DEM simulations, however, do not contain many in-depth performance studies, and most of them focus on evaluating distributed-memory parallelism in weak-scaling settings.
With P4IRS, we intend to bridge the gap between performance strategies across different simulation domains, and use the most efficient algorithm whenever it is possible, without requiring significant efforts such as re-implementing kernels and re-designing applications with tailored data structures for improving performance.

To evaluate performance in a single CPU core or GPU in this paper, we use MD-Bench and miniMD as the baseline performance, as they are well-known implementations that reach state-of-the-art performance for MD simulations.
For DEM cases, we use MESA-PD as the comparison target, since our focus is to show that we can achieve performance-portability with the flexibility that MESA-PD provides. 

\section{Background and Theory}
\label{sec:background}

Particle simulations consist of solving N-body problems and has a wide range of applications as many fields can be represented as a system of interacting particles.
In this sense, particles can be either atoms, rigid bodies or even planets, stars and galaxies.
Interaction contributions can come either from inter-atomic van-der-Walls forces based on the Lennard-Jones potential, or even from gravitational forces generated by super-massive particles.
In order to simulate such systems, however, we need well-defined and closed system of particles with known boundary conditions.
In the scope of this paper, particle contributions are computed for particles that lie within a range or ``cutoff radius''. 
Strategies to compute approximations from particles beyond this range (also known as long-range contributions) also exist, but are not disclosed.
To provide distinct examples of simulations and display a certain level of flexibility from P4IRS, we focus on MD and DEM simulations in this work.

\subsection{Molecular Dynamics}
\label{sec:md}

\ac{MD} simulations are widely used today to study the behavior of microscopic structures.
These simulations reproduce the interactions among atoms in these structures on a macroscopic scale while allowing us to observe the evolution in the system of particles in a time-scale that is simpler to analyze.

Different areas such as material science to study the evolution of the system for specific materials, chemistry to analyze the evolution of chemical processes, or biology to reproduce the behavior of certain proteins and bio-molecules resort to simulations of \ac{MD} systems.

A \ac{MD} system to be simulated constitutes of a number of atoms, the initial state (such as the atoms' position or velocities), and the boundary conditions of the system.
Here, we use \ac{PBC} in all directions, hence when one atom crosses the domain, it reappears on the opposite side with the same velocity.
%These simulations are defined by establishing the number of atoms in a system, the initial state of the system (such as the atoms' positions or velocities), and the boundary conditions for the system.
Fundamentally, atom trajectories in classical \ac{MD} systems are computed by integrating Newton-Euler equations for every atom $i$:
\begin{equation}
    \hat F_i = m \dot{\hat v}_i \label{eq:newton_force}
\end{equation}
\begin{equation}
    \hat v_i = m \dot{\hat x}_i \label{eq:newton_velocity}
\end{equation}
The forces $F_i$ are described by the negative gradient of the potential of interest.
The mutual force between atom $i$ and $j$ as governed by the \ac{LJ} potential
is given as
\begin{equation}
    \hat{F}_{LJ}(\hat{x_i}, \hat{x_j}) = -\frac{24\varepsilon}{x_{ij}} \left( \frac{\sigma}{x_{ij}} \right)^{6} \left[ 2\left(\frac{\sigma}{x_{ij}}\right)^{6} - 1\right] \hat{x}_{ij}~,
    %V_{LJ} = 4\varepsilon \left[ \left(\frac{\sigma}{r}\right)^{12} - \left(\frac{\sigma}{r}\right)^6 \right]
    \label{eq:lennard_jones}
\end{equation}
where $\hat{x}_{ij}$ is the distance vector between atoms $i$ and $j$, $x_{ij}$ is the distance between atoms $i$ and $j$, $\varepsilon$ is the width of the potential well, and $\sigma$ specifies at which distance the potential is~$0$.

\subsection{Discrete Element Method}
\label{sec:dem}

Besides computing trajectories for zero-dimensional point masses as done in MD, we can model particles as objects with specific shapes and spatial extension using the Discrete Element Method (DEM).
This requires the simulation tool to support more degrees of freedom for particles, as it needs to take some of their spatial properties (such as rotation) into consideration.
In this work, we consider particles with homogeneous density, and then use the center of mass for each particle as its position, which due to its density properties coincides with its geometrical center.
Equations of motions are extended with the following terms for the torque and angular velocity:

\begin{equation}
    \hat{\tau}{_i} = I \dot{\hat \omega}_i + {\hat \omega}_i \times I {\hat \omega}_i \label{eq:newton_torque}
\end{equation}
\begin{equation}
    \hat{\omega}{_i} = \dot{\hat{\phi}}_i \label{eq:newton_angular_velocity}
\end{equation}

Also, a collision detection step is also needed to check when two particles overlap.
When particles overlap, a collision is detected and information regarding the collision such as the contact point and penetration depth are calculated.
Note that there may be penetration when a collision happens because there is no guarantee that it is detect right when it starts, as the simulation is discretized by the timestep size.
For simplification purposes, we only calculate collision detections analytically in this work since there are no particles with complex shapes involved.

In this work, we use the penalty-based Linear Spring-Dashpot model to calculate forces among particles for the DEM cases.
The collision force is splitted into two components, the force in the normal direction and in the tangential direction.

\begin{equation}
	\hat{F}_{SD}^{n}(\hat{x_i}, \hat{x_j}) = k^{n}\delta_{ij}\hat{n}_{ij} - \xi^{n}\hat{u}_{ij}^{n}
    %-\frac{24\varepsilon}{x_{ij}} \left( \frac{\sigma}{x_{ij}} \right)^{6} \left[ 2\left(\frac{\sigma}{x_{ij}}\right)^{6} - 1\right] \hat{x}_{ij}~,
    %V_{LJ} = 4\varepsilon \left[ \left(\frac{\sigma}{r}\right)^{12} - \left(\frac{\sigma}{r}\right)^6 \right]
    \label{eq:linear_spring_dashpot_normal}
\end{equation}

% Spring-Dashpot

\subsection{Optimization Strategies}
\label{sec:opts}

To obtain the force for a particle in a specific time-step, it is necessary to compute its partial contributions from each of the other particles present in the system.
In a naive strategy, this results in a quadractic complexity and severely affects the application performance, which limits the system sizes and time-scales of such simulations.

When force contributions become negligible at long-distance interactions, different algorithm strategies can be employed to perform computations only for particles within a specific cut-off region.
There are two main strategies which are commonly used for particle simulations in state-of-the-art packages: the Linked Cells approach and the Verlet Lists.

Linked Cells strategy sorts atoms spatially according to their position in cells (also called bins), where cells have a fixed size, and mapping particles into cells can be done straightforward by computing in which cell a particle is contained within.
After, forces can be computed for each particle by traversing its current and neighbor cells, and only adding contributions for particles within these cells.

The Linked Cells results in computing only forces for particles within a region that is discretized by their current and neighbor cells/bins, which can still results in a significant amount of redundant computations.
In order avoid even more redundant computations, the Verlet Lists strategy can be used jointly with Linked Cells.
The Verlet Lists constructs a list of neighbor particles for every particle, and this list can be created by using the Linked Cells structure, and doing particle-particle distance comparison to evaluate wether a particle should be added into the list.
At the end, the region for each particle search is defined by a sphere.
Note that, for the neighbor-lists construction to be worthwhile, it should be used more than once force calculation, hence neighbor-lists are not created at every simulation time-step, and a "skin" can be added to the cutoff radius to extend the neighbor-search and avoid missing computations from atoms that end up entering the cut-off region.

Despite avoiding computations from long-distance particles, another common optimization is to take advantage of Newton's Third Law and computing forces in only one direction.
This means that only half of particle-pairs are traversed during force computation, and forces are decreased for the neighbor atom during their computation.
Ideally, this should lead to a factor of two speedup, but this strategy has a few drawbacks: (a) parallelism is harmed since decreasing force of j-atoms requires atomic operations to avoid race conditions and (b) it is required to gather and scatter forces for j-atoms into a SIMD fashion, which results in scattered accesses and extra instructions to properly organize the SIMD vectors.
At the end, this strategy is likely to be beneficial for more arithmetic-intensive kernels, and preferably when the trade-off between parallelism and less arithmetic operation is compensated.

% Linked Cells, Verlet Lists, Cluster Pair
% Half-neighbor lists, Full-neighbor lists

\section{P4IRS}
\label{sec:pairs}

P4IRS is a standalone compiler and domain-specific language for particle simulations which aims at generating optimized code for different target hardwares.
It is released as a Python package and allows users to define kernels, integrators and other particle routines in a high-level and straightforward fashion.

\subsection{Frontend and features}
\label{sec:frontend}

\autoref{lst:pairslj} shows a simple Lennard-Jones kernel implemented in P4IRS.
Functions can be provided through a method (line 1), and the parameters in the method refer to different particles in the system.
In this way, many-body potentials can also be described using methods with more than two parameters.
Some keywords are also provided by P4IRS for writing the kernels, in line 2 the \emph{squared\_distance} method is used for obtaining the squared distance for the atoms \emph{i} and \emph{j}, and in line 4 the \emph{delta} method is used for obtaining the delta vector.
Also, in lines 3 and 4 it is possible to see accesses to feature properties called \emph{sigma6} and \emph{epsilon}, which are defined during the simulation setup as properties that depends on the atom types feature.
In line 4, the \emph{apply} method is used for applying the computed term into the \emph{force} property of the particles, the method allows for identifying particle properties reductions and is also used when generating the kernel variant using the half neighbor-lists.

\begin{lstlisting}[language=Python,
		   label={lst:pairslj},
		   caption={Lennard-Jones force description in P4IRS.}]
def lj(i, j):
    sr2 = 1.0 / squared_distance(i, j)
    sr6 = sr2 * sr2 * sr2 * sigma6[i, j]
    apply(force, delta(i, j) * (48.0 * sr6 * (sr6 - 0.5) * sr2 * epsilon[i, j]))
\end{lstlisting}

Functions that update every particle separately can also be described with a single parameter.
\autoref{lst:euler} displays how to implement a simple Euler integrator for \ac{MD} (see line 1).
In lines 2-3, there are also property accesses for the particles' linear velocities, forces, masses and positions.
Note that vector operations are intrinsically supported in P4IRS, which is also the case for matrices and quaternions data structures.
P4IRS methods also allow the specification of parameters such as the \emph{dt} specified in line 3, these must be given when setting up the simulation.

\begin{lstlisting}[language=Python,
		   label={lst:euler},
		   caption={Euler integrator description in P4IRS.}]
def euler(i):
    linear_velocity[i] += dt * force[i] / mass[i]
    position[i] += dt * linear_velocity[i]
\end{lstlisting}

\autoref{lst:pairs_md_setup} shows the simulation setup for the \ac{MD} case. In the first part, a \emph{simulation} instance is created, and its identifier, the list of particle shapes used, the number of time-steps and the floating-point precision are specified.
Further, the properties needed for each particle in the simulation are specified, together with their data types and (if needed) initial values.
Some properties are also defined as volatile (in this case the \emph{force}) property, which means they are reset after every time-step.
To add feature properties, it is first required to add the specified feature (in this example, each atom has a feature named \emph{type}), and then specifiy the feature name for these properties.
Next, the simulation domain is specified and the initial state of the system is defined by reading data from a file, a list of properties must be given in the same order as they appear in the file, and the particles shape must also be included.

\begin{lstlisting}[language=Python,
		   label={lst:pairs_md_setup},
		   caption={Simple example for MD simulation setup in P4IRS.}]
import pairs

# ...

# Simulation setup
psim = pairs.simulation(
	"md", 			# Simulation identifier
	[pairs.point_mass()], 	# List of shapes
	timesteps=200,		# Number of time-steps
	double_prec=True)	# Use double-precision

# Particle properties
psim.add_position('position')
psim.add_property('mass', pairs.real(), 1.0)
psim.add_property('linear_velocity', pairs.vector())
psim.add_property('force', pairs.vector(), volatile=True)

# Features and their properties
psim.add_feature('type', ntypes)
psim.add_feature_property('type', 'epsilon', pairs.real(), [...])
psim.add_feature_property('type', 'sigma6', pairs.real(), [...])

# Simulation domain
psim.set_domain([xmin, ymin, zmin, xmax, ymax, zmax])

# Initial state
psim.read_particle_data(
	"data/minimd_setup_32x32x32.input",
	['type', 'mass', 'position', 'linear_velocity', 'flags'],
	pairs.point_mass())

# Optimization settings
psim.reneighbor_every(20)
psim.compute_half()
psim.build_neighbor_lists(cutoff_radius + skin)
psim.vtk_output(f"output/lj_{target}")

# Kernels to compute
psim.compute(lj, cutoff_radius)
psim.compute(euler, symbols={'dt': dt})

# Target hardware
if target == 'gpu':
    psim.target(pairs.target_gpu())
else:
    psim.target(pairs.target_cpu())

psim.generate()
\end{lstlisting}

After the properties and simulation settings, the optimization settings must also be specified (whether neighbor-lists are used, the reneighbouring frequency and if these must be half-neighbor lists).
The proper kernels are specified using the \emph{compute} method. The cutoff radius must be specified for kernels with more than one parameter/particle and other parameters used in such kernels (such as the \emph{dt} in the Euler kernel) must also be provided as a Python dictionary.

%\begin{lstlisting}[language=Python,
%		   label={lst:pairslj},
%		   caption={Lennard-Jones force description in P4IRS.}]
%\end{lstlisting}

\subsection{Contact history and contact properties}
\label{sec:contact_history}

When implementing DEM kernels such as the Linear Spring-Dashpot, properties computed during two contacted particles must be used across subsequent time-step interactions whenever they happen.
For this reason, P4IRS supports contact properties, which are properties tighted to a contact in the simulation.
These can be simply added through the \emph{add\_contact\_property} method as can be seen in \autoref{lst:contactprops1} for the sticking, tangential spring displacement and the impact velocity magnitude:

\begin{lstlisting}[language=Python,
		   label={lst:contactprops1},
		   caption={Setup example for contact properties.}]
psim.add_contact_property('is_sticking', pairs.int32(), 0)
psim.add_contact_property('tangential_spring_displacement', pairs.vector(), [0.0, 0.0, 0.0])
psim.add_contact_property('impact_velocity_magnitude', pairs.real(), 0.0)
\end{lstlisting}

These properties can be used directly in kernels in the same way as feature properties.
\autoref{lst:contactprops2} shows an example of their usage in the Linear Spring-Dashpot kernel.
Note that when such properties are not stored for the particle pair, then the default values are always used.

\begin{lstlisting}[language=Python,
		   label={lst:contactprops2},
		   caption={Setup example for contact properties.}]
def linear_spring_dashpot(i, j):
    # ...
    tan_spring_disp = tangential_spring_displacement[i, j]
    impact_vel_magnitude = impact_velocity_magnitude[i, j]
    impact_magnitude = select(impact_vel_magnitude > 0.0, impact_vel_magnitude, length(rel_vel))
    # ...
    tangential_spring_displacement[i, j] = new_tan_spring_disp
    impact_velocity_magnitude[i, j] = impact_magnitude
    is_sticking[i, j] = n_sticking
\end{lstlisting}

To provide fast and efficient accesses of contact properties when using neighbor-lists, P4IRS keep the particles in the same indexes in both data structures.
This prevents a lookup operation within the kernel for finding the contact index, and also keeps data access contiguous for the contact history data structure.
When Linked Cells are used without a Verlet Lists, such optimizations are not possible, which should significantly impact the performance for the kernel.

%\begin{lstlisting}[language=Python,
%		   label={lst:pairslj},
%		   caption={Lennard-Jones force description in P4IRS.}]
%\end{lstlisting}

\subsection{Backend and Code Generation}
\label{sec:backend}

Generating the final backend code for the specified kernels with proper parallelism, optimization strategies and GPU offloading is challenging.
P4IRS uses its own \ac{IR} to facilite this process, where internal routines to build the cell lists, neighbor lists and generate the communication buffers are deep-embedded into Python to further be mapped into the P4IRS \ac{AST}.
Afterwards, compile analyses and transformations are performed to properly add parallelism and optimizations into the code.
In its current state, P4IRS generates C++ and CUDA code with MPI for distributed-memory parallelism, but new backend languages, intermediate representations and technologies can be included without difficulties by extending its code generator.

Functions that require external libraries and usually run on the CPU (such as reading particle data from file and writing data to VTK files) can be implemented using P4IRS runtime routines.
P4IRS provide a C++ instance that contains all the meta-data for the simulation, which can be useful for such routines and provide a simple way to integrate functionalities that do not need extensive optimizations and may rely on external libraries.

\begin{lstlisting}[language=C++,
		   label={lst:pairsruntime},
	   	   caption={P4IRS runtime routine example for writing VTK data into a file.}]
void vtk_write_data(PairsSimulation *ps, const char *filename, int start, int end, int timestep) {
    std::string output_filename(filename);
    auto position = ps->getAsVectorProperty(ps->getPropertyByName("position"));
    // ...
    filename_oss << filename << "_";
    if(ps->getDomainPartitioner()->getWorldSize() > 1) {
        filename_oss << "r" << ps->getDomainPartitioner()->getRank() << "_";
    }
    // ...
    filename_oss << timestep << ".vtk";
    std::ofstream out_file(filename_oss.str());
    // ...
    ps->copyPropertyToHost(position);
    for(int i = start; i < end; i++) {
        out_file << std::fixed << std::setprecision(4) << position(i, 0) << " ";
        out_file << std::fixed << std::setprecision(4) << position(i, 1) << " ";
        out_file << std::fixed << std::setprecision(4) << position(i, 2) << "\n";
    }
    // ...
}
\end{lstlisting}

\autoref{lst:pairsruntime} shows an example for the routine to write data into VTK files, note that the first parameter is a \emph{PairsSimulation} object, and can be used to check whether properties exist, check if they are synced with the host device and (in case not) copy the data from the host.
The meta-data object is feed at the start of the simulation with the code generated by P4IRS, and it is therefore dependent on each simulation.

\subsection{Shape Partitioning}
\label{sec:shape_partitioning}

\subsection{Domain Partitioning and Communication}
\label{sec:domain_partitioning}

% Show examples (LJ, EAM)
% Linked Cells / Verlet Lists
% Can generate CPU/GPU code
% Vectors, quaternions and matrices
% Properties, feature properties and contact properties
% Contact history
% Shape partitioning

\section{Evaluation}
\label{sec:evaluation}

% Testbed

% Single core CPU vs. MD-Bench (Runtime, MFLOPS/s, ...?)
% CPU MD weak-scaling
% Single GPU (vs. MD-Bench?)
% GPU MD weak-scaling

% DEM vs MESA-PD on CPU
%  - Linked Cells and Neighbor-lists
% DEM on a single GPU
% GPU DEM weak-scaling (or strong scaling?)

\section{Conclusion and Outlook}
\label{sec:conclusion}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% For citations use: 
%%       \citet{<label>} ==> Jones et al. [21]
%%       \citep{<label>} ==> [21]
%%

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-num-names} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

\begin{thebibliography}{00}

%% \bibitem[Author(year)]{label}
%% Text of bibliographic item

\bibitem[ ()]{}

\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.
